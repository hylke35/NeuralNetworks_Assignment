{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.callbacks import EarlyStopping\n",
        "from collections import namedtuple\n",
        "from matplotlib.ticker import FormatStrFormatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_rows = 2\n",
        "num_cols = 2\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "plots = [0, 4, 60, 98]\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < len(df_micro):\n",
        "        ax.plot(df_micro.iloc[plots[i]], marker=\"o\")\n",
        "        ax.set_title(f\"Time-Series {plots[i]+1}\")\n",
        "        ax.set_xlabel(\"Time\")\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        ax.set_xticks(range(1, 21))\n",
        "    else:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.xticks(list(range(1, 21)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "df_train_original = df_train.to_numpy().reshape(-1, 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoothing\n",
        "#df_train_unsmoothed = df_train.to_numpy().reshape(-1,14)\n",
        "def exponential_smoothing(data, alpha=0.8):\n",
        "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
        "\n",
        "df_train = exponential_smoothing(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "# Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = df_train.to_numpy().reshape(-1,14)\n",
        "df_test = df_test.to_numpy().reshape(-1,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "data = scaled_data\n",
        "\n",
        "# Plotting the density plot\n",
        "sns.kdeplot(data, shade=True, color='blue')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Density Plot of Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_unsmoothed = df_train\n",
        "num_rows = 2\n",
        "num_cols = 2\n",
        "\n",
        "smoothed_graphs = [0, 4, 60, 98]\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "counter = 0\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < len(df_micro):\n",
        "        ax.plot(df_train[smoothed_graphs[counter]], label=\"Pre-Processed Data\", marker=\"o\")\n",
        "        ax.set_title(f\"Time-Series {smoothed_graphs[counter] + 1}\")\n",
        "        ax.set_xlabel(\"Time\")\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        ax.set_xticks(range(1, 14))\n",
        "        ax.legend()\n",
        "    else:\n",
        "        ax.axis(\"off\")\n",
        "    counter = counter + 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.xticks(list(range(1, 14)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows.reshape(-1,4), labels.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = []\n",
        "train_y = []\n",
        "test_x = []\n",
        "test_y = []\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "    windows_train, labels_train = make_windows(df_train[i], window_size=4, horizon=1)\n",
        "    windows_test, labels_test = make_windows(df_test[i], window_size=4, horizon=1)\n",
        "    train_x = np.concatenate((np.array(train_x).reshape(-1,4), windows_train.reshape(-1,4)))\n",
        "    train_y = np.concatenate((np.array(train_y).reshape(-1,1), labels_train.reshape(-1,1)))\n",
        "    test_x = np.concatenate((np.array(test_x).reshape(-1,4), windows_test.reshape(-1,4)))\n",
        "    test_y = np.concatenate((np.array(test_y).reshape(-1,1), labels_test.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    numerator = tf.abs(y_pred - y_true)\n",
        "    denominator = (tf.abs(y_pred) + tf.abs(y_true))/2\n",
        "    smape = tf.reduce_mean(numerator / denominator) * 100\n",
        "    return smape\n",
        "\n",
        "def smape_loss(y_true, y_pred):\n",
        "    numerator = tf.abs(y_pred - y_true)\n",
        "    denominator = ((tf.abs(y_pred) + tf.abs(y_true))/2) + 0.1\n",
        "    smape = tf.reduce_mean(numerator / denominator)\n",
        "    return smape\n",
        "\n",
        "def metric_mdape(y_true, y_pred):\n",
        "    return tfp.stats.percentile((tf.abs(tf.math.subtract(y_true, y_pred)/ y_true)), 50.0, interpolation='midpoint')\n",
        "      \n",
        "def evaluate_mdape(y_true, y_pred):\n",
        "    return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers hidden_neurons\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128])\n",
        "regularizations = np.array([0.001, 0.01, 0.001])\n",
        "hidden_layers = np.array([2, 3, 4, 5, 6, 8]) # 2, 3, 4, 6, 8\n",
        "hidden_neurons = np.array([2, 3, 4, 5])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers, hidden_neurons)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-series expanding window validation\n",
        "#with tf.device('/cpu:0'):\n",
        "    \n",
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, val_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, val_y_cv = train_y[train_index], train_y[test_index]\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential(name=\"model\")\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            model_cv.add(tf.keras.layers.Dense(combination.hidden_neurons, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=smape_loss,\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[metric_mdape, \"mae\", \"mse\"]) # Backpropagation\n",
        "        \n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "\n",
        "        predictions = model_cv.predict(val_x_cv)\n",
        "\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(val_y_cv), de_standardise(predictions))\n",
        "        \n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        \n",
        "    mean_smape = np.mean(smape_scores)\n",
        "    mean_mdape = np.mean(mdape_scores)\n",
        "    hyperparameters = {\n",
        "        'learning_rate': combination.learning_rate,\n",
        "        'batch_size': combination.batch_size,\n",
        "        'regularization': combination.regularization,\n",
        "        'hidden_neurons': combination.hidden_neurons,\n",
        "        'hidden_layers': combination.hidden_layers\n",
        "    }\n",
        "    print(f\"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}\")\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 20)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "print(f\"Hidden Layers: {optimal_hyperparameters['hidden_layers']}\")\n",
        "print(f\"Hidden Neurons: {optimal_hyperparameters['hidden_neurons']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    #tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=smape_loss,\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[metric_mdape, \"mae\", \"mse\"]) # Backpropagation\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_5.hdf5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=500, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return standardised_x[-horizon:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = []\n",
        "def evaluate_model_on_test(model, df_train=df_train, df_test=df_test, horizon=1):\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "    \n",
        "    for i in range(len(df_train)):\n",
        "        window = de_standardise(df_train[i][10:14])\n",
        "        labels = df_test[i][0:horizon]\n",
        "        test_preds = autoregression(model, window, horizon)\n",
        "        #print(f\"Destandardised test pred: {de_standardise(test_preds)}\")\n",
        "        \n",
        "        #print(f\"Full Labels: {df_test[i]}\")\n",
        "        #print(f\"Window: {window}\")\n",
        "        predictions.append(de_standardise(test_preds))\n",
        "        print(f\"Labels: {labels}, Window: {window}\")\n",
        "        print(f\"Label: {labels}, Pred: {de_standardise(test_preds)}\")\n",
        "        smape_score, mdape_score = evaluate_pred(labels, de_standardise(test_preds))\n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        print(f\"Current mean SMAPE: {smape_score}, Current mean MDAPE: {mdape_score}\")\n",
        "\n",
        "    mean_smape_score = np.mean(smape_scores)\n",
        "    mean_mdape_score = np.mean(mdape_scores)\n",
        "    return mean_smape_score, mean_mdape_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(\"best_model_4.hdf5\", custom_objects={\"smape_loss\": smape_loss, \"metric_mdape\": metric_mdape})\n",
        "test1, test2 = evaluate_model_on_test(loaded_model, horizon=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_rows = 2\n",
        "num_cols = 2\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 8))\n",
        "axes = axes.flatten()\n",
        "plots = [1, 7, 99, 134]\n",
        "for i, ax in enumerate(axes):\n",
        "    if i < len(df_micro):\n",
        "        testplt = np.append(df_micro.iloc[plots[i]][:14], predictions[plots[i]])\n",
        "        ax.plot(list(range(1, 21)), testplt, marker='o', label='Predicted')\n",
        "        ax.plot(list(range(1, 21)), df_micro.iloc[plots[i]].to_numpy(), marker='o', label=\"Actual\")\n",
        "        ax.axvline(x = 14, color = 'gray', linestyle='--')\n",
        "        ax.set_title(f\"Time-Series {plots[i]+1} - Original Data vs. Predicted Data\")\n",
        "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
        "        ax.set_xticks(range(1, 21))\n",
        "        ax.set_xlabel(\"Time\")\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        ax.legend()\n",
        "    else:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.xticks(list(range(1, 21)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pred(index):\n",
        "    testplt = np.append(df_micro.iloc[index][:14], predictions[index])\n",
        "    plt.plot(list(range(1, 21)), testplt, marker='o', label='Predicted')\n",
        "    plt.plot(list(range(1, 21)), df_micro.iloc[index].to_numpy(), marker='o', label=\"Actual\")\n",
        "    plt.axvline(x = 14, color = 'gray', linestyle='--')\n",
        "    plt.title(f\"Time-Series {index+1} - Original Data vs. Predicted Data\")\n",
        "    plt.xticks(list(range(1, 21)))\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_pred(98)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
