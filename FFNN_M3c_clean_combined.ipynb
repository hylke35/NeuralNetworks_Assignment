{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess(y, details):\n",
    "    # print(details)\n",
    "    mean = details[2][0]\n",
    "    std = details[2][1]\n",
    "    PF = details[2][2]\n",
    "    PFtype = details[2][3]\n",
    "    time = details[3]\n",
    "\n",
    "    # print(\"mean:\", mean, \"std:\", std, \"PF\", PF, \"type\", PFtype, \"time\", time)\n",
    "\n",
    "    if(PFtype == 1):\n",
    "        return ((y * std) + mean) * np.exp(PF[0] * time + PF[1])    \n",
    "    # print(\"here\")\n",
    "    return (y * std + mean) * (PF[0] * np.square(time) + PF[1] * time + PF[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, smoothing, alpha = None):\n",
    "\n",
    "    # #Exponential smoothing\n",
    "    if smoothing:\n",
    "        data = data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    PFtype = -1\n",
    "    PF1 = np.polyfit(np.linspace(0,len(data) - 1,num=len(data)), np.log(data), 1)\n",
    "    PF2 = np.polyfit(np.linspace(0,len(data) - 1,num=len(data)),data, 2)\n",
    "    error1 = data - (np.exp(PF1[0] * np.linspace(0, len(data) - 1, num=len(data)) + PF1[1]))\n",
    "    error2 = data - (PF2[0] * np.square(np.linspace(0,len(data) - 1,num=len(data))) + PF2[1] * np.linspace(0, len(data) - 1, num=len(data)) + PF2[2])\n",
    "    \n",
    "    #Otto: dit is de keuze voro welke je preporcessed. je kan die plotjes un commenten om de fit te zien\n",
    "    if(np.sum(np.square(error1)) < np.sum(np.square(error2))):\n",
    "        PF = PF1\n",
    "        preprocessed = data / (np.exp(PF[0] * np.linspace(0,len(data) - 1,num=len(data)) + PF[1]))\n",
    "        PFtype = 1\n",
    "    else:\n",
    "        PF = PF2\n",
    "        preprocessed = data / (PF2[0] * np.square(np.linspace(0,len(data) - 1,num=len(data))) + PF2[1] * np.linspace(0,len(data) - 1,num=len(data)) + PF2[2])\n",
    "        PFtype = 2\n",
    "    \n",
    "    m = np.mean(preprocessed)\n",
    "    s = np.std(preprocessed)\n",
    "    preprocessed = (preprocessed - m)/s\n",
    "    details = [m, s, PF, PFtype]\n",
    "    \n",
    "    return preprocessed, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_clean(y_true, y_pred):\n",
    "    smape = 100 * np.mean(2*np.abs(y_pred - y_true) / (y_true + y_pred))\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_validation, y_validation, window_size, options): #x_validation, y_validation\n",
    "    # Build the FFNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(window_size, 1))) \n",
    "    model.add(keras.layers.Dense(options.layers[0], \n",
    "                                activation=options.activation1,\n",
    "                                kernel_initializer=tf.initializers.HeNormal(), \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(options.regularization)))\n",
    "\n",
    "    if len(options.layers) > 2:\n",
    "        for i in range(1,len(options.layers)-1):\n",
    "            model.add(keras.layers.Dense(options.layers[i], \n",
    "                                        activation=options.activation,\n",
    "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(options.regularization)))\n",
    "                \n",
    "    model.add(keras.layers.Dense(1, activation=options.activation2))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    #early stopping and saving the best model SOURCE: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50, min_delta=0.001)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "    # fit model\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), batch_size=options.batchSize, verbose = 0, callbacks=[es, mc], epochs= 1000) #fit the model with early stop\n",
    "\n",
    "    ##this is the best model\n",
    "    saved_model = load_model(\"best_model.h5\")\n",
    "\n",
    "    _, train_acc = saved_model.evaluate(x_train, y_train, verbose = 0)\n",
    "    _, test_acc = saved_model.evaluate(x_validation, y_validation, verbose = 0)\n",
    "\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # # plot the different accuracies. maybe then dont do a early stop. \n",
    "    # pyplot.plot(history.history['loss'], label='train')\n",
    "    # pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    # pyplot.legend()\n",
    "    # pyplot.show()\n",
    "\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_train = df.iloc[:,:14]\n",
    "df_test = df.iloc[:,14:]\n",
    "\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split(data, fold, v, window_size, alpha):\n",
    "    prep = []\n",
    "    validationSet = []\n",
    "    validation = data.iloc[fold-v:fold]\n",
    "    train = data[~data.isin(validation)].dropna()\n",
    "\n",
    "    for index, row in train.iterrows():\n",
    "        p, details = preprocess(row, smoothing = 1, alpha = alpha)\n",
    "        prep.append(p)\n",
    "\n",
    "    train = np.array(prep)\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        for j in range(train.shape[1] - window_size):\n",
    "            x_train.append(train[i][j:j + window_size])\n",
    "            y_train.append(train[i][j + window_size])\n",
    "\n",
    "    x_train = np.array(x_train).reshape(len(x_train), window_size)\n",
    "    y_train = np.array(y_train).reshape(len(y_train))\n",
    "\n",
    "    for index, row in validation.iterrows():\n",
    "        p, details = preprocess(row, smoothing = 0)\n",
    "        prep.append(p)\n",
    "        for i in range(len(p) - window_size):\n",
    "            validationSet.append([p[i:i + window_size], p[i + window_size], details, i + window_size])\n",
    "\n",
    "    x_validation = np.array([x[0] for x in validationSet]).reshape(len(validationSet), window_size)\n",
    "    y_validation = np.array([x[1] for x in validationSet]).reshape(len(validationSet))\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, validationSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(rows, k):\n",
    "    folds = []\n",
    "    rows = np.array_split(np.arange(rows), k)\n",
    "    for row in rows:\n",
    "        folds.append(row[-1])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(df_train, k, options, window_size, val_frac, alpha):\n",
    "    avgSmape = []\n",
    "    train = df_train.sample(frac = 1)\n",
    "    folds = get_folds(len(train), k) #Indicates the points to which data should be used in every fold\n",
    "\n",
    "    validation_size = int(np.floor(val_frac*len(train)))\n",
    "\n",
    "    for fold in folds:\n",
    "        x_train = []\n",
    "        y_train= []\n",
    "        x_train, y_train, x_validation, y_validation, validationSet = preprocess_and_split(train, fold, validation_size, window_size, options.smoothingFactor) #takes df's returns np arrays\n",
    "\n",
    "\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "\n",
    "        validationPrediction = model.predict(x_validation, verbose=0)\n",
    "        yHatReal = []\n",
    "        yReal = []\n",
    "\n",
    "        for i in range(len(y_validation)):\n",
    "            yReal.append(reprocess(y_validation[i], validationSet[i]))\n",
    "            yHatReal.append(reprocess(validationPrediction[i], validationSet[i]) )\n",
    "        smapeVal = 0\n",
    "        for i in range(len(yReal)):\n",
    "            smapeVal += smape_clean(yReal[i], yHatReal[i])\n",
    "        smapeVal /= len(yReal)\n",
    "        print(\"smape equals\", smapeVal, \"with\", fold, \"time step as training\")\n",
    "        avgSmape.append(smapeVal)\n",
    "    return np.mean(avgSmape), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.776, Test: 0.826\n",
      "smape equals 7.606298726523755 with 29 time step as training\n",
      "Train: 0.771, Test: 0.637\n",
      "smape equals 12.743873720911596 with 58 time step as training\n",
      "Train: 0.777, Test: 0.734\n",
      "smape equals 10.67999713092607 with 87 time step as training\n",
      "Train: 0.797, Test: 0.710\n",
      "smape equals 10.691417434537033 with 116 time step as training\n",
      "Train: 0.784, Test: 0.697\n",
      "smape equals 11.701616141400844 with 145 time step as training\n",
      "10.68464063085986\n"
     ]
    }
   ],
   "source": [
    "options = [[[3,3],'sigmoid','relu', 'sigmoid', 1e-4, 16, 1, 0,0]]\n",
    "options = pd.DataFrame(options)\n",
    "options = options.set_axis(['layers', 'activation1', 'activation', 'activation2', 'regularization', 'batchSize', 'smoothingFactor', 'averageSmape', 'varianceSmape'], axis=1)\n",
    "# print(options)\n",
    "window_size = 3\n",
    "val_frac = 0.1\n",
    "smape, model = kfolds(df_train, 5, options.iloc[0], window_size, val_frac, options.iloc[0].smoothingFactor)\n",
    "print(smape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lays = [[1],[2],[2,2],[3,3],[2,5]]\n",
    "batchSizes = [8,16,32]\n",
    "acts1 = ['sigmoid', 'relu','linear']\n",
    "activationFunctions = ['sigmoid', 'relu']\n",
    "acts2 = ['sigmoid', 'relu','linear']\n",
    "smoothingfactors = [0.9,1]\n",
    "window_size = 3\n",
    "val_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.775, Test: 0.731\n",
      "smape equals 9.53191671018765 with 29 time step as training\n",
      "Train: 0.769, Test: 0.728\n",
      "smape equals 9.495957032190901 with 58 time step as training\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "lays = [[3,3]]\n",
    "batchSizes = [16]\n",
    "acts1 = ['sigmoid', 'relu','linear']\n",
    "activationFunctions = ['sigmoid', 'relu']\n",
    "acts2 = ['sigmoid', 'relu','linear']\n",
    "regs = [1e-4]\n",
    "smoothingfactors = [1]\n",
    "window_size = 3\n",
    "val_frac = 0.1\n",
    "\n",
    "options = []\n",
    "\n",
    "for layer in lays:\n",
    "    for act1 in acts1:\n",
    "        for act2 in acts2:\n",
    "            for activation in activationFunctions:\n",
    "                for reg in regs:\n",
    "                    for smooth in smoothingfactors:\n",
    "                        for batchSize in batchSizes:\n",
    "                            options.append([layer, act1, activation, act2, reg, batchSize, smooth, 0, 0])\n",
    "\n",
    "\n",
    "options = pd.DataFrame(options)\n",
    "options = options.set_axis(['layers', 'activation1', 'activation', 'activation2','regularization', 'batchSize', 'smoothingFactor', 'averageSmape', 'varianceSmape'], axis=1)\n",
    "\n",
    "# print(options.iloc[0].layers)\n",
    "\n",
    "for i in range(len(options)):\n",
    "    smape_avg=[]\n",
    "    for j in range(1):\n",
    "        smp, model = kfolds(df_train, 5, options.iloc[0], 2, window_size, val_frac)\n",
    "        smape_avg.append(smp)\n",
    "\n",
    "    options.iat[i,3] = np.mean(smape_avg)\n",
    "    options.iat[i,4] = np.std(smape_avg)\n",
    "\n",
    "\n",
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(by=\"averageSmape\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   layers activation1 activation activation2  regularization  batchSize  \\\n",
      "0  [3, 3]     sigmoid       relu     sigmoid          0.0001         16   \n",
      "\n",
      "   smoothingFactor  averageSmape  varianceSmape  \n",
      "0                1             0              0  \n"
     ]
    }
   ],
   "source": [
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(by=\"averageSmape\")\n",
    "print(res)\n",
    "\n",
    "res = options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.663, Test: 0.535\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train= []\n",
    "val_frac = 0.1\n",
    "validation_size = int(np.floor(val_frac*len(df_train)))\n",
    "x_train, y_train, x_validation, y_validation, validationSet = preprocess_and_split(df_train, len(df_train), validation_size, window_size, res.iloc[0]['smoothingFactor']) #takes df's returns np arrays\n",
    "\n",
    "model = build_model(x_train, y_train, x_validation, y_validation, window_size, res.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   layers activation  batchSize  smoothingFactor  averageSmape  varianceSmape\n",
      "0  [2, 2]       relu         16                1             0              0\n"
     ]
    }
   ],
   "source": [
    "print(op.sort_values(by=\"averageSmape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "(146, 1)\n",
      "10/10 [==============================] - 0s 520us/step\n",
      "(292, 1)\n",
      "14/14 [==============================] - 0s 506us/step\n",
      "(438, 1)\n",
      "19/19 [==============================] - 0s 502us/step\n",
      "(584, 1)\n",
      "23/23 [==============================] - 0s 530us/step\n",
      "(730, 1)\n",
      "28/28 [==============================] - 0s 546us/step\n",
      "(876, 1)\n",
      "             0          1          2          3          4          5\n",
      "0     0.170762  13.671464  24.593617  37.532119  44.087357  52.117407\n",
      "1    31.907790  46.886814  37.258096  37.086392  37.300013  39.968017\n",
      "2    18.666267  34.340722  28.936940  31.573629  29.806002  16.602476\n",
      "3     6.072573  17.625750  25.405210  14.704635  20.885191  13.372797\n",
      "4    20.878399  13.999055   1.391686   6.056615   1.518975  12.363949\n",
      "..         ...        ...        ...        ...        ...        ...\n",
      "141   4.261588  11.877855   7.439210   4.997700  10.269803  11.819102\n",
      "142  10.521921   4.942593  16.118498   7.762845   9.658802   2.954356\n",
      "143   9.997748  21.850536   7.830317  23.972911  40.944614  57.332700\n",
      "144   7.308143  17.055890  25.665108  37.889772  53.109436  76.324439\n",
      "145  17.033094  17.162834   0.620561   7.912973  51.882849  66.641181\n",
      "\n",
      "[146 rows x 6 columns]\n",
      "[8.689954186237703, 12.275550270085102, 23.422265272628053, 24.230145733031186, 28.777677208291482, 28.17330865447111]\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "observations = []\n",
    "window_size = 3\n",
    "\n",
    "df_full = pd.DataFrame()\n",
    "df_full = df_train\n",
    "df_full = df_full.drop(df_full.columns[14:], axis=1)\n",
    "\n",
    "num_predictions = 6\n",
    "\n",
    "# Make predictions using autoregressive approach\n",
    "for pred in range(num_predictions):\n",
    "\n",
    "    PF = []\n",
    "    for index, row in df_full.iterrows():\n",
    "        # print(row)\n",
    "        if pred == 0:\n",
    "            preprocessed, details = preprocess(row, smoothing = 1, alpha = res.iloc[0].smoothingFactor)\n",
    "            PF.append(details[2:])\n",
    "            observations.append([preprocessed[11+pred:14+pred],0, details, 14+pred]) #y is unknown and first time point to predict is 15(or 14?)`\n",
    "            # df_full.loc[index,:] = preprocessed\n",
    "        else:\n",
    "            observations.append([row[11+pred:14+pred],0, details, 14+pred])\n",
    "\n",
    "    # Reshape the input for prediction\n",
    "    x = []\n",
    "    for i in (range(len(observations))):\n",
    "        x.append(observations[i][0])\n",
    "    x = np.array(x).reshape(len(x),window_size)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(x)\n",
    "\n",
    "    print(prediction.shape)\n",
    "\n",
    "    y_u = []\n",
    "    for i in range(len(prediction)):\n",
    "        y_u.append(reprocess(prediction[i], observations[i]))\n",
    "        # y_u.append(prediction[i])\n",
    "\n",
    "    # print(pd.DataFrame(y_u).shape)\n",
    "    predictions[15+pred] = pd.DataFrame(y_u)\n",
    "    df_full[15+pred] = pd.DataFrame(y_u)\n",
    "\n",
    "smapes = pd.DataFrame(columns=[i for i in range(num_predictions)])\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    smape_row = []\n",
    "    for j in range(num_predictions):\n",
    "        smape_row.append(smape_clean(predictions.iloc[i, j], df_test.iloc[i, j]))\n",
    "    smapes.loc[i] = smape_row\n",
    "\n",
    "print(smapes)\n",
    "\n",
    "smape_avgs = []\n",
    "for i in range(num_predictions):\n",
    "    smape_avgs.append(np.mean(smapes.iloc[:,i]))\n",
    "print(smape_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = []\n",
    "# y_hat = []\n",
    "# details = []\n",
    "# offset = 7\n",
    "# for i in range(0,10):\n",
    "#     y.append(observations[i+offset][1])\n",
    "#     y_hat.append(y[i] + 0.4)\n",
    "#     details.append(observations[i+offset])\n",
    "\n",
    "# repY = []\n",
    "# repY_hat = []\n",
    "# smape = 0\n",
    "# for i in range(10):\n",
    "#     repY.append(reprocess(y[i], details[i]))\n",
    "#     repY_hat.append(reprocess(y_hat[i], details[i]))\n",
    "#     smape += smape_clean(repY[i], repY_hat[i])\n",
    "\n",
    "# smape /= len(repY)\n",
    "# print(smape)\n",
    "# pyplot.plot(repY, label='original')\n",
    "# pyplot.plot(repY_hat, label='altered')\n",
    "# pyplot.plot(df_train.iloc[1,3:10], label = 'og')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38918c03f5e50da0883dd5b0b10b29c968b274fb52fc312fcc1e11a6fe51f463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
