{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed-window sequences for training and validation data\n",
    "def create_sequences(X, window_size):\n",
    "    seq_X = []\n",
    "    seq_y = []\n",
    "    for i in range(len(X) - window_size):\n",
    "        seq_X.append(X[i:i+window_size])\n",
    "        seq_y.append(X[i+window_size])\n",
    "    return seq_X, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    PF = np.polyfit(np.linspace(0,len(data),num=len(data)), np.log(data), 1)\n",
    "\n",
    "    preprocessed = data / (np.exp(PF[0] * np.linspace(0,len(data),num=len(data)) + PF[1]))\n",
    "    m = np.mean(preprocessed)\n",
    "    s = np.std(preprocessed)\n",
    "    preprocessed = (preprocessed - m)/s\n",
    "\n",
    "    details = [m, s, PF]\n",
    "\n",
    "    return preprocessed, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess(y, details):\n",
    "    mean = details[2][0]\n",
    "    std = details[2][1]\n",
    "    PF = details[2]\n",
    "    time = details[3]\n",
    "    \n",
    "    return ((y * std) + mean) * np.exp(PF[0] * time + PF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_loss(y_true, y_pred):\n",
    "    smape = 100 * tf.reduce_mean(2*tf.abs(y_pred - y_true) / (y_true + y_pred))\n",
    "    return smape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(model, validation):\n",
    "    smape = 0\n",
    "    prediction = model.predict(x_validation)\n",
    "    for i in range(len(validation)):\n",
    "        observation = validation[i]\n",
    "        pred = prediction[i]\n",
    "\n",
    "        x_hat = reprocess(pred, observation)\n",
    "        x = reprocess(observation[1], observation)\n",
    "\n",
    "        smape += 2*np.abs(x_hat-x)/(x+x_hat)\n",
    "\n",
    "    smape /= len(validation)\n",
    "    smape *=100\n",
    "\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_validation, y_validation, window_size, options):\n",
    "    # Build the FFNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(window_size, 1))) \n",
    "    model.add(keras.layers.Dense(options[0][0], activation='relu'))\n",
    "\n",
    "    if len(options[0]) > 1:\n",
    "        if len(options[0]) > 2:\n",
    "            for i in range(1,len(options[0])-1):\n",
    "                model.add(keras.layers.Dense(options[0][i], activation=options[1]))\n",
    "                \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Train the model\n",
    "    # model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], validation_data=(x_validation, y_validation), verbose = 0)\n",
    "    model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], verbose = 0)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(x_validation)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_validation, y_validation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_train = df.iloc[:,:14]\n",
    "df_test = df.iloc[:,14:]\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "observations = []\n",
    "PF = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    preprocessed, p = preprocess(np.array(row))\n",
    "    PF.append(p)\n",
    "   \n",
    "    for i in range(len(preprocessed) - window_size):\n",
    "        observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], p, i+window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling: dont use for now\n",
    "\n",
    "# np.random.shuffle(observations)\n",
    "# train = observations[:int(np.floor(len(observations)*0.8))]\n",
    "# validation = observations[int(np.floor(len(observations)*0.8)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.54158501  0.04810624  0.61488873]\n",
      " [-1.42635554 -0.98945875  0.05018875]\n",
      " [ 1.1468326  -0.46334693 -0.40423917]\n",
      " ...\n",
      " [ 1.66547463  1.27434186  1.19783833]\n",
      " [-0.40087186 -0.80585459 -0.94831183]\n",
      " [-0.34010422 -0.54019186 -0.98996967]]\n",
      "[ 0.96866218  1.55673215  0.10154063 ...  0.41226976 -0.6901943\n",
      " -1.31998537]\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    x_train.append(train[i][0])\n",
    "    y_train.append(train[i][1])\n",
    "\n",
    "x_validation = []\n",
    "y_validation = []\n",
    "\n",
    "for i in range(len(validation)):\n",
    "    x_validation.append(validation[i][0])\n",
    "    y_validation.append(validation[i][1])\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train),window_size)\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation),window_size)\n",
    "y_validation = np.array(y_validation).reshape(len(x_validation))\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "    df = df.iloc[:146,6:26]\n",
    "\n",
    "    df_train = df.iloc[:,:14]\n",
    "    df_test = df.iloc[:,14:]\n",
    "\n",
    "    window_size = 3\n",
    "\n",
    "    observations = []\n",
    "    PF = []\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        preprocessed, p = preprocess(np.array(row))\n",
    "        PF.append(p)\n",
    "   \n",
    "    for i in range(len(preprocessed) - window_size):\n",
    "        observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], p, i+window_size])\n",
    "\n",
    "    np.random.shuffle(observations)\n",
    "    train = observations[:int(np.floor(len(observations)*0.8))]\n",
    "    validation = observations[int(np.floor(len(observations)*0.8)):]\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        x_train.append(train[i][0])\n",
    "        y_train.append(train[i][1])\n",
    "\n",
    "    x_validation = []\n",
    "    y_validation = []\n",
    "\n",
    "    for i in range(len(validation)):\n",
    "        x_validation.append(validation[i][0])\n",
    "        y_validation.append(validation[i][1])\n",
    "\n",
    "    x_train = np.array(x_train).reshape(len(x_train),window_size)\n",
    "    y_train = np.array(y_train).reshape(len(y_train))\n",
    "    x_validation = np.array(x_validation).reshape(len(x_validation),window_size)\n",
    "    y_validation = np.array(y_validation).reshape(len(x_validation))\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smape(params):\n",
    "    x_train, y_train, x_validation, y_validation, window_size = get_data()\n",
    "    n_layers, l1, l2, l3, epochs, n_batch = params\n",
    "\n",
    "    layers = [int(l1), int(l2), int(l3)]\n",
    "    lays = []\n",
    "    for i in range(int(n_layers)):\n",
    "        lays.append(layers[i])\n",
    "        \n",
    "    acts = ['sigmoid','relu']\n",
    "\n",
    "    options = [lays, 'sigmoid', int(n_batch), int(epochs), 0, 0]\n",
    "\n",
    "    smape_avg=[]\n",
    "    for j in range(10):\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "        smape_avg.append(smape(model, validation))\n",
    "\n",
    "    return np.mean(smape_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geneticalgorithm import geneticalgorithm as ga\n",
    "\n",
    "# varbound = np.array([[1, 3], [2, 20], [2, 20], [2, 20], [1, 50], [1, 50]])\n",
    "# vartype = np.array([['int'], ['int'], ['int'], ['int'], ['int'], ['int']])\n",
    "\n",
    "# model = ga(function=get_smape, \n",
    "#             dimension=6,\n",
    "#             variable_type_mixed=vartype, \n",
    "#             variable_boundaries=varbound,\n",
    "#             function_timeout=240.0)\n",
    "\n",
    "# model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 511us/step\n",
      "11/11 [==============================] - 0s 582us/step - loss: 0.7746 - mse: 0.7746\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 481us/step\n",
      "11/11 [==============================] - 0s 616us/step\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 0.7740 - mse: 0.7740\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 492us/step\n",
      "11/11 [==============================] - 0s 582us/step - loss: 0.7715 - mse: 0.7715\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 591us/step\n",
      "11/11 [==============================] - 0s 567us/step\n",
      "11/11 [==============================] - 0s 613us/step - loss: 0.7718 - mse: 0.7718\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 536us/step\n",
      "11/11 [==============================] - 0s 464us/step\n",
      "11/11 [==============================] - 0s 580us/step - loss: 0.7696 - mse: 0.7696\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 491us/step\n",
      "11/11 [==============================] - 0s 520us/step\n",
      "11/11 [==============================] - 0s 605us/step - loss: 0.7743 - mse: 0.7743\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 579us/step\n",
      "11/11 [==============================] - 0s 495us/step\n",
      "11/11 [==============================] - 0s 603us/step - loss: 0.7736 - mse: 0.7736\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 578us/step\n",
      "11/11 [==============================] - 0s 493us/step\n",
      "11/11 [==============================] - 0s 604us/step - loss: 0.7696 - mse: 0.7696\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 494us/step\n",
      "11/11 [==============================] - 0s 512us/step\n",
      "11/11 [==============================] - 0s 591us/step - loss: 0.7733 - mse: 0.7733\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 479us/step\n",
      "11/11 [==============================] - 0s 709us/step\n",
      "11/11 [==============================] - 0s 578us/step - loss: 0.7726 - mse: 0.7726\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 592us/step\n",
      "          0        1   2   3          4         5\n",
      "0  [50, 50]  sigmoid  16  50  10.645826  0.011507\n"
     ]
    }
   ],
   "source": [
    "lays = [[50,50]]\n",
    "epochs = [50]\n",
    "batchSizes = [16]\n",
    "activationFunctions = ['sigmoid']\n",
    "\n",
    "options = []\n",
    "\n",
    "for layer in lays:\n",
    "    for activation in activationFunctions:\n",
    "        for batchSize in batchSizes:\n",
    "            for epoch in epochs:\n",
    "                options.append([layer, activation, batchSize, epoch, 0, 0])\n",
    "\n",
    "\n",
    "for i in range(len(options)):\n",
    "    smape_avg=[]\n",
    "    for j in range(10):\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options[i])\n",
    "\n",
    "        print(options[i])\n",
    "        \n",
    "        smape_avg.append(smape(model, validation))\n",
    "\n",
    "    options[i][4] = np.mean(smape_avg)\n",
    "    options[i][5] = np.std(smape_avg)\n",
    "\n",
    "\n",
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(4, ascending=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        1   2   3          4         5\n",
      "0  [50, 50]  sigmoid  16  50  10.645826  0.011507\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[406], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m activation \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m options \u001b[39m=\u001b[39m [lays, activation, batchsize, epochs, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m model \u001b[39m=\u001b[39m build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n\u001b[1;32m     19\u001b[0m observations \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m PF \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[399], line 19\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(x_train, y_train, x_validation, y_validation, window_size, options)\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], validation_data=(x_validation, y_validation), verbose = 0)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49moptions[\u001b[39m3\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49moptions[\u001b[39m2\u001b[39;49m], verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_validation)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py:1267\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m   1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1263\u001b[0m     x,\n\u001b[1;32m   1264\u001b[0m     y,\n\u001b[1;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39msteps_per_epoch,\n\u001b[0;32m-> 1267\u001b[0m     epochs\u001b[39m=\u001b[39mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[1;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39mworkers,\n\u001b[1;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy(),\n\u001b[1;32m   1274\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   1275\u001b[0m )\n\u001b[1;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "##Testing\n",
    "df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_test = df.iloc[:,1:14]\n",
    "\n",
    "window_size = 3\n",
    "n_predictions = 6\n",
    "\n",
    "lays = [500,500]\n",
    "epochs = [15]\n",
    "batchsize = [16]\n",
    "activation = ['sigmoid']\n",
    "\n",
    "options = [lays, activation, batchsize, epochs, 0, 0]\n",
    "\n",
    "model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "\n",
    "observations = []\n",
    "PF = []\n",
    "\n",
    "for p in range(n_predictions):  \n",
    "    for index, row in df_test.iterrows():\n",
    "        preprocessed, p = preprocess(np.array(row))\n",
    "        PF.append(p)\n",
    "        observations.append([preprocessed[11+p:],None, p, len(preprocessed)])\n",
    "    x = []\n",
    "    for i in range(len(observations)):\n",
    "        x.append(observations[i][0])\n",
    "    predictions = model.predict(x)\n",
    "    print(predictions)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lays = [500,500]\n",
    "epochs = [15]\n",
    "batchsize = [16]\n",
    "activation = ['sigmoid']\n",
    "\n",
    "options = [lays, activation, batchsize, epochs, 0, 0]\n",
    "\n",
    "smape_avg=[]\n",
    "for j in range(20):\n",
    "    model = test_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "    smape_avg.append(smape(model, validation))\n",
    "\n",
    "    options[i][4] = np.mean(smape_avg)\n",
    "    options[i][5] = np.std(smape_avg)\n",
    "\n",
    "print(pd.DataFrame(options))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38918c03f5e50da0883dd5b0b10b29c968b274fb52fc312fcc1e11a6fe51f463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
