{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 469,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# De-trend (each time series at a time)\n",
        "data = pd.DataFrame(df_micro.iloc[0])\n",
        "data.columns = [\"value\"]\n",
        "year = np.arange(0, 20)\n",
        "data['year'] = year\n",
        "data = data.set_index('year')\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(data['value'], model='additive', period=10)\n",
        "\n",
        "# Access the components of the decomposition\n",
        "trend = decomposition.trend\n",
        "#seasonal = decomposition.seasonal\n",
        "#residual = decomposition.resid\n",
        "test2 = pd.DataFrame(trend).plot()\n",
        "trend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 470,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "# Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_test.to_numpy().reshape(-1,1)\n",
        "df_test = pd.DataFrame(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.28740269],\n",
              "       [-1.21687544],\n",
              "       [-1.13856182],\n",
              "       [-1.0407236 ]])"
            ]
          },
          "execution_count": 473,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_x, train_y = make_windows(df_train.to_numpy(), window_size=4, horizon=1)\n",
        "test2_x, test2_y = make_windows(df_test.to_numpy(), window_size=4, horizon=1)\n",
        "\n",
        "train_x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {
        "id": "1J7pKjWMmxVN"
      },
      "outputs": [],
      "source": [
        "# Create a function to implement a ModelCheckpoint callback\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            verbose=0,\n",
        "                                            save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "def evaluate_mdape(y_true, y_pred):\n",
        " return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def calculate_average_rankings(y_true, y_pred):\n",
        "    num_series = len(y_pred)\n",
        "    num_methods = len(y_pred[0])\n",
        "\n",
        "    ranks = []  # to store ranks for each series\n",
        "\n",
        "    for series_index in range(num_series):\n",
        "        sape_values = [\n",
        "            abs((y_true[series_index] - forecast) / y_true[series_index]) * 100\n",
        "            for forecast in y_pred[series_index]\n",
        "        ]\n",
        "        sorted_sape = sorted(sape_values)  # sort SAPE values in ascending order\n",
        "        series_ranks = [sorted_sape.index(sape) + 1 for sape in sape_values]  # assign ranks to SAPE values\n",
        "        ranks.append(series_ranks)\n",
        "\n",
        "    mean_ranks = []  # to store mean ranks for each forecasting method\n",
        "\n",
        "    for method_index in range(num_methods):\n",
        "        total_rank = sum(ranks[series_index][method_index] for series_index in range(num_series))\n",
        "        mean_rank = total_rank / num_series\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    return mean_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true_set, y_pred_set):\n",
        "    # Average Ranking\n",
        "    avg_ranking = None\n",
        "    # Percentage Better\n",
        "    percentage_better = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "regularizations = np.array([0.001, 0.01, 0.1])\n",
        "hidden_layers = np.array([2, 3, 4, 5, 6, 10])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 813us/step\n",
            "11/11 [==============================] - 0s 952us/step\n",
            "11/11 [==============================] - 0s 928us/step\n",
            "11/11 [==============================] - 0s 844us/step\n",
            "11/11 [==============================] - 0s 825us/step\n",
            "11/11 [==============================] - 0s 789us/step\n",
            "11/11 [==============================] - 0s 945us/step\n",
            "11/11 [==============================] - 0s 966us/step\n",
            "11/11 [==============================] - 0s 799us/step\n",
            "11/11 [==============================] - 0s 916us/step\n",
            "11/11 [==============================] - 0s 953us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 886us/step\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "11/11 [==============================] - 0s 816us/step\n",
            "11/11 [==============================] - 0s 727us/step\n",
            "11/11 [==============================] - 0s 949us/step\n",
            "11/11 [==============================] - 0s 787us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 5ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 948us/step\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "11/11 [==============================] - 0s 993us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 902us/step\n",
            "11/11 [==============================] - 0s 995us/step\n",
            "11/11 [==============================] - 0s 831us/step\n",
            "11/11 [==============================] - 0s 879us/step\n",
            "11/11 [==============================] - 0s 909us/step\n",
            "11/11 [==============================] - 0s 930us/step\n",
            "11/11 [==============================] - 0s 958us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 911us/step\n",
            "11/11 [==============================] - 0s 850us/step\n",
            "11/11 [==============================] - 0s 873us/step\n",
            "11/11 [==============================] - 0s 849us/step\n",
            "11/11 [==============================] - 0s 859us/step\n",
            "11/11 [==============================] - 0s 941us/step\n",
            "11/11 [==============================] - 0s 945us/step\n",
            "11/11 [==============================] - 0s 892us/step\n",
            "11/11 [==============================] - 0s 935us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "Best Hyperparameters: {'learning_rate': 0.01, 'batch_size': 16, 'regularization': 0.001, 'hidden_neurons': [8, 8], 'hidden_layers': 2}\n",
            "Best SMAPE Score: 16.57030521045738\n",
            "Best MDAPE Score: 7.911011477052167\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    best_smape = float('inf')\n",
        "    best_hyperparameters = {}\n",
        "    hidden_neurons = np.arange(2, 9)\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, test_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, test_y_cv = train_y[train_index], train_y[test_index]\n",
        "        \n",
        "\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            random_neuron = random.choice(hidden_neurons)\n",
        "            chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(random_neuron, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mse\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "        predictions = model_cv.predict(test_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(test_y_cv), de_standardise(predictions))\n",
        "\n",
        "        if smape_score < best_smape:\n",
        "            best_smape = smape_score\n",
        "            best_mdape = mdape_score\n",
        "            best_hyperparameters = {\n",
        "                'learning_rate': combination.learning_rate,\n",
        "                'batch_size': combination.batch_size,\n",
        "                'regularization': combination.regularization,\n",
        "                'hidden_neurons': chosen_hidden_neurons,\n",
        "                'hidden_layers': combination.hidden_layers\n",
        "            }\n",
        "    return best_smape, best_mdape, best_hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 10)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.01\n",
            "Batch Size: 16\n",
            "Hidden Neurons 8 in Layer 1.\n",
            "Hidden Neurons 8 in Layer 2.\n",
            "\n",
            "Epoch 1/50\n",
            "126/128 [============================>.] - ETA: 0s - loss: 0.5995 - mse: 0.5665 - mae: 0.4788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 1s 1ms/step - loss: 0.5970 - mse: 0.5639 - mae: 0.4775\n",
            "Epoch 2/50\n",
            " 90/128 [====================>.........] - ETA: 0s - loss: 0.5030 - mse: 0.4726 - mae: 0.3998WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4912 - mse: 0.4611 - mae: 0.3952\n",
            "Epoch 3/50\n",
            " 90/128 [====================>.........] - ETA: 0s - loss: 0.4801 - mse: 0.4518 - mae: 0.3824WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4649 - mse: 0.4370 - mae: 0.3790\n",
            "Epoch 4/50\n",
            " 87/128 [===================>..........] - ETA: 0s - loss: 0.4885 - mse: 0.4627 - mae: 0.3871WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4569 - mse: 0.4314 - mae: 0.3752\n",
            "Epoch 5/50\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 0.4307 - mse: 0.4073 - mae: 0.3697WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4484 - mse: 0.4253 - mae: 0.3654\n",
            "Epoch 6/50\n",
            "117/128 [==========================>...] - ETA: 0s - loss: 0.4371 - mse: 0.4155 - mae: 0.3556WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4333 - mse: 0.4117 - mae: 0.3575\n",
            "Epoch 7/50\n",
            " 94/128 [=====================>........] - ETA: 0s - loss: 0.4445 - mse: 0.4243 - mae: 0.3708WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4429 - mse: 0.4228 - mae: 0.3682\n",
            "Epoch 8/50\n",
            "120/128 [===========================>..] - ETA: 0s - loss: 0.4360 - mse: 0.4173 - mae: 0.3641WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4328 - mse: 0.4142 - mae: 0.3644\n",
            "Epoch 9/50\n",
            "126/128 [============================>.] - ETA: 0s - loss: 0.4260 - mse: 0.4089 - mae: 0.3624WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4271 - mse: 0.4100 - mae: 0.3623\n",
            "Epoch 10/50\n",
            " 83/128 [==================>...........] - ETA: 0s - loss: 0.4763 - mse: 0.4601 - mae: 0.3728WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4247 - mse: 0.4086 - mae: 0.3555\n",
            "Epoch 11/50\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 0.4163 - mse: 0.4009 - mae: 0.3529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4228 - mse: 0.4074 - mae: 0.3604\n",
            "Epoch 12/50\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 0.4431 - mse: 0.4284 - mae: 0.3655WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4262 - mse: 0.4117 - mae: 0.3616\n",
            "Epoch 13/50\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 0.4097 - mse: 0.3957 - mae: 0.3452WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1000us/step - loss: 0.4148 - mse: 0.4009 - mae: 0.3496\n",
            "Epoch 14/50\n",
            "110/128 [========================>.....] - ETA: 0s - loss: 0.4244 - mse: 0.4109 - mae: 0.3694WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4228 - mse: 0.4094 - mae: 0.3637\n",
            "Epoch 15/50\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 0.3695 - mse: 0.3568 - mae: 0.3434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4143 - mse: 0.4017 - mae: 0.3523\n",
            "Epoch 16/50\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 0.4328 - mse: 0.4204 - mae: 0.3545WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 984us/step - loss: 0.4187 - mse: 0.4065 - mae: 0.3562\n",
            "Epoch 17/50\n",
            " 99/128 [======================>.......] - ETA: 0s - loss: 0.4295 - mse: 0.4179 - mae: 0.3616WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4159 - mse: 0.4043 - mae: 0.3579\n",
            "Epoch 18/50\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 0.4433 - mse: 0.4323 - mae: 0.3680WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4188 - mse: 0.4077 - mae: 0.3587\n",
            "Epoch 19/50\n",
            " 94/128 [=====================>........] - ETA: 0s - loss: 0.4303 - mse: 0.4193 - mae: 0.3582WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4169 - mse: 0.4059 - mae: 0.3609\n",
            "Epoch 20/50\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 0.4461 - mse: 0.4353 - mae: 0.3748WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4177 - mse: 0.4069 - mae: 0.3604\n",
            "Epoch 21/50\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 0.4206 - mse: 0.4103 - mae: 0.3515WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4137 - mse: 0.4034 - mae: 0.3547\n",
            "Epoch 22/50\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 0.4395 - mse: 0.4294 - mae: 0.3658WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4159 - mse: 0.4058 - mae: 0.3628\n",
            "Epoch 23/50\n",
            " 96/128 [=====================>........] - ETA: 0s - loss: 0.4249 - mse: 0.4148 - mae: 0.3615WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4118 - mse: 0.4018 - mae: 0.3567\n",
            "Epoch 24/50\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 0.4341 - mse: 0.4244 - mae: 0.3581WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4109 - mse: 0.4012 - mae: 0.3543\n",
            "Epoch 25/50\n",
            "104/128 [=======================>......] - ETA: 0s - loss: 0.4459 - mse: 0.4365 - mae: 0.3726WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4144 - mse: 0.4050 - mae: 0.3632\n",
            "Epoch 26/50\n",
            "117/128 [==========================>...] - ETA: 0s - loss: 0.4080 - mse: 0.3986 - mae: 0.3558WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4086 - mse: 0.3993 - mae: 0.3572\n",
            "Epoch 27/50\n",
            " 95/128 [=====================>........] - ETA: 0s - loss: 0.4196 - mse: 0.4104 - mae: 0.3597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 2ms/step - loss: 0.4134 - mse: 0.4042 - mae: 0.3577\n",
            "Epoch 28/50\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 0.4322 - mse: 0.4229 - mae: 0.3626WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4115 - mse: 0.4023 - mae: 0.3624\n",
            "Epoch 29/50\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 0.4245 - mse: 0.4152 - mae: 0.3510WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4142 - mse: 0.4050 - mae: 0.3548\n",
            "Epoch 30/50\n",
            " 99/128 [======================>.......] - ETA: 0s - loss: 0.4206 - mse: 0.4115 - mae: 0.3548WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4108 - mse: 0.4018 - mae: 0.3571\n",
            "Epoch 31/50\n",
            " 96/128 [=====================>........] - ETA: 0s - loss: 0.4082 - mse: 0.3993 - mae: 0.3509WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4095 - mse: 0.4007 - mae: 0.3557\n",
            "Epoch 32/50\n",
            "102/128 [======================>.......] - ETA: 0s - loss: 0.4225 - mse: 0.4137 - mae: 0.3535WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4120 - mse: 0.4032 - mae: 0.3551\n",
            "Epoch 33/50\n",
            " 98/128 [=====================>........] - ETA: 0s - loss: 0.3986 - mse: 0.3901 - mae: 0.3484WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4124 - mse: 0.4039 - mae: 0.3593\n",
            "Epoch 34/50\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 0.4205 - mse: 0.4120 - mae: 0.3557WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 979us/step - loss: 0.4114 - mse: 0.4028 - mae: 0.3558\n",
            "Epoch 35/50\n",
            "105/128 [=======================>......] - ETA: 0s - loss: 0.4178 - mse: 0.4090 - mae: 0.3566WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 999us/step - loss: 0.4099 - mse: 0.4011 - mae: 0.3559\n",
            "Epoch 36/50\n",
            "103/128 [=======================>......] - ETA: 0s - loss: 0.3688 - mse: 0.3605 - mae: 0.3552WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4080 - mse: 0.3998 - mae: 0.3532\n",
            "Epoch 37/50\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 0.3996 - mse: 0.3912 - mae: 0.3471WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4082 - mse: 0.3997 - mae: 0.3536\n",
            "Epoch 38/50\n",
            " 96/128 [=====================>........] - ETA: 0s - loss: 0.4329 - mse: 0.4243 - mae: 0.3664WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4063 - mse: 0.3976 - mae: 0.3534\n",
            "Epoch 39/50\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 0.4441 - mse: 0.4353 - mae: 0.3777WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4088 - mse: 0.4000 - mae: 0.3659\n",
            "Epoch 40/50\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 0.4068 - mse: 0.3981 - mae: 0.3479WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4089 - mse: 0.4002 - mae: 0.3540\n",
            "Epoch 41/50\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 0.3560 - mse: 0.3472 - mae: 0.3390WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4114 - mse: 0.4027 - mae: 0.3543\n",
            "Epoch 42/50\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 0.4268 - mse: 0.4180 - mae: 0.3603WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4168 - mse: 0.4081 - mae: 0.3670\n",
            "Epoch 43/50\n",
            " 95/128 [=====================>........] - ETA: 0s - loss: 0.4447 - mse: 0.4361 - mae: 0.3713WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4106 - mse: 0.4020 - mae: 0.3573\n",
            "Epoch 44/50\n",
            "108/128 [========================>.....] - ETA: 0s - loss: 0.4226 - mse: 0.4140 - mae: 0.3556WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 955us/step - loss: 0.4077 - mse: 0.3990 - mae: 0.3531\n",
            "Epoch 45/50\n",
            "106/128 [=======================>......] - ETA: 0s - loss: 0.4265 - mse: 0.4179 - mae: 0.3588WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4116 - mse: 0.4030 - mae: 0.3537\n",
            "Epoch 46/50\n",
            "101/128 [======================>.......] - ETA: 0s - loss: 0.4271 - mse: 0.4186 - mae: 0.3540WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4034 - mse: 0.3949 - mae: 0.3509\n",
            "Epoch 47/50\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 0.4161 - mse: 0.4071 - mae: 0.3573WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4136 - mse: 0.4047 - mae: 0.3623\n",
            "Epoch 48/50\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 0.3726 - mse: 0.3637 - mae: 0.3531WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4097 - mse: 0.4008 - mae: 0.3512\n",
            "Epoch 49/50\n",
            "100/128 [======================>.......] - ETA: 0s - loss: 0.4181 - mse: 0.4093 - mae: 0.3580WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4145 - mse: 0.4057 - mae: 0.3612\n",
            "Epoch 50/50\n",
            " 97/128 [=====================>........] - ETA: 0s - loss: 0.3991 - mse: 0.3900 - mae: 0.3478WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "128/128 [==============================] - 0s 1ms/step - loss: 0.4057 - mse: 0.3966 - mae: 0.3541\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x13ff44460>"
            ]
          },
          "execution_count": 480,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    print(f\"Hidden Neurons {optimal_hyperparameters['hidden_neurons'][i]} in Layer {i+1}.\")\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"][i], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=\"mse\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[create_model_checkpoint(model_name=model.name)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544]\n",
            " [-1.13856182]\n",
            " [-1.0407236 ]\n",
            " [-0.92424603]]\n"
          ]
        }
      ],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "\n",
        "    standardised_x = standardised_x.reshape(1,-1)\n",
        "    for i in range(horizon):\n",
        "        #standardised_x = standardised_x.reshape(1,-1)\n",
        "        print(f\"Arr: {np.array([standardised_x[i:i+4]])}, Shape: {np.array([standardised_x[i:i+4]]).shape}\")\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x[0], pred))\n",
        "    return de_standardise(standardised_x[-horizon:])\n",
        "#autoregression(model, train_x[1])\n",
        "print(train_x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544], [-1.13856182], [-1.0407236], [-0.92424603]]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model' (type Sequential).\n    \n    Input 0 of layer \"dense_1077\" is incompatible with the layer: expected axis -1 of input shape to have value 4, but received input with shape (None, 1)\n    \n    Call arguments received by layer 'model' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 1), dtype=float32)\n      • training=False\n      • mask=None\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-491-b4848b38472a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-491-b4848b38472a>\u001b[0m in \u001b[0;36mmake_preds\u001b[0;34m(model, input_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'model' (type Sequential).\n    \n    Input 0 of layer \"dense_1077\" is incompatible with the layer: expected axis -1 of input shape to have value 4, but received input with shape (None, 1)\n    \n    Call arguments received by layer 'model' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 1), dtype=float32)\n      • training=False\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "data =[[-1.21687544],[-1.13856182],[-1.0407236 ],[-0.92424603]]\n",
        "#data = (np.array([[940.66], [1084.86], [1244.98], [1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "print(data)\n",
        "def make_preds(model, input_data):\n",
        "  forecast = model.predict(input_data)\n",
        "  preds = tf.squeeze(forecast)\n",
        "  return preds\n",
        "\n",
        "pred = make_preds(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 372,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([[940.66, 1084.86, 1244.98, 1445.02]]).shape\n",
        "#autoregression(model, test2_x[1], 1)\n",
        "#standardised_x = np.array(standardise(test2_x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1084.86, 1244.98, 1445.02])"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array = np.array([[940.66, 1084.86, 1244.98, 1445.02]])\n",
        "array[0][1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInanWFa8qIZ",
        "outputId": "c2d65ad7-6fe9-45ff-8898-20b21197b221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]]\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds7L0_rE-g4Z",
        "outputId": "a86889ff-fd57-49ec-c83d-c52f549b126c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1800.24010693])"
            ]
          },
          "execution_count": 314,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inversed = de_standardise(np.array(pred))\n",
        "inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
