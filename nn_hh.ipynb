{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 635,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# De-trend (each time series at a time)\n",
        "data = pd.DataFrame(df_micro.iloc[0])\n",
        "data.columns = [\"value\"]\n",
        "year = np.arange(0, 20)\n",
        "data['year'] = year\n",
        "data = data.set_index('year')\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(data['value'], model='additive', period=10)\n",
        "\n",
        "# Access the components of the decomposition\n",
        "trend = decomposition.trend\n",
        "#seasonal = decomposition.seasonal\n",
        "#residual = decomposition.resid\n",
        "test2 = pd.DataFrame(trend).plot()\n",
        "trend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 636,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "# Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 637,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows.reshape(-1,4), labels.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 638,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_test.to_numpy().reshape(-1,1)\n",
        "df_test = pd.DataFrame(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 647,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x, train_y = make_windows(df_train.to_numpy(), window_size=4, horizon=1)\n",
        "test2_x, test2_y = make_windows(df_test.to_numpy(), window_size=4, horizon=1)\n",
        "train_x_2 = train_x + 2\n",
        "train_x_2 = np.log(train_x_2)\n",
        "train_y_2 = train_y + 2\n",
        "train_y_2 = np.log(train_y_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 640,
      "metadata": {
        "id": "1J7pKjWMmxVN"
      },
      "outputs": [],
      "source": [
        "# Create a function to implement a ModelCheckpoint callback\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            verbose=0,\n",
        "                                            save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 641,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "def evaluate_mdape(y_true, y_pred):\n",
        " return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def calculate_average_rankings(y_true, y_pred):\n",
        "    num_series = len(y_pred)\n",
        "    num_methods = len(y_pred[0])\n",
        "\n",
        "    ranks = []  # to store ranks for each series\n",
        "\n",
        "    for series_index in range(num_series):\n",
        "        sape_values = [\n",
        "            abs((y_true[series_index] - forecast) / y_true[series_index]) * 100\n",
        "            for forecast in y_pred[series_index]\n",
        "        ]\n",
        "        sorted_sape = sorted(sape_values)  # sort SAPE values in ascending order\n",
        "        series_ranks = [sorted_sape.index(sape) + 1 for sape in sape_values]  # assign ranks to SAPE values\n",
        "        ranks.append(series_ranks)\n",
        "\n",
        "    mean_ranks = []  # to store mean ranks for each forecasting method\n",
        "\n",
        "    for method_index in range(num_methods):\n",
        "        total_rank = sum(ranks[series_index][method_index] for series_index in range(num_series))\n",
        "        mean_rank = total_rank / num_series\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    return mean_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 642,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 643,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true_set, y_pred_set):\n",
        "    # Average Ranking\n",
        "    avg_ranking = None\n",
        "    # Percentage Better\n",
        "    percentage_better = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 644,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 645,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "regularizations = np.array([0.001, 0.01, 0.1])\n",
        "hidden_layers = np.array([2, 3, 4, 5, 6, 10])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1454c4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x146fb3d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 2ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "3/3 [==============================] - 0s 1ms/step\n",
            "Current mean SMAPE: 22.07166089233959, Current hyperparameters: {'learning_rate': 0.01, 'batch_size': 16, 'regularization': 0.001, 'hidden_neurons': [3, 6], 'hidden_layers': 2}\n",
            "Best Hyperparameters: {'learning_rate': 0.01, 'batch_size': 16, 'regularization': 0.001, 'hidden_neurons': [3, 6], 'hidden_layers': 2}\n",
            "Best SMAPE Score: 22.07166089233959\n",
            "Best MDAPE Score: 12.31710780128442\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=24)\n",
        "\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    best_smape = float('inf')\n",
        "    best_hyperparameters = {}\n",
        "    hidden_neurons = np.arange(2, 9)\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, test_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, test_y_cv = train_y[train_index], train_y[test_index]\n",
        "        \n",
        "\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            random_neuron = random.choice(hidden_neurons)\n",
        "            chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(random_neuron, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mse\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "        predictions = model_cv.predict(test_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(test_y_cv), de_standardise(predictions))\n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        \n",
        "    mean_smape = np.mean(smape_scores)\n",
        "    mean_mdape = np.mean(mdape_scores)\n",
        "    hyperparameters = {\n",
        "        'learning_rate': combination.learning_rate,\n",
        "        'batch_size': combination.batch_size,\n",
        "        'regularization': combination.regularization,\n",
        "        'hidden_neurons': chosen_hidden_neurons,\n",
        "        'hidden_layers': combination.hidden_layers\n",
        "    }\n",
        "    print(f\"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}\")\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 1)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 622,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | batch_... | hidden... | hidden... | learni... | regula... |\n",
            "-------------------------------------------------------------------------------------\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "11/11 [==============================] - 0s 954us/step\n",
            "11/11 [==============================] - 0s 867us/step\n",
            "11/11 [==============================] - 0s 828us/step\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-24.15   \u001b[0m | \u001b[0m68.74    \u001b[0m | \u001b[0m1.331    \u001b[0m | \u001b[0m3.294    \u001b[0m | \u001b[0m0.06519  \u001b[0m | \u001b[0m0.01557  \u001b[0m |\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 989us/step\n",
            "11/11 [==============================] - 0s 793us/step\n",
            "11/11 [==============================] - 0s 911us/step\n",
            "11/11 [==============================] - 0s 996us/step\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-29.05   \u001b[0m | \u001b[0m122.6    \u001b[0m | \u001b[0m2.582    \u001b[0m | \u001b[0m3.792    \u001b[0m | \u001b[0m0.06996  \u001b[0m | \u001b[0m0.08723  \u001b[0m |\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 15ms/step\n",
            "11/11 [==============================] - 0s 4ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-27.18   \u001b[0m | \u001b[0m21.83    \u001b[0m | \u001b[0m3.267    \u001b[0m | \u001b[0m8.486    \u001b[0m | \u001b[0m0.002729 \u001b[0m | \u001b[0m0.06609  \u001b[0m |\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-29.19   \u001b[0m | \u001b[0m73.91    \u001b[0m | \u001b[0m3.458    \u001b[0m | \u001b[0m8.933    \u001b[0m | \u001b[0m0.04349  \u001b[0m | \u001b[0m0.07644  \u001b[0m |\n",
            "11/11 [==============================] - 0s 883us/step\n",
            "11/11 [==============================] - 0s 992us/step\n",
            "11/11 [==============================] - 0s 881us/step\n",
            "11/11 [==============================] - 0s 793us/step\n",
            "11/11 [==============================] - 0s 890us/step\n",
            "| \u001b[95m5        \u001b[0m | \u001b[95m-23.98   \u001b[0m | \u001b[95m75.38    \u001b[0m | \u001b[95m1.923    \u001b[0m | \u001b[95m9.856    \u001b[0m | \u001b[95m0.0745   \u001b[0m | \u001b[95m0.01713  \u001b[0m |\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 930us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-32.98   \u001b[0m | \u001b[0m50.66    \u001b[0m | \u001b[0m4.205    \u001b[0m | \u001b[0m2.405    \u001b[0m | \u001b[0m0.02365  \u001b[0m | \u001b[0m0.06016  \u001b[0m |\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-25.39   \u001b[0m | \u001b[0m64.8     \u001b[0m | \u001b[0m3.521    \u001b[0m | \u001b[0m6.997    \u001b[0m | \u001b[0m0.01076  \u001b[0m | \u001b[0m0.04106  \u001b[0m |\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-50.52   \u001b[0m | \u001b[0m64.95    \u001b[0m | \u001b[0m4.39     \u001b[0m | \u001b[0m3.998    \u001b[0m | \u001b[0m0.08694  \u001b[0m | \u001b[0m0.07278  \u001b[0m |\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 956us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 3ms/step\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-28.12   \u001b[0m | \u001b[0m127.7    \u001b[0m | \u001b[0m2.677    \u001b[0m | \u001b[0m3.768    \u001b[0m | \u001b[0m0.0546   \u001b[0m | \u001b[0m0.086    \u001b[0m |\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-24.53   \u001b[0m | \u001b[0m76.04    \u001b[0m | \u001b[0m4.568    \u001b[0m | \u001b[0m9.754    \u001b[0m | \u001b[0m0.01317  \u001b[0m | \u001b[0m0.03489  \u001b[0m |\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 995us/step\n",
            "11/11 [==============================] - 0s 977us/step\n",
            "11/11 [==============================] - 0s 811us/step\n",
            "11/11 [==============================] - 0s 785us/step\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-28.6    \u001b[0m | \u001b[0m75.87    \u001b[0m | \u001b[0m3.131    \u001b[0m | \u001b[0m9.31     \u001b[0m | \u001b[0m0.03308  \u001b[0m | \u001b[0m0.09039  \u001b[0m |\n",
            "11/11 [==============================] - 0s 846us/step\n",
            "11/11 [==============================] - 0s 992us/step\n",
            "11/11 [==============================] - 0s 747us/step\n",
            "11/11 [==============================] - 0s 872us/step\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-24.01   \u001b[0m | \u001b[0m70.22    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m3.161    \u001b[0m | \u001b[0m0.0564   \u001b[0m | \u001b[0m0.0001   \u001b[0m |\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Queue is empty, no more objects to retrieve.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-622-cc5ead51610d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mbest_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, utility_function)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_constrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 self.constraint.fit(self._space.params,\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mtheta_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                     optima.append(\n\u001b[0;32m--> 304\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constrained_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                     )\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# Select result from run with minimal (negative) log-marginal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_constrained_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fmin_l_bfgs_b\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             opt_res = scipy.optimize.minimize(\n\u001b[0m\u001b[1;32m    623\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                  **options)\n\u001b[1;32m    695\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    697\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    698\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0miprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    306\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    333\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    best_smape = float('inf')\n",
        "    best_hyperparameters = {}\n",
        "    hidden_neurons = np.arange(2, 20)\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, test_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, test_y_cv = train_y[train_index], train_y[test_index]\n",
        "        \n",
        "\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            model_cv.add(tf.keras.layers.Dense(combination.hidden_neurons, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mse\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "        predictions = model_cv.predict(test_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(test_y_cv), de_standardise(predictions))\n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        \n",
        "        mean_smape = np.mean(smape_scores)\n",
        "        mean_mdape = np.mean(mdape_scores)\n",
        "        hyperparameters = {\n",
        "            'learning_rate': combination.learning_rate,\n",
        "            'batch_size': combination.batch_size,\n",
        "            'regularization': combination.regularization,\n",
        "            'hidden_neurons': combination.hidden_neurons,\n",
        "            'hidden_layers': combination.hidden_layers\n",
        "        }\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers hidden_neurons\")\n",
        "\n",
        "def objective(learning_rate, batch_size, regularization, hidden_layers, hidden_neurons):\n",
        "    # Convert hyperparameters to appropriate types\n",
        "    learning_rate = float(learning_rate)\n",
        "    batch_size = int(batch_size)\n",
        "    regularization = float(regularization)\n",
        "    hidden_layers = int(hidden_layers)\n",
        "    hidden_neurons = int(hidden_neurons)\n",
        "\n",
        "    # Define the combination object with the selected hyperparameters\n",
        "    combination = Combination(learning_rate, batch_size, regularization, hidden_layers, hidden_neurons)\n",
        "\n",
        "    # Call the cross_validation function with the selected hyperparameters\n",
        "    smape, mdape, _ = cross_validation(combination)\n",
        "\n",
        "    # Return the negative SMAPE score (Bayesian optimization maximizes the objective function)\n",
        "    return -smape\n",
        "\n",
        "pbounds = {'learning_rate': (0.0001, 0.1),\n",
        "           'batch_size': (16, 128),\n",
        "           'regularization': (0.0001, 0.1),\n",
        "           'hidden_layers': (1, 5),\n",
        "           'hidden_neurons': (2,10)}\n",
        "\n",
        "optimizer = BayesianOptimization(f=objective, pbounds=pbounds)\n",
        "optimizer.maximize(init_points=10, n_iter=10)\n",
        "\n",
        "best_hyperparameters = optimizer.max['params']\n",
        "best_smape = -optimizer.max['target']\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", best_smape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 551,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.01\n",
            "Batch Size: 32\n",
            "Hidden Neurons 2 in Layer 1.\n",
            "Hidden Neurons 7 in Layer 2.\n",
            "\n",
            "Epoch 1/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.8392 - mse: 0.8240 - mae: 0.6140 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 1s 1ms/step - loss: 0.7296 - mse: 0.7151 - mae: 0.5659\n",
            "Epoch 2/50\n",
            "36/64 [===============>..............] - ETA: 0s - loss: 0.6245 - mse: 0.6122 - mae: 0.5046WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5776 - mse: 0.5656 - mae: 0.4831\n",
            "Epoch 3/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.5208 - mse: 0.5098 - mae: 0.4278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4935 - mse: 0.4827 - mae: 0.4201\n",
            "Epoch 4/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.4128 - mse: 0.4026 - mae: 0.3790WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4504 - mse: 0.4403 - mae: 0.3729\n",
            "Epoch 5/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4141 - mse: 0.4047 - mae: 0.3710WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4327 - mse: 0.4234 - mae: 0.3613\n",
            "Epoch 6/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.4382 - mse: 0.4295 - mae: 0.3538WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4216 - mse: 0.4130 - mae: 0.3547\n",
            "Epoch 7/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.4497 - mse: 0.4417 - mae: 0.3723WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4218 - mse: 0.4138 - mae: 0.3624\n",
            "Epoch 8/50\n",
            "35/64 [===============>..............] - ETA: 0s - loss: 0.4123 - mse: 0.4046 - mae: 0.3416WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4139 - mse: 0.4064 - mae: 0.3530\n",
            "Epoch 9/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.4609 - mse: 0.4538 - mae: 0.3678WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4152 - mse: 0.4082 - mae: 0.3590\n",
            "Epoch 10/50\n",
            "40/64 [=================>............] - ETA: 0s - loss: 0.4682 - mse: 0.4614 - mae: 0.3754WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4133 - mse: 0.4066 - mae: 0.3583\n",
            "Epoch 11/50\n",
            "34/64 [==============>...............] - ETA: 0s - loss: 0.4658 - mse: 0.4595 - mae: 0.3619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4115 - mse: 0.4053 - mae: 0.3520\n",
            "Epoch 12/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4210 - mse: 0.4150 - mae: 0.3564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4103 - mse: 0.4044 - mae: 0.3582\n",
            "Epoch 13/50\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4138 - mse: 0.4081 - mae: 0.3493WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.4074 - mse: 0.4016 - mae: 0.3489\n",
            "Epoch 14/50\n",
            "54/64 [========================>.....] - ETA: 0s - loss: 0.4143 - mse: 0.4088 - mae: 0.3607WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 968us/step - loss: 0.4100 - mse: 0.4045 - mae: 0.3557\n",
            "Epoch 15/50\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.4068 - mse: 0.4015 - mae: 0.3498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 861us/step - loss: 0.4074 - mse: 0.4021 - mae: 0.3507\n",
            "Epoch 16/50\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4048 - mse: 0.3997 - mae: 0.3488WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 880us/step - loss: 0.4071 - mse: 0.4020 - mae: 0.3510\n",
            "Epoch 17/50\n",
            "56/64 [=========================>....] - ETA: 0s - loss: 0.4207 - mse: 0.4157 - mae: 0.3529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 952us/step - loss: 0.4067 - mse: 0.4018 - mae: 0.3485\n",
            "Epoch 18/50\n",
            "60/64 [===========================>..] - ETA: 0s - loss: 0.4143 - mse: 0.4095 - mae: 0.3503WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 901us/step - loss: 0.4064 - mse: 0.4017 - mae: 0.3477\n",
            "Epoch 19/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4056 - mse: 0.4009 - mae: 0.3487WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 880us/step - loss: 0.4084 - mse: 0.4038 - mae: 0.3511\n",
            "Epoch 20/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4065 - mse: 0.4019 - mae: 0.3563WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 864us/step - loss: 0.4095 - mse: 0.4049 - mae: 0.3572\n",
            "Epoch 21/50\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4053 - mse: 0.4008 - mae: 0.3460WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 850us/step - loss: 0.4053 - mse: 0.4008 - mae: 0.3460\n",
            "Epoch 22/50\n",
            "53/64 [=======================>......] - ETA: 0s - loss: 0.4249 - mse: 0.4205 - mae: 0.3568WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4082 - mse: 0.4038 - mae: 0.3543\n",
            "Epoch 23/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.4355 - mse: 0.4311 - mae: 0.3610WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4088 - mse: 0.4044 - mae: 0.3572\n",
            "Epoch 24/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4075 - mse: 0.4033 - mae: 0.3501WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 868us/step - loss: 0.4073 - mse: 0.4030 - mae: 0.3506\n",
            "Epoch 25/50\n",
            "57/64 [=========================>....] - ETA: 0s - loss: 0.4243 - mse: 0.4200 - mae: 0.3588WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4087 - mse: 0.4044 - mae: 0.3552\n",
            "Epoch 26/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.3956 - mse: 0.3913 - mae: 0.3590WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4081 - mse: 0.4039 - mae: 0.3542\n",
            "Epoch 27/50\n",
            "33/64 [==============>...............] - ETA: 0s - loss: 0.3439 - mse: 0.3396 - mae: 0.3284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4061 - mse: 0.4019 - mae: 0.3485\n",
            "Epoch 28/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.4287 - mse: 0.4245 - mae: 0.3501WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4067 - mse: 0.4025 - mae: 0.3513\n",
            "Epoch 29/50\n",
            "42/64 [==================>...........] - ETA: 0s - loss: 0.4344 - mse: 0.4302 - mae: 0.3433WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4042 - mse: 0.4000 - mae: 0.3441\n",
            "Epoch 30/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.4547 - mse: 0.4506 - mae: 0.3649WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4062 - mse: 0.4020 - mae: 0.3524\n",
            "Epoch 31/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4114 - mse: 0.4072 - mae: 0.3528WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4056 - mse: 0.4014 - mae: 0.3518\n",
            "Epoch 32/50\n",
            "43/64 [===================>..........] - ETA: 0s - loss: 0.4311 - mse: 0.4271 - mae: 0.3540WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4035 - mse: 0.3995 - mae: 0.3485\n",
            "Epoch 33/50\n",
            "43/64 [===================>..........] - ETA: 0s - loss: 0.3481 - mse: 0.3440 - mae: 0.3321WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4049 - mse: 0.4009 - mae: 0.3492\n",
            "Epoch 34/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.3396 - mse: 0.3355 - mae: 0.3350WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4040 - mse: 0.4000 - mae: 0.3485\n",
            "Epoch 35/50\n",
            "41/64 [==================>...........] - ETA: 0s - loss: 0.4041 - mse: 0.4000 - mae: 0.3401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4037 - mse: 0.3996 - mae: 0.3438\n",
            "Epoch 36/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.3584 - mse: 0.3544 - mae: 0.3434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4043 - mse: 0.4003 - mae: 0.3480\n",
            "Epoch 37/50\n",
            "33/64 [==============>...............] - ETA: 0s - loss: 0.3283 - mse: 0.3243 - mae: 0.3278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4043 - mse: 0.4003 - mae: 0.3470\n",
            "Epoch 38/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.3991 - mse: 0.3952 - mae: 0.3668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4051 - mse: 0.4012 - mae: 0.3491\n",
            "Epoch 39/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.4448 - mse: 0.4408 - mae: 0.3590WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4054 - mse: 0.4015 - mae: 0.3520\n",
            "Epoch 40/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4025 - mse: 0.3986 - mae: 0.3395WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4054 - mse: 0.4015 - mae: 0.3462\n",
            "Epoch 41/50\n",
            "51/64 [======================>.......] - ETA: 0s - loss: 0.3441 - mse: 0.3402 - mae: 0.3291WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4032 - mse: 0.3993 - mae: 0.3448\n",
            "Epoch 42/50\n",
            "41/64 [==================>...........] - ETA: 0s - loss: 0.4082 - mse: 0.4043 - mae: 0.3339WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4053 - mse: 0.4015 - mae: 0.3495\n",
            "Epoch 43/50\n",
            "47/64 [=====================>........] - ETA: 0s - loss: 0.4411 - mse: 0.4373 - mae: 0.3619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4043 - mse: 0.4005 - mae: 0.3476\n",
            "Epoch 44/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4444 - mse: 0.4405 - mae: 0.3614WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4056 - mse: 0.4017 - mae: 0.3522\n",
            "Epoch 45/50\n",
            "47/64 [=====================>........] - ETA: 0s - loss: 0.4352 - mse: 0.4314 - mae: 0.3562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4047 - mse: 0.4009 - mae: 0.3452\n",
            "Epoch 46/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4368 - mse: 0.4330 - mae: 0.3574WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4036 - mse: 0.3998 - mae: 0.3479\n",
            "Epoch 47/50\n",
            "51/64 [======================>.......] - ETA: 0s - loss: 0.4114 - mse: 0.4075 - mae: 0.3500WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4066 - mse: 0.4028 - mae: 0.3500\n",
            "Epoch 48/50\n",
            "48/64 [=====================>........] - ETA: 0s - loss: 0.3654 - mse: 0.3616 - mae: 0.3480WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4037 - mse: 0.3999 - mae: 0.3464\n",
            "Epoch 49/50\n",
            "49/64 [=====================>........] - ETA: 0s - loss: 0.4122 - mse: 0.4085 - mae: 0.3448WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4067 - mse: 0.4030 - mae: 0.3538\n",
            "Epoch 50/50\n",
            "50/64 [======================>.......] - ETA: 0s - loss: 0.3974 - mse: 0.3936 - mae: 0.3432WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4045 - mse: 0.4008 - mae: 0.3479\n",
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]\n",
            " [-1.21687544 -1.13856182 -1.0407236  -0.92424603]\n",
            " [-1.13856182 -1.0407236  -0.92424603 -0.75062769]\n",
            " ...\n",
            " [-0.61201865 -0.67357104 -0.27405519 -0.33851768]\n",
            " [-0.67357104 -0.27405519 -0.33851768 -0.50434476]\n",
            " [-0.27405519 -0.33851768 -0.50434476 -0.27173199]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    print(f\"Hidden Neurons {optimal_hyperparameters['hidden_neurons'][i]} in Layer {i+1}.\")\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"][i], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=\"mse\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[create_model_checkpoint(model_name=model.name)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 552,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([4802.48985437])"
            ]
          },
          "execution_count": 552,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return de_standardise(standardised_x[-horizon:])\n",
        "\n",
        "autoregression(model, np.array([4793.2, 5602, 5065, 5056]), 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544 -1.13856182 -1.0407236  -0.92424603]]\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "data = (np.array([[940.66, 1084.86, 1244.98, 1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "print(data.shape)\n",
        "#def make_preds(model, input_data):\n",
        "#  forecast = model.predict(input_data)\n",
        "#  preds = tf.squeeze(forecast)\n",
        "#  return preds\n",
        "\n",
        "#pred = make_preds(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 511,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([[940.66, 1084.86, 1244.98, 1445.02]]).shape\n",
        "#autoregression(model, test2_x[1], 1)\n",
        "#standardised_x = np.array(standardise(test2_x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1084.86, 1244.98, 1445.02])"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array = np.array([[940.66, 1084.86, 1244.98, 1445.02]])\n",
        "array[0][1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInanWFa8qIZ",
        "outputId": "c2d65ad7-6fe9-45ff-8898-20b21197b221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]]\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds7L0_rE-g4Z",
        "outputId": "a86889ff-fd57-49ec-c83d-c52f549b126c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1897.52436208])"
            ]
          },
          "execution_count": 512,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inversed = de_standardise(np.array(pred))\n",
        "inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
