{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# De-trend (each time series at a time)\n",
        "data = pd.DataFrame(df_micro.iloc[0])\n",
        "data.columns = [\"value\"]\n",
        "year = np.arange(0, 20)\n",
        "data['year'] = year\n",
        "data = data.set_index('year')\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(data['value'], model='additive', period=10)\n",
        "\n",
        "# Access the components of the decomposition\n",
        "trend = decomposition.trend\n",
        "#seasonal = decomposition.seasonal\n",
        "#residual = decomposition.resid\n",
        "test2 = pd.DataFrame(trend).plot()\n",
        "trend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "# Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_test.to_numpy().reshape(-1,1)\n",
        "df_test = pd.DataFrame(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x, train_y = make_windows(df_train.to_numpy(), window_size=4, horizon=1)\n",
        "test2_x, test2_y = make_windows(df_test.to_numpy(), window_size=4, horizon=1)\n",
        "train_x = train_x.reshape(-1,4)\n",
        "train_y = train_y.reshape(-1,1)\n",
        "test2_x = test2_x.reshape(-1,4)\n",
        "test2_y = test2_y.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {
        "id": "1J7pKjWMmxVN"
      },
      "outputs": [],
      "source": [
        "# Create a function to implement a ModelCheckpoint callback\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            verbose=0,\n",
        "                                            save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "def evaluate_mdape(y_true, y_pred):\n",
        " return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def calculate_average_rankings(y_true, y_pred):\n",
        "    num_series = len(y_pred)\n",
        "    num_methods = len(y_pred[0])\n",
        "\n",
        "    ranks = []  # to store ranks for each series\n",
        "\n",
        "    for series_index in range(num_series):\n",
        "        sape_values = [\n",
        "            abs((y_true[series_index] - forecast) / y_true[series_index]) * 100\n",
        "            for forecast in y_pred[series_index]\n",
        "        ]\n",
        "        sorted_sape = sorted(sape_values)  # sort SAPE values in ascending order\n",
        "        series_ranks = [sorted_sape.index(sape) + 1 for sape in sape_values]  # assign ranks to SAPE values\n",
        "        ranks.append(series_ranks)\n",
        "\n",
        "    mean_ranks = []  # to store mean ranks for each forecasting method\n",
        "\n",
        "    for method_index in range(num_methods):\n",
        "        total_rank = sum(ranks[series_index][method_index] for series_index in range(num_series))\n",
        "        mean_rank = total_rank / num_series\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    return mean_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true_set, y_pred_set):\n",
        "    # Average Ranking\n",
        "    avg_ranking = None\n",
        "    # Percentage Better\n",
        "    percentage_better = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "regularizations = np.array([0.001, 0.01, 0.1])\n",
        "hidden_layers = np.array([2, 3, 4, 5, 6, 10])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 0s 997us/step\n",
            "11/11 [==============================] - 0s 903us/step\n",
            "11/11 [==============================] - 0s 913us/step\n",
            "11/11 [==============================] - 0s 853us/step\n",
            "11/11 [==============================] - 0s 917us/step\n",
            "Best Hyperparameters: {'learning_rate': 0.1, 'batch_size': 64, 'regularization': 0.001, 'hidden_neurons': [4, 5], 'hidden_layers': 2}\n",
            "Best SMAPE Score: 17.6887355441468\n",
            "Best MDAPE Score: 8.289270636613685\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    best_smape = float('inf')\n",
        "    best_hyperparameters = {}\n",
        "    hidden_neurons = np.arange(2, 9)\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, test_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, test_y_cv = train_y[train_index], train_y[test_index]\n",
        "        \n",
        "\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            random_neuron = random.choice(hidden_neurons)\n",
        "            chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(random_neuron, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mse\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "        predictions = model_cv.predict(test_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(test_y_cv), de_standardise(predictions))\n",
        "\n",
        "        if smape_score < best_smape:\n",
        "            best_smape = smape_score\n",
        "            best_mdape = mdape_score\n",
        "            best_hyperparameters = {\n",
        "                'learning_rate': combination.learning_rate,\n",
        "                'batch_size': combination.batch_size,\n",
        "                'regularization': combination.regularization,\n",
        "                'hidden_neurons': chosen_hidden_neurons,\n",
        "                'hidden_layers': combination.hidden_layers\n",
        "            }\n",
        "    return best_smape, best_mdape, best_hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 2)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.1\n",
            "Batch Size: 64\n",
            "Hidden Neurons 4 in Layer 1.\n",
            "Hidden Neurons 5 in Layer 2.\n",
            "\n",
            "Epoch 1/50\n",
            " 1/32 [..............................] - ETA: 20s - loss: 0.9685 - mse: 0.9550 - mae: 0.8186WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 1s 1ms/step - loss: 0.6313 - mse: 0.6189 - mae: 0.4967\n",
            "Epoch 2/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2302 - mse: 0.2170 - mae: 0.3061WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4778 - mse: 0.4661 - mae: 0.4196\n",
            "Epoch 3/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3500 - mse: 0.3379 - mae: 0.3804WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4694 - mse: 0.4571 - mae: 0.3974\n",
            "Epoch 4/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.1984 - mse: 0.1866 - mae: 0.2686WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4551 - mse: 0.4440 - mae: 0.3856\n",
            "Epoch 5/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4198 - mse: 0.4101 - mae: 0.4553WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4581 - mse: 0.4484 - mae: 0.3914\n",
            "Epoch 6/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.8615 - mse: 0.8528 - mae: 0.5195WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4268 - mse: 0.4174 - mae: 0.3691\n",
            "Epoch 7/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.5763 - mse: 0.5671 - mae: 0.4953WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4572 - mse: 0.4480 - mae: 0.4008\n",
            "Epoch 8/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 1.5131 - mse: 1.5037 - mae: 0.5618WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4391 - mse: 0.4296 - mae: 0.3811\n",
            "Epoch 9/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3199 - mse: 0.3114 - mae: 0.4097WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4455 - mse: 0.4374 - mae: 0.4083\n",
            "Epoch 10/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3501 - mse: 0.3420 - mae: 0.3788WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4305 - mse: 0.4228 - mae: 0.3700\n",
            "Epoch 11/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3347 - mse: 0.3275 - mae: 0.3545WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4345 - mse: 0.4266 - mae: 0.3861\n",
            "Epoch 12/50\n",
            "23/32 [====================>.........] - ETA: 0s - loss: 0.4405 - mse: 0.4324 - mae: 0.3824WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4333 - mse: 0.4253 - mae: 0.3882\n",
            "Epoch 13/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3775 - mse: 0.3697 - mae: 0.3641WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4148 - mse: 0.4074 - mae: 0.3583\n",
            "Epoch 14/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.1906 - mse: 0.1836 - mae: 0.2579WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4318 - mse: 0.4248 - mae: 0.3928\n",
            "Epoch 15/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.1225 - mse: 0.1150 - mae: 0.2258WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4266 - mse: 0.4194 - mae: 0.3803\n",
            "Epoch 16/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2461 - mse: 0.2392 - mae: 0.2853WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4160 - mse: 0.4092 - mae: 0.3566\n",
            "Epoch 17/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.5308 - mse: 0.5244 - mae: 0.4080WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4272 - mse: 0.4210 - mae: 0.3730\n",
            "Epoch 18/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2617 - mse: 0.2557 - mae: 0.3014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4259 - mse: 0.4199 - mae: 0.3751\n",
            "Epoch 19/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4485 - mse: 0.4422 - mae: 0.3336WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4184 - mse: 0.4121 - mae: 0.3628\n",
            "Epoch 20/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3251 - mse: 0.3187 - mae: 0.3352WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4214 - mse: 0.4148 - mae: 0.3669\n",
            "Epoch 21/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4215 - mse: 0.4150 - mae: 0.4061WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4141 - mse: 0.4081 - mae: 0.3541\n",
            "Epoch 22/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4669 - mse: 0.4610 - mae: 0.4110WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4400 - mse: 0.4344 - mae: 0.4004\n",
            "Epoch 23/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.7548 - mse: 0.7492 - mae: 0.5245WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4497 - mse: 0.4434 - mae: 0.4118\n",
            "Epoch 24/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3859 - mse: 0.3792 - mae: 0.3316WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4200 - mse: 0.4133 - mae: 0.3608\n",
            "Epoch 25/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.6154 - mse: 0.6090 - mae: 0.4975WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4235 - mse: 0.4174 - mae: 0.3732\n",
            "Epoch 26/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.5186 - mse: 0.5126 - mae: 0.4274WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4264 - mse: 0.4202 - mae: 0.3756\n",
            "Epoch 27/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2528 - mse: 0.2461 - mae: 0.3032WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4233 - mse: 0.4171 - mae: 0.3763\n",
            "Epoch 28/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4721 - mse: 0.4655 - mae: 0.3897WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4421 - mse: 0.4357 - mae: 0.4091\n",
            "Epoch 29/50\n",
            "24/32 [=====================>........] - ETA: 0s - loss: 0.4333 - mse: 0.4268 - mae: 0.3567WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4186 - mse: 0.4122 - mae: 0.3576\n",
            "Epoch 30/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 1.8770 - mse: 1.8703 - mae: 0.5334WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4190 - mse: 0.4128 - mae: 0.3627\n",
            "Epoch 31/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.7582 - mse: 0.7525 - mae: 0.5957WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4200 - mse: 0.4139 - mae: 0.3744\n",
            "Epoch 32/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3157 - mse: 0.3100 - mae: 0.3388WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4198 - mse: 0.4138 - mae: 0.3643\n",
            "Epoch 33/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4598 - mse: 0.4541 - mae: 0.3106WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4337 - mse: 0.4279 - mae: 0.3953\n",
            "Epoch 34/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3399 - mse: 0.3339 - mae: 0.3284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4322 - mse: 0.4260 - mae: 0.3880\n",
            "Epoch 35/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2602 - mse: 0.2539 - mae: 0.2954WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4158 - mse: 0.4095 - mae: 0.3620\n",
            "Epoch 36/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2693 - mse: 0.2635 - mae: 0.3337WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4218 - mse: 0.4161 - mae: 0.3785\n",
            "Epoch 37/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.5029 - mse: 0.4975 - mae: 0.3988WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4187 - mse: 0.4128 - mae: 0.3690\n",
            "Epoch 38/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3451 - mse: 0.3385 - mae: 0.3517WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4231 - mse: 0.4163 - mae: 0.3696\n",
            "Epoch 39/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2766 - mse: 0.2698 - mae: 0.3456WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4272 - mse: 0.4206 - mae: 0.3889\n",
            "Epoch 40/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3891 - mse: 0.3823 - mae: 0.3943WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4281 - mse: 0.4211 - mae: 0.3750\n",
            "Epoch 41/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4811 - mse: 0.4741 - mae: 0.4647WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4160 - mse: 0.4085 - mae: 0.3614\n",
            "Epoch 42/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4962 - mse: 0.4890 - mae: 0.4680WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4265 - mse: 0.4199 - mae: 0.3795\n",
            "Epoch 43/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.5285 - mse: 0.5219 - mae: 0.4268WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4165 - mse: 0.4095 - mae: 0.3583\n",
            "Epoch 44/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.6448 - mse: 0.6378 - mae: 0.4745WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4251 - mse: 0.4176 - mae: 0.3756\n",
            "Epoch 45/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2995 - mse: 0.2919 - mae: 0.3155WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4204 - mse: 0.4129 - mae: 0.3593\n",
            "Epoch 46/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4480 - mse: 0.4402 - mae: 0.3844WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4148 - mse: 0.4074 - mae: 0.3620\n",
            "Epoch 47/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.2908 - mse: 0.2837 - mae: 0.3484WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4300 - mse: 0.4226 - mae: 0.3837\n",
            "Epoch 48/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3380 - mse: 0.3306 - mae: 0.4058WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4216 - mse: 0.4140 - mae: 0.3697\n",
            "Epoch 49/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.4105 - mse: 0.4034 - mae: 0.3476WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4244 - mse: 0.4179 - mae: 0.3697\n",
            "Epoch 50/50\n",
            " 1/32 [..............................] - ETA: 0s - loss: 0.3921 - mse: 0.3853 - mae: 0.3385WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "32/32 [==============================] - 0s 1ms/step - loss: 0.4189 - mse: 0.4124 - mae: 0.3675\n",
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]\n",
            " [-1.21687544 -1.13856182 -1.0407236  -0.92424603]\n",
            " [-1.13856182 -1.0407236  -0.92424603 -0.75062769]\n",
            " ...\n",
            " [-0.61201865 -0.67357104 -0.27405519 -0.33851768]\n",
            " [-0.67357104 -0.27405519 -0.33851768 -0.50434476]\n",
            " [-0.27405519 -0.33851768 -0.50434476 -0.27173199]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    print(f\"Hidden Neurons {optimal_hyperparameters['hidden_neurons'][i]} in Layer {i+1}.\")\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"][i], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=\"mse\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[create_model_checkpoint(model_name=model.name)])\n",
        "print(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 549,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 143ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([4765.73871794])"
            ]
          },
          "execution_count": 549,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return de_standardise(standardised_x[-horizon:])\n",
        "\n",
        "autoregression(model, np.array([4793.2, 5602, 5065, 5056]), 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544 -1.13856182 -1.0407236  -0.92424603]]\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "data = (np.array([[940.66, 1084.86, 1244.98, 1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "print(data.shape)\n",
        "#def make_preds(model, input_data):\n",
        "#  forecast = model.predict(input_data)\n",
        "#  preds = tf.squeeze(forecast)\n",
        "#  return preds\n",
        "\n",
        "#pred = make_preds(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 511,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([[940.66, 1084.86, 1244.98, 1445.02]]).shape\n",
        "#autoregression(model, test2_x[1], 1)\n",
        "#standardised_x = np.array(standardise(test2_x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1084.86, 1244.98, 1445.02])"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array = np.array([[940.66, 1084.86, 1244.98, 1445.02]])\n",
        "array[0][1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInanWFa8qIZ",
        "outputId": "c2d65ad7-6fe9-45ff-8898-20b21197b221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]]\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds7L0_rE-g4Z",
        "outputId": "a86889ff-fd57-49ec-c83d-c52f549b126c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1897.52436208])"
            ]
          },
          "execution_count": 512,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inversed = de_standardise(np.array(pred))\n",
        "inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
