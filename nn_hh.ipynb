{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# De-trend (each time series at a time)\n",
        "data = pd.DataFrame(df_micro.iloc[0])\n",
        "data.columns = [\"value\"]\n",
        "year = np.arange(0, 20)\n",
        "data['year'] = year\n",
        "data = data.set_index('year')\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(data['value'], model='additive', period=10)\n",
        "\n",
        "# Access the components of the decomposition\n",
        "trend = decomposition.trend\n",
        "#seasonal = decomposition.seasonal\n",
        "#residual = decomposition.resid\n",
        "test2 = pd.DataFrame(trend).plot()\n",
        "trend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "# Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows.reshape(-1,4), labels.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_test.to_numpy().reshape(-1,1)\n",
        "df_test = pd.DataFrame(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x, train_y = make_windows(df_train.to_numpy(), window_size=4, horizon=1)\n",
        "test2_x, test2_y = make_windows(df_test.to_numpy(), window_size=4, horizon=1)\n",
        "train_x_2 = train_x + 2\n",
        "train_x_2 = np.log(train_x_2)\n",
        "train_y_2 = train_y + 2\n",
        "train_y_2 = np.log(train_y_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1J7pKjWMmxVN"
      },
      "outputs": [],
      "source": [
        "# Create a function to implement a ModelCheckpoint callback\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
        "                                            verbose=0,\n",
        "                                            save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "def evaluate_mdape(y_true, y_pred):\n",
        " return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def calculate_average_rankings(y_true, y_pred):\n",
        "    num_series = len(y_pred)\n",
        "    num_methods = len(y_pred[0])\n",
        "\n",
        "    ranks = []  # to store ranks for each series\n",
        "\n",
        "    for series_index in range(num_series):\n",
        "        sape_values = [\n",
        "            abs((y_true[series_index] - forecast) / y_true[series_index]) * 100\n",
        "            for forecast in y_pred[series_index]\n",
        "        ]\n",
        "        sorted_sape = sorted(sape_values)  # sort SAPE values in ascending order\n",
        "        series_ranks = [sorted_sape.index(sape) + 1 for sape in sape_values]  # assign ranks to SAPE values\n",
        "        ranks.append(series_ranks)\n",
        "\n",
        "    mean_ranks = []  # to store mean ranks for each forecasting method\n",
        "\n",
        "    for method_index in range(num_methods):\n",
        "        total_rank = sum(ranks[series_index][method_index] for series_index in range(num_series))\n",
        "        mean_rank = total_rank / num_series\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    return mean_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true_set, y_pred_set):\n",
        "    # Average Ranking\n",
        "    avg_ranking = None\n",
        "    # Percentage Better\n",
        "    percentage_better = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128, 256])\n",
        "regularizations = np.array([0.001, 0.01, 0.1])\n",
        "hidden_layers = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n",
            "2.28564676437255\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "8.269711365198848\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "3.354759198718868\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "6.086669227270108\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "8.289196723247459\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "16.27673753421188\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "62.74988925209174\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "17.22669832909634\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "5.342656021927347\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "5.031246051124759\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "17.129687887988883\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "20.782477417608572\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "6.427630832609401\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "0.975278021685941\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "22.625168510978472\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "20.34157850804163\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "17.009158788164886\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "20.941867414099548\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "31.291766856792435\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "45.34972996734419\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "178.5686145801082\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "153.8590677355196\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "146.24610607598686\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "140.8706431370773\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "131.88043860152519\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ab66600eb1f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0moptimal_mdape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0moptimal_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0msmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msmape\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0moptimal_smape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-ab66600eb1f0>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(combination, train_x, train_y, tscv)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmdape_scores_fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_x_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0msmape_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdape_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_standardise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_cv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_standardise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#print(f\"test_y_cv[j]: {de_standardise(test_y_cv[j])[0]}, predictions: {de_standardise(predictions).flatten()[0]}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2347\u001b[0m                     )\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2350\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwith_options\u001b[0;34m(self, options, name)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0man\u001b[0m \u001b[0moption\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0monce\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \"\"\"\n\u001b[0;32m-> 2942\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, options, name)\u001b[0m\n\u001b[1;32m   4737\u001b[0m     \u001b[0moptions_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4738\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4739\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4740\u001b[0m       variant_tensor = gen_dataset_ops.options_dataset(\n\u001b[1;32m   4741\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mold_device_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_device_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m       \u001b[0mnew_device_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_device_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_device_parsing_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2042\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m       \u001b[0;31m# Error while trying to compute the cache key.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    best_smape = float('inf')\n",
        "    best_hyperparameters = {}\n",
        "    hidden_neurons = np.arange(2, 9)\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, test_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, test_y_cv = train_y[train_index], train_y[test_index]\n",
        "        \n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            random_neuron = random.choice(hidden_neurons)\n",
        "            chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(random_neuron, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mse\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "        \n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=30, batch_size=combination.batch_size, verbose=0)\n",
        "        smape_scores_fold = []\n",
        "        mdape_scores_fold = []\n",
        "        for j in range(len(test_x_cv)):\n",
        "            predictions = model_cv.predict(np.array([test_x_cv[j]]))\n",
        "            smape_score, mdape_score = evaluate_pred(de_standardise(test_y_cv[j])[0], de_standardise(predictions).flatten()[0])\n",
        "            #print(f\"test_y_cv[j]: {de_standardise(test_y_cv[j])[0]}, predictions: {de_standardise(predictions).flatten()[0]}\")\n",
        "            smape_scores_fold.append(smape_score)\n",
        "            mdape_scores_fold.append(mdape_score)\n",
        "            print(smape_score)\n",
        "        \n",
        "        smape_scores.append(np.mean(smape_scores_fold))\n",
        "        mdape_scores.append(np.mean(mdape_scores_fold))\n",
        "        \n",
        "    mean_smape = np.mean(smape_scores)\n",
        "    mean_mdape = np.mean(mdape_scores)\n",
        "    hyperparameters = {\n",
        "        'learning_rate': combination.learning_rate,\n",
        "        'batch_size': combination.batch_size,\n",
        "        'regularization': combination.regularization,\n",
        "        'hidden_neurons': chosen_hidden_neurons,\n",
        "        'hidden_layers': combination.hidden_layers\n",
        "    }\n",
        "    print(f\"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}\")\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 1)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 551,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.01\n",
            "Batch Size: 32\n",
            "Hidden Neurons 2 in Layer 1.\n",
            "Hidden Neurons 7 in Layer 2.\n",
            "\n",
            "Epoch 1/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.8392 - mse: 0.8240 - mae: 0.6140 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 1s 1ms/step - loss: 0.7296 - mse: 0.7151 - mae: 0.5659\n",
            "Epoch 2/50\n",
            "36/64 [===============>..............] - ETA: 0s - loss: 0.6245 - mse: 0.6122 - mae: 0.5046WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.5776 - mse: 0.5656 - mae: 0.4831\n",
            "Epoch 3/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.5208 - mse: 0.5098 - mae: 0.4278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4935 - mse: 0.4827 - mae: 0.4201\n",
            "Epoch 4/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.4128 - mse: 0.4026 - mae: 0.3790WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4504 - mse: 0.4403 - mae: 0.3729\n",
            "Epoch 5/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4141 - mse: 0.4047 - mae: 0.3710WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4327 - mse: 0.4234 - mae: 0.3613\n",
            "Epoch 6/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.4382 - mse: 0.4295 - mae: 0.3538WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4216 - mse: 0.4130 - mae: 0.3547\n",
            "Epoch 7/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.4497 - mse: 0.4417 - mae: 0.3723WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4218 - mse: 0.4138 - mae: 0.3624\n",
            "Epoch 8/50\n",
            "35/64 [===============>..............] - ETA: 0s - loss: 0.4123 - mse: 0.4046 - mae: 0.3416WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4139 - mse: 0.4064 - mae: 0.3530\n",
            "Epoch 9/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.4609 - mse: 0.4538 - mae: 0.3678WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4152 - mse: 0.4082 - mae: 0.3590\n",
            "Epoch 10/50\n",
            "40/64 [=================>............] - ETA: 0s - loss: 0.4682 - mse: 0.4614 - mae: 0.3754WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4133 - mse: 0.4066 - mae: 0.3583\n",
            "Epoch 11/50\n",
            "34/64 [==============>...............] - ETA: 0s - loss: 0.4658 - mse: 0.4595 - mae: 0.3619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4115 - mse: 0.4053 - mae: 0.3520\n",
            "Epoch 12/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4210 - mse: 0.4150 - mae: 0.3564WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4103 - mse: 0.4044 - mae: 0.3582\n",
            "Epoch 13/50\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4138 - mse: 0.4081 - mae: 0.3493WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 3ms/step - loss: 0.4074 - mse: 0.4016 - mae: 0.3489\n",
            "Epoch 14/50\n",
            "54/64 [========================>.....] - ETA: 0s - loss: 0.4143 - mse: 0.4088 - mae: 0.3607WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 968us/step - loss: 0.4100 - mse: 0.4045 - mae: 0.3557\n",
            "Epoch 15/50\n",
            "63/64 [============================>.] - ETA: 0s - loss: 0.4068 - mse: 0.4015 - mae: 0.3498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 861us/step - loss: 0.4074 - mse: 0.4021 - mae: 0.3507\n",
            "Epoch 16/50\n",
            "61/64 [===========================>..] - ETA: 0s - loss: 0.4048 - mse: 0.3997 - mae: 0.3488WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 880us/step - loss: 0.4071 - mse: 0.4020 - mae: 0.3510\n",
            "Epoch 17/50\n",
            "56/64 [=========================>....] - ETA: 0s - loss: 0.4207 - mse: 0.4157 - mae: 0.3529WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 952us/step - loss: 0.4067 - mse: 0.4018 - mae: 0.3485\n",
            "Epoch 18/50\n",
            "60/64 [===========================>..] - ETA: 0s - loss: 0.4143 - mse: 0.4095 - mae: 0.3503WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 901us/step - loss: 0.4064 - mse: 0.4017 - mae: 0.3477\n",
            "Epoch 19/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4056 - mse: 0.4009 - mae: 0.3487WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 880us/step - loss: 0.4084 - mse: 0.4038 - mae: 0.3511\n",
            "Epoch 20/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4065 - mse: 0.4019 - mae: 0.3563WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 864us/step - loss: 0.4095 - mse: 0.4049 - mae: 0.3572\n",
            "Epoch 21/50\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4053 - mse: 0.4008 - mae: 0.3460WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 850us/step - loss: 0.4053 - mse: 0.4008 - mae: 0.3460\n",
            "Epoch 22/50\n",
            "53/64 [=======================>......] - ETA: 0s - loss: 0.4249 - mse: 0.4205 - mae: 0.3568WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4082 - mse: 0.4038 - mae: 0.3543\n",
            "Epoch 23/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.4355 - mse: 0.4311 - mae: 0.3610WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4088 - mse: 0.4044 - mae: 0.3572\n",
            "Epoch 24/50\n",
            "62/64 [============================>.] - ETA: 0s - loss: 0.4075 - mse: 0.4033 - mae: 0.3501WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 868us/step - loss: 0.4073 - mse: 0.4030 - mae: 0.3506\n",
            "Epoch 25/50\n",
            "57/64 [=========================>....] - ETA: 0s - loss: 0.4243 - mse: 0.4200 - mae: 0.3588WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4087 - mse: 0.4044 - mae: 0.3552\n",
            "Epoch 26/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.3956 - mse: 0.3913 - mae: 0.3590WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4081 - mse: 0.4039 - mae: 0.3542\n",
            "Epoch 27/50\n",
            "33/64 [==============>...............] - ETA: 0s - loss: 0.3439 - mse: 0.3396 - mae: 0.3284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4061 - mse: 0.4019 - mae: 0.3485\n",
            "Epoch 28/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.4287 - mse: 0.4245 - mae: 0.3501WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4067 - mse: 0.4025 - mae: 0.3513\n",
            "Epoch 29/50\n",
            "42/64 [==================>...........] - ETA: 0s - loss: 0.4344 - mse: 0.4302 - mae: 0.3433WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4042 - mse: 0.4000 - mae: 0.3441\n",
            "Epoch 30/50\n",
            "37/64 [================>.............] - ETA: 0s - loss: 0.4547 - mse: 0.4506 - mae: 0.3649WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4062 - mse: 0.4020 - mae: 0.3524\n",
            "Epoch 31/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4114 - mse: 0.4072 - mae: 0.3528WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4056 - mse: 0.4014 - mae: 0.3518\n",
            "Epoch 32/50\n",
            "43/64 [===================>..........] - ETA: 0s - loss: 0.4311 - mse: 0.4271 - mae: 0.3540WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4035 - mse: 0.3995 - mae: 0.3485\n",
            "Epoch 33/50\n",
            "43/64 [===================>..........] - ETA: 0s - loss: 0.3481 - mse: 0.3440 - mae: 0.3321WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4049 - mse: 0.4009 - mae: 0.3492\n",
            "Epoch 34/50\n",
            "38/64 [================>.............] - ETA: 0s - loss: 0.3396 - mse: 0.3355 - mae: 0.3350WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4040 - mse: 0.4000 - mae: 0.3485\n",
            "Epoch 35/50\n",
            "41/64 [==================>...........] - ETA: 0s - loss: 0.4041 - mse: 0.4000 - mae: 0.3401WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4037 - mse: 0.3996 - mae: 0.3438\n",
            "Epoch 36/50\n",
            "44/64 [===================>..........] - ETA: 0s - loss: 0.3584 - mse: 0.3544 - mae: 0.3434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4043 - mse: 0.4003 - mae: 0.3480\n",
            "Epoch 37/50\n",
            "33/64 [==============>...............] - ETA: 0s - loss: 0.3283 - mse: 0.3243 - mae: 0.3278WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4043 - mse: 0.4003 - mae: 0.3470\n",
            "Epoch 38/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.3991 - mse: 0.3952 - mae: 0.3668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4051 - mse: 0.4012 - mae: 0.3491\n",
            "Epoch 39/50\n",
            "39/64 [=================>............] - ETA: 0s - loss: 0.4448 - mse: 0.4408 - mae: 0.3590WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4054 - mse: 0.4015 - mae: 0.3520\n",
            "Epoch 40/50\n",
            "45/64 [====================>.........] - ETA: 0s - loss: 0.4025 - mse: 0.3986 - mae: 0.3395WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4054 - mse: 0.4015 - mae: 0.3462\n",
            "Epoch 41/50\n",
            "51/64 [======================>.......] - ETA: 0s - loss: 0.3441 - mse: 0.3402 - mae: 0.3291WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4032 - mse: 0.3993 - mae: 0.3448\n",
            "Epoch 42/50\n",
            "41/64 [==================>...........] - ETA: 0s - loss: 0.4082 - mse: 0.4043 - mae: 0.3339WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4053 - mse: 0.4015 - mae: 0.3495\n",
            "Epoch 43/50\n",
            "47/64 [=====================>........] - ETA: 0s - loss: 0.4411 - mse: 0.4373 - mae: 0.3619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4043 - mse: 0.4005 - mae: 0.3476\n",
            "Epoch 44/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4444 - mse: 0.4405 - mae: 0.3614WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4056 - mse: 0.4017 - mae: 0.3522\n",
            "Epoch 45/50\n",
            "47/64 [=====================>........] - ETA: 0s - loss: 0.4352 - mse: 0.4314 - mae: 0.3562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4047 - mse: 0.4009 - mae: 0.3452\n",
            "Epoch 46/50\n",
            "46/64 [====================>.........] - ETA: 0s - loss: 0.4368 - mse: 0.4330 - mae: 0.3574WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4036 - mse: 0.3998 - mae: 0.3479\n",
            "Epoch 47/50\n",
            "51/64 [======================>.......] - ETA: 0s - loss: 0.4114 - mse: 0.4075 - mae: 0.3500WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4066 - mse: 0.4028 - mae: 0.3500\n",
            "Epoch 48/50\n",
            "48/64 [=====================>........] - ETA: 0s - loss: 0.3654 - mse: 0.3616 - mae: 0.3480WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4037 - mse: 0.3999 - mae: 0.3464\n",
            "Epoch 49/50\n",
            "49/64 [=====================>........] - ETA: 0s - loss: 0.4122 - mse: 0.4085 - mae: 0.3448WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 1ms/step - loss: 0.4067 - mse: 0.4030 - mae: 0.3538\n",
            "Epoch 50/50\n",
            "50/64 [======================>.......] - ETA: 0s - loss: 0.3974 - mse: 0.3936 - mae: 0.3432WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.4045 - mse: 0.4008 - mae: 0.3479\n",
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]\n",
            " [-1.21687544 -1.13856182 -1.0407236  -0.92424603]\n",
            " [-1.13856182 -1.0407236  -0.92424603 -0.75062769]\n",
            " ...\n",
            " [-0.61201865 -0.67357104 -0.27405519 -0.33851768]\n",
            " [-0.67357104 -0.27405519 -0.33851768 -0.50434476]\n",
            " [-0.27405519 -0.33851768 -0.50434476 -0.27173199]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    print(f\"Hidden Neurons {optimal_hyperparameters['hidden_neurons'][i]} in Layer {i+1}.\")\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"][i], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=\"mse\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=50, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[create_model_checkpoint(model_name=model.name)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 552,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([4802.48985437])"
            ]
          },
          "execution_count": 552,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return de_standardise(standardised_x[-horizon:])\n",
        "\n",
        "autoregression(model, np.array([4793.2, 5602, 5065, 5056]), 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544 -1.13856182 -1.0407236  -0.92424603]]\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "data = (np.array([[940.66, 1084.86, 1244.98, 1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "print(data.shape)\n",
        "#def make_preds(model, input_data):\n",
        "#  forecast = model.predict(input_data)\n",
        "#  preds = tf.squeeze(forecast)\n",
        "#  return preds\n",
        "\n",
        "#pred = make_preds(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 4)"
            ]
          },
          "execution_count": 511,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([[940.66, 1084.86, 1244.98, 1445.02]]).shape\n",
        "#autoregression(model, test2_x[1], 1)\n",
        "#standardised_x = np.array(standardise(test2_x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1084.86, 1244.98, 1445.02])"
            ]
          },
          "execution_count": 251,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array = np.array([[940.66, 1084.86, 1244.98, 1445.02]])\n",
        "array[0][1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInanWFa8qIZ",
        "outputId": "c2d65ad7-6fe9-45ff-8898-20b21197b221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.28740269 -1.21687544 -1.13856182 -1.0407236 ]]\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds7L0_rE-g4Z",
        "outputId": "a86889ff-fd57-49ec-c83d-c52f549b126c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1897.52436208])"
            ]
          },
          "execution_count": 512,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inversed = de_standardise(np.array(pred))\n",
        "inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
