{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.callbacks import EarlyStopping\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Detrend\n",
        "#for i in range(len(df_train)):\n",
        "#    data = df_train.iloc[i]\n",
        "#    poly_fit = np.polyfit(np.arange(14), data, 2)\n",
        "#    trend = np.polyval(poly_fit, np.arange(14))\n",
        "#    df_train.iloc[i] = df_train.iloc[i] - trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "##Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n",
        "\n",
        "def exponential_smoothing(data, alpha):\n",
        "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
        "\n",
        "# Assuming 'data' is a pandas Series or DataFrame column\n",
        "alpha = 0.825 \n",
        "df_train = exponential_smoothing(df_train, alpha)\n",
        "df_train = df_train.to_numpy().reshape(-1,14)\n",
        "df_test = df_test.to_numpy().reshape(-1,6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows.reshape(-1,4), labels.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.28740269, -1.22921771, -1.1544266 , -1.06062162],\n",
              "       [-1.22921771, -1.1544266 , -1.06062162, -0.94811176],\n",
              "       [-1.1544266 , -1.06062162, -0.94811176, -0.7851874 ],\n",
              "       ...,\n",
              "       [-0.61448562, -0.66323109, -0.34216097, -0.33915526],\n",
              "       [-0.66323109, -0.34216097, -0.33915526, -0.4754366 ],\n",
              "       [-0.34216097, -0.33915526, -0.4754366 , -0.3073803 ]])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_x = []\n",
        "train_y = []\n",
        "test_x = []\n",
        "test_y = []\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "    windows_train, labels_train = make_windows(df_train[i], window_size=4, horizon=1)\n",
        "    windows_test, labels_test = make_windows(df_test[i], window_size=4, horizon=1)\n",
        "    train_x = np.concatenate((np.array(train_x).reshape(-1,4), windows_train.reshape(-1,4)))\n",
        "    train_y = np.concatenate((np.array(train_y).reshape(-1,1), labels_train.reshape(-1,1)))\n",
        "    test_x = np.concatenate((np.array(test_x).reshape(-1,4), windows_test.reshape(-1,4)))\n",
        "    test_y = np.concatenate((np.array(test_y).reshape(-1,1), labels_test.reshape(-1,1)))\n",
        "\n",
        "train_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    numerator = 2 * tf.abs(y_pred - y_true)\n",
        "    denominator = tf.abs(y_pred) + tf.abs(y_true)\n",
        "    smape = tf.reduce_mean(numerator / denominator) * 100\n",
        "    return smape\n",
        "\n",
        "def smape_loss(y_true, y_pred):\n",
        "    epsilon = 0.1  # to avoid division by zero\n",
        "    denominator = tf.abs(y_true) + tf.abs(y_pred) + epsilon\n",
        "    diff = tf.abs(y_true - y_pred) / denominator\n",
        "    smape = 2.0 * tf.reduce_mean(diff)\n",
        "    return smape\n",
        "\n",
        "def metric_mdape(y_true, y_pred):\n",
        "    return tfp.stats.percentile((tf.abs(tf.math.subtract(y_true, y_pred)/ y_true)), 50.0, interpolation='midpoint')\n",
        "      \n",
        "def evaluate_mdape(y_true, y_pred):\n",
        "    return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers hidden_neurons\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128])\n",
        "regularizations = np.array([0.001, 0.01, 0.001])\n",
        "hidden_layers = np.array([2, 3, 4, 6, 8])\n",
        "hidden_neurons = np.array([2, 3, 4, 5])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers, hidden_neurons)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 1s 4ms/step - loss: 1.7977 - metric_mdape: 1.8479 - mae: 1.5425 - mse: 3.7755\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.6805 - metric_mdape: 1.2486 - mae: 1.2559 - mse: 2.4892\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5838 - metric_mdape: 1.1601 - mae: 1.0535 - mse: 1.7222\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.5028 - metric_mdape: 1.2258 - mae: 0.9209 - mse: 1.3175\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.4433 - metric_mdape: 1.0202 - mae: 0.8289 - mse: 1.0746\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.3922 - metric_mdape: 1.0109 - mae: 0.7667 - mse: 0.9292\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.3460 - metric_mdape: 1.0840 - mae: 0.7255 - mse: 0.8424\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.3135 - metric_mdape: 1.0015 - mae: 0.7032 - mse: 0.7943\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.2854 - metric_mdape: 1.0658 - mae: 0.6893 - mse: 0.7629\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2618 - metric_mdape: 1.0123 - mae: 0.6815 - mse: 0.7445\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.2420 - metric_mdape: 1.0167 - mae: 0.6759 - mse: 0.7302\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.2267 - metric_mdape: 0.9988 - mae: 0.6739 - mse: 0.7220\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.2119 - metric_mdape: 0.9841 - mae: 0.6728 - mse: 0.7165\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1989 - metric_mdape: 1.0365 - mae: 0.6729 - mse: 0.7143\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1869 - metric_mdape: 1.1583 - mae: 0.6743 - mse: 0.7145\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1767 - metric_mdape: 1.1323 - mae: 0.6763 - mse: 0.7152\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1666 - metric_mdape: 1.1946 - mae: 0.6780 - mse: 0.7175\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1592 - metric_mdape: 1.2499 - mae: 0.6803 - mse: 0.7196\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1514 - metric_mdape: 1.0521 - mae: 0.6819 - mse: 0.7212\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1453 - metric_mdape: 1.2150 - mae: 0.6838 - mse: 0.7236\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1407 - metric_mdape: 1.2775 - mae: 0.6863 - mse: 0.7258\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1361 - metric_mdape: 1.3434 - mae: 0.6884 - mse: 0.7281\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1314 - metric_mdape: 1.2322 - mae: 0.6899 - mse: 0.7303\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1276 - metric_mdape: 1.2882 - mae: 0.6917 - mse: 0.7324\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1242 - metric_mdape: 1.2644 - mae: 0.6940 - mse: 0.7356\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1207 - metric_mdape: 1.2092 - mae: 0.6961 - mse: 0.7387\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1176 - metric_mdape: 1.3246 - mae: 0.6983 - mse: 0.7419\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1148 - metric_mdape: 1.3543 - mae: 0.7001 - mse: 0.7447\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1123 - metric_mdape: 1.3215 - mae: 0.7020 - mse: 0.7477\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1099 - metric_mdape: 1.4223 - mae: 0.7038 - mse: 0.7504\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1074 - metric_mdape: 1.4960 - mae: 0.7054 - mse: 0.7532\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1053 - metric_mdape: 1.5292 - mae: 0.7070 - mse: 0.7558\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.1037 - metric_mdape: 1.3209 - mae: 0.7091 - mse: 0.7593\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.1018 - metric_mdape: 1.3163 - mae: 0.7106 - mse: 0.7616\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.1001 - metric_mdape: 1.3157 - mae: 0.7123 - mse: 0.7645\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0986 - metric_mdape: 1.3140 - mae: 0.7138 - mse: 0.7673\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0972 - metric_mdape: 1.3936 - mae: 0.7150 - mse: 0.7691\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0961 - metric_mdape: 1.3685 - mae: 0.7160 - mse: 0.7707\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0950 - metric_mdape: 1.5279 - mae: 0.7173 - mse: 0.7726\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0938 - metric_mdape: 1.5075 - mae: 0.7179 - mse: 0.7737\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0928 - metric_mdape: 1.3626 - mae: 0.7187 - mse: 0.7749\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0918 - metric_mdape: 1.6053 - mae: 0.7197 - mse: 0.7767\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0909 - metric_mdape: 1.4295 - mae: 0.7208 - mse: 0.7785\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0900 - metric_mdape: 1.4667 - mae: 0.7215 - mse: 0.7798\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 1.0892 - metric_mdape: 1.4040 - mae: 0.7219 - mse: 0.7805\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 1.0885 - metric_mdape: 1.3603 - mae: 0.7225 - mse: 0.7815\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0878 - metric_mdape: 1.4036 - mae: 0.7232 - mse: 0.7827\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0871 - metric_mdape: 1.5930 - mae: 0.7239 - mse: 0.7838\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0864 - metric_mdape: 1.6332 - mae: 0.7248 - mse: 0.7854\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 1.0859 - metric_mdape: 1.5820 - mae: 0.7258 - mse: 0.7870\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "Epoch 1/50\n",
            "8/8 [==============================] - 1s 2ms/step - loss: 1.6511 - metric_mdape: 1.0471 - mae: 0.7352 - mse: 0.8979\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.3990 - metric_mdape: 1.1001 - mae: 0.7279 - mse: 0.8367\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.3174 - metric_mdape: 1.4388 - mae: 0.7657 - mse: 0.8940\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.2872 - metric_mdape: 1.5963 - mae: 0.7820 - mse: 0.9205\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.2512 - metric_mdape: 1.4750 - mae: 0.7316 - mse: 0.8236\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.2096 - metric_mdape: 1.3443 - mae: 0.6534 - mse: 0.6785\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.1754 - metric_mdape: 1.2633 - mae: 0.6000 - mse: 0.5881\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.1342 - metric_mdape: 1.1493 - mae: 0.5404 - mse: 0.4964\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.0804 - metric_mdape: 1.0021 - mae: 0.4809 - mse: 0.4141\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 1.0445 - metric_mdape: 0.9152 - mae: 0.4389 - mse: 0.3571\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.8841 - metric_mdape: 0.7161 - mae: 0.3941 - mse: 0.2990\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7293 - metric_mdape: 0.5605 - mae: 0.3543 - mse: 0.2516\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6559 - metric_mdape: 0.4760 - mae: 0.3275 - mse: 0.2154\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6147 - metric_mdape: 0.4503 - mae: 0.3085 - mse: 0.1950\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5822 - metric_mdape: 0.3962 - mae: 0.2889 - mse: 0.1757\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5576 - metric_mdape: 0.3700 - mae: 0.2854 - mse: 0.1706\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5399 - metric_mdape: 0.3495 - mae: 0.2691 - mse: 0.1479\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5252 - metric_mdape: 0.3375 - mae: 0.2665 - mse: 0.1480\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5103 - metric_mdape: 0.3371 - mae: 0.2513 - mse: 0.1339\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4944 - metric_mdape: 0.3212 - mae: 0.2507 - mse: 0.1322\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4871 - metric_mdape: 0.3101 - mae: 0.2431 - mse: 0.1253\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4710 - metric_mdape: 0.3060 - mae: 0.2337 - mse: 0.1182\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4753 - metric_mdape: 0.3213 - mae: 0.2344 - mse: 0.1139\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4798 - metric_mdape: 0.3218 - mae: 0.2319 - mse: 0.1130\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4678 - metric_mdape: 0.3075 - mae: 0.2250 - mse: 0.1074\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4433 - metric_mdape: 0.2964 - mae: 0.2167 - mse: 0.1033\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4384 - metric_mdape: 0.2757 - mae: 0.2128 - mse: 0.0987\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4353 - metric_mdape: 0.2718 - mae: 0.2111 - mse: 0.0990\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4350 - metric_mdape: 0.2529 - mae: 0.2080 - mse: 0.0939\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4334 - metric_mdape: 0.2449 - mae: 0.2094 - mse: 0.0957\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4454 - metric_mdape: 0.2661 - mae: 0.2141 - mse: 0.0972\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4302 - metric_mdape: 0.2560 - mae: 0.2033 - mse: 0.0918\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4455 - metric_mdape: 0.2582 - mae: 0.2089 - mse: 0.0907\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4331 - metric_mdape: 0.2404 - mae: 0.2028 - mse: 0.0869\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4248 - metric_mdape: 0.2531 - mae: 0.2004 - mse: 0.0899\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4249 - metric_mdape: 0.2484 - mae: 0.1968 - mse: 0.0838\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4166 - metric_mdape: 0.2375 - mae: 0.1934 - mse: 0.0832\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4267 - metric_mdape: 0.2574 - mae: 0.2042 - mse: 0.0920\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4278 - metric_mdape: 0.2541 - mae: 0.1954 - mse: 0.0814\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4203 - metric_mdape: 0.2389 - mae: 0.1952 - mse: 0.0856\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4121 - metric_mdape: 0.2465 - mae: 0.1920 - mse: 0.0824\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4130 - metric_mdape: 0.2520 - mae: 0.1897 - mse: 0.0802\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4146 - metric_mdape: 0.2249 - mae: 0.1890 - mse: 0.0817\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4062 - metric_mdape: 0.2226 - mae: 0.1853 - mse: 0.0776\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4093 - metric_mdape: 0.2191 - mae: 0.1856 - mse: 0.0777\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4004 - metric_mdape: 0.2128 - mae: 0.1798 - mse: 0.0738\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4067 - metric_mdape: 0.2307 - mae: 0.1825 - mse: 0.0760\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4130 - metric_mdape: 0.2474 - mae: 0.1901 - mse: 0.0817\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4083 - metric_mdape: 0.2204 - mae: 0.1809 - mse: 0.0728\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4123 - metric_mdape: 0.2362 - mae: 0.1854 - mse: 0.0770\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "Epoch 1/50\n",
            "12/12 [==============================] - 1s 2ms/step - loss: 1.6490 - metric_mdape: 1.3172 - mae: 0.9713 - mse: 1.6180\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 1.5080 - metric_mdape: 1.2627 - mae: 0.8326 - mse: 1.1948\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.4257 - metric_mdape: 1.0958 - mae: 0.7078 - mse: 0.8915\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.3490 - metric_mdape: 1.0266 - mae: 0.6406 - mse: 0.7712\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.2990 - metric_mdape: 1.0060 - mae: 0.5872 - mse: 0.6902\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.2459 - metric_mdape: 1.0184 - mae: 0.5540 - mse: 0.6613\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.2185 - metric_mdape: 0.9693 - mae: 0.5512 - mse: 0.6743\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.2020 - metric_mdape: 1.0040 - mae: 0.5464 - mse: 0.6690\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1878 - metric_mdape: 1.0205 - mae: 0.5410 - mse: 0.6617\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1713 - metric_mdape: 0.9499 - mae: 0.5328 - mse: 0.6495\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1589 - metric_mdape: 0.9926 - mae: 0.5274 - mse: 0.6479\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1495 - metric_mdape: 1.0220 - mae: 0.5280 - mse: 0.6513\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1437 - metric_mdape: 1.0170 - mae: 0.5225 - mse: 0.6427\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1361 - metric_mdape: 1.0079 - mae: 0.5167 - mse: 0.6315\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.1345 - metric_mdape: 1.0004 - mae: 0.5294 - mse: 0.6511\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 1.1367 - metric_mdape: 0.9561 - mae: 0.5227 - mse: 0.6298\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.9919 - metric_mdape: 0.8033 - mae: 0.4757 - mse: 0.5514\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.7660 - metric_mdape: 0.5735 - mae: 0.4144 - mse: 0.4275\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.6555 - metric_mdape: 0.4447 - mae: 0.3582 - mse: 0.3463\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.6191 - metric_mdape: 0.4205 - mae: 0.3347 - mse: 0.3016\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5707 - metric_mdape: 0.3653 - mae: 0.3101 - mse: 0.2735\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5444 - metric_mdape: 0.3299 - mae: 0.2942 - mse: 0.2536\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5308 - metric_mdape: 0.3294 - mae: 0.2857 - mse: 0.2379\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.5315 - metric_mdape: 0.3426 - mae: 0.2825 - mse: 0.2276\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5499 - metric_mdape: 0.3695 - mae: 0.2993 - mse: 0.2276\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5177 - metric_mdape: 0.3371 - mae: 0.2756 - mse: 0.2111\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5040 - metric_mdape: 0.3094 - mae: 0.2684 - mse: 0.2085\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5218 - metric_mdape: 0.3285 - mae: 0.2708 - mse: 0.2005\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5389 - metric_mdape: 0.3171 - mae: 0.2826 - mse: 0.2021\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.5460 - metric_mdape: 0.3536 - mae: 0.2899 - mse: 0.2050\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4872 - metric_mdape: 0.2872 - mae: 0.2535 - mse: 0.1857\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4706 - metric_mdape: 0.2739 - mae: 0.2459 - mse: 0.1806\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4605 - metric_mdape: 0.2671 - mae: 0.2393 - mse: 0.1743\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4564 - metric_mdape: 0.2487 - mae: 0.2378 - mse: 0.1720\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4667 - metric_mdape: 0.2707 - mae: 0.2388 - mse: 0.1686\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4473 - metric_mdape: 0.2525 - mae: 0.2299 - mse: 0.1631\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4480 - metric_mdape: 0.2531 - mae: 0.2307 - mse: 0.1599\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4451 - metric_mdape: 0.2568 - mae: 0.2289 - mse: 0.1587\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4465 - metric_mdape: 0.2542 - mae: 0.2278 - mse: 0.1546\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4442 - metric_mdape: 0.2518 - mae: 0.2259 - mse: 0.1522\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.4350 - metric_mdape: 0.2347 - mae: 0.2212 - mse: 0.1493\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4548 - metric_mdape: 0.2673 - mae: 0.2262 - mse: 0.1487\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4450 - metric_mdape: 0.2496 - mae: 0.2243 - mse: 0.1466\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4529 - metric_mdape: 0.2595 - mae: 0.2274 - mse: 0.1450\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4367 - metric_mdape: 0.2403 - mae: 0.2187 - mse: 0.1412\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4508 - metric_mdape: 0.2716 - mae: 0.2288 - mse: 0.1439\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4374 - metric_mdape: 0.2581 - mae: 0.2166 - mse: 0.1373\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4305 - metric_mdape: 0.2428 - mae: 0.2141 - mse: 0.1366\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4360 - metric_mdape: 0.2501 - mae: 0.2151 - mse: 0.1351\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.4290 - metric_mdape: 0.2352 - mae: 0.2134 - mse: 0.1335\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 1.6743 - metric_mdape: 1.0011 - mae: 0.7182 - mse: 0.8331\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.4612 - metric_mdape: 1.0458 - mae: 0.7230 - mse: 0.8158\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3395 - metric_mdape: 1.1174 - mae: 0.7362 - mse: 0.8261\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2705 - metric_mdape: 1.3408 - mae: 0.7831 - mse: 0.9049\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2396 - metric_mdape: 1.5550 - mae: 0.8326 - mse: 1.0007\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2241 - metric_mdape: 1.6340 - mae: 0.8508 - mse: 1.0378\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2124 - metric_mdape: 1.6077 - mae: 0.8420 - mse: 1.0191\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.2044 - metric_mdape: 1.4872 - mae: 0.8278 - mse: 0.9902\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1989 - metric_mdape: 1.5008 - mae: 0.8262 - mse: 0.9872\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1952 - metric_mdape: 1.5390 - mae: 0.8236 - mse: 0.9819\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1930 - metric_mdape: 1.5084 - mae: 0.8195 - mse: 0.9737\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1915 - metric_mdape: 1.5032 - mae: 0.8204 - mse: 0.9754\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1904 - metric_mdape: 1.5209 - mae: 0.8243 - mse: 0.9834\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1899 - metric_mdape: 1.4953 - mae: 0.8268 - mse: 0.9883\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1900 - metric_mdape: 1.5022 - mae: 0.8214 - mse: 0.9770\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1892 - metric_mdape: 1.5858 - mae: 0.8268 - mse: 0.9880\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1890 - metric_mdape: 1.5456 - mae: 0.8287 - mse: 0.9920\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1893 - metric_mdape: 1.5039 - mae: 0.8246 - mse: 0.9834\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1889 - metric_mdape: 1.5842 - mae: 0.8283 - mse: 0.9913\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1887 - metric_mdape: 1.4751 - mae: 0.8247 - mse: 0.9839\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1892 - metric_mdape: 1.5146 - mae: 0.8169 - mse: 0.9684\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1886 - metric_mdape: 1.5061 - mae: 0.8240 - mse: 0.9828\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1898 - metric_mdape: 1.5252 - mae: 0.8416 - mse: 1.0174\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1889 - metric_mdape: 1.4523 - mae: 0.8312 - mse: 0.9970\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1887 - metric_mdape: 1.5531 - mae: 0.8300 - mse: 0.9946\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1887 - metric_mdape: 1.5249 - mae: 0.8335 - mse: 1.0016\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1887 - metric_mdape: 1.6157 - mae: 0.8394 - mse: 1.0136\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1884 - metric_mdape: 1.6237 - mae: 0.8345 - mse: 1.0038\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1892 - metric_mdape: 1.4996 - mae: 0.8185 - mse: 0.9719\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1887 - metric_mdape: 1.4556 - mae: 0.8209 - mse: 0.9766\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1886 - metric_mdape: 1.5251 - mae: 0.8217 - mse: 0.9782\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1884 - metric_mdape: 1.5369 - mae: 0.8243 - mse: 0.9833\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.5150 - mae: 0.8269 - mse: 0.9885\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.6114 - mae: 0.8329 - mse: 1.0005\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.5850 - mae: 0.8300 - mse: 0.9946\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.5517 - mae: 0.8273 - mse: 0.9892\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.5022 - mae: 0.8234 - mse: 0.9813\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1882 - metric_mdape: 1.4746 - mae: 0.8258 - mse: 0.9861\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1883 - metric_mdape: 1.4863 - mae: 0.8282 - mse: 0.9910\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1886 - metric_mdape: 1.5077 - mae: 0.8223 - mse: 0.9793\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1877 - metric_mdape: 1.5319 - mae: 0.8349 - mse: 1.0057\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1888 - metric_mdape: 1.6849 - mae: 0.8515 - mse: 1.0390\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1881 - metric_mdape: 1.5392 - mae: 0.8398 - mse: 1.0149\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.1884 - metric_mdape: 1.5491 - mae: 0.8337 - mse: 1.0016\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1881 - metric_mdape: 1.5529 - mae: 0.8304 - mse: 0.9954\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1881 - metric_mdape: 1.5420 - mae: 0.8330 - mse: 1.0008\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1880 - metric_mdape: 1.5165 - mae: 0.8331 - mse: 1.0010\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1882 - metric_mdape: 1.5308 - mae: 0.8320 - mse: 0.9985\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1882 - metric_mdape: 1.5463 - mae: 0.8344 - mse: 1.0036\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.1880 - metric_mdape: 1.5583 - mae: 0.8382 - mse: 1.0115\n",
            "8/8 [==============================] - 0s 940us/step\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 1s 3ms/step - loss: 1.3270 - metric_mdape: 1.3009 - mae: 0.7387 - mse: 1.0406\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 1.1952 - metric_mdape: 1.0086 - mae: 0.5172 - mse: 0.5141\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.9745 - metric_mdape: 0.6736 - mae: 0.4532 - mse: 0.4238\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.7653 - metric_mdape: 0.4863 - mae: 0.3831 - mse: 0.3400\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.6852 - metric_mdape: 0.4262 - mae: 0.3429 - mse: 0.2881\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.6682 - metric_mdape: 0.4292 - mae: 0.3386 - mse: 0.2789\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5888 - metric_mdape: 0.3468 - mae: 0.2973 - mse: 0.2364\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5448 - metric_mdape: 0.2987 - mae: 0.2717 - mse: 0.2102\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5394 - metric_mdape: 0.2824 - mae: 0.2678 - mse: 0.1990\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5167 - metric_mdape: 0.3288 - mae: 0.2511 - mse: 0.1953\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5489 - metric_mdape: 0.3225 - mae: 0.2688 - mse: 0.2010\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4849 - metric_mdape: 0.2514 - mae: 0.2370 - mse: 0.1739\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5048 - metric_mdape: 0.2766 - mae: 0.2523 - mse: 0.1844\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4771 - metric_mdape: 0.6054 - mae: 0.2438 - mse: 0.1832\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4590 - metric_mdape: 0.2351 - mae: 0.2250 - mse: 0.1668\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4641 - metric_mdape: 0.2380 - mae: 0.2304 - mse: 0.1703\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5042 - metric_mdape: 0.2987 - mae: 0.2560 - mse: 0.1786\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5222 - metric_mdape: 0.2983 - mae: 0.2645 - mse: 0.1853\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4838 - metric_mdape: 0.2620 - mae: 0.2541 - mse: 0.1830\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4786 - metric_mdape: 0.2398 - mae: 0.2346 - mse: 0.1649\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4507 - metric_mdape: 0.2275 - mae: 0.2225 - mse: 0.1623\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4535 - metric_mdape: 0.2495 - mae: 0.2271 - mse: 0.1617\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4838 - metric_mdape: 0.2640 - mae: 0.2349 - mse: 0.1583\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4568 - metric_mdape: 0.2296 - mae: 0.2169 - mse: 0.1503\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4488 - metric_mdape: 0.2174 - mae: 0.2180 - mse: 0.1543\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4509 - metric_mdape: 0.3677 - mae: 0.2240 - mse: 0.1565\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4354 - metric_mdape: 0.2139 - mae: 0.2121 - mse: 0.1503\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4320 - metric_mdape: 0.2124 - mae: 0.2077 - mse: 0.1452\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4673 - metric_mdape: 0.2408 - mae: 0.2270 - mse: 0.1528\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4359 - metric_mdape: 0.2133 - mae: 0.2147 - mse: 0.1509\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4277 - metric_mdape: 0.2046 - mae: 0.2112 - mse: 0.1517\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4482 - metric_mdape: 0.2478 - mae: 0.2142 - mse: 0.1440\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.7658 - metric_mdape: 0.7308 - mae: 0.4596 - mse: 0.3701\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.5530 - metric_mdape: 0.3849 - mae: 0.3350 - mse: 0.2613\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4746 - metric_mdape: 0.4820 - mae: 0.2427 - mse: 0.1693\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4754 - metric_mdape: 0.2459 - mae: 0.2332 - mse: 0.1565\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4397 - metric_mdape: 0.2282 - mae: 0.2172 - mse: 0.1455\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4195 - metric_mdape: 0.2138 - mae: 0.2041 - mse: 0.1404\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4436 - metric_mdape: 0.2242 - mae: 0.2175 - mse: 0.1458\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4246 - metric_mdape: 0.2044 - mae: 0.2055 - mse: 0.1394\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4348 - metric_mdape: 0.2107 - mae: 0.2100 - mse: 0.1409\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4556 - metric_mdape: 0.2576 - mae: 0.2238 - mse: 0.1434\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4263 - metric_mdape: 0.2149 - mae: 0.2058 - mse: 0.1361\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4538 - metric_mdape: 0.2367 - mae: 0.2316 - mse: 0.1603\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4376 - metric_mdape: 0.2683 - mae: 0.2096 - mse: 0.1376\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4290 - metric_mdape: 0.2155 - mae: 0.2095 - mse: 0.1417\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4543 - metric_mdape: 0.2335 - mae: 0.2257 - mse: 0.1561\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4375 - metric_mdape: 0.2191 - mae: 0.2085 - mse: 0.1400\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4271 - metric_mdape: 0.2097 - mae: 0.2041 - mse: 0.1357\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.4380 - metric_mdape: 0.2380 - mae: 0.2147 - mse: 0.1421\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "Current mean SMAPE: 28.863786699858537, Current hyperparameters: {'learning_rate': 0.01, 'batch_size': 64, 'regularization': 0.01, 'hidden_neurons': 3, 'hidden_layers': 4}\n",
            "Epoch 1/50\n",
            "16/16 [==============================] - 2s 2ms/step - loss: 1.4427 - metric_mdape: 1.0855 - mae: 0.7172 - mse: 0.9318\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3818 - metric_mdape: 1.3055 - mae: 0.8293 - mse: 1.1990\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3764 - metric_mdape: 1.5071 - mae: 0.8575 - mse: 1.2663\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3655 - metric_mdape: 1.3039 - mae: 0.8137 - mse: 1.1609\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 1.3601 - metric_mdape: 1.3104 - mae: 0.8058 - mse: 1.1433\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3548 - metric_mdape: 1.4621 - mae: 0.8273 - mse: 1.1942\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3522 - metric_mdape: 1.2908 - mae: 0.8009 - mse: 1.1290\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3510 - metric_mdape: 1.2825 - mae: 0.7979 - mse: 1.1215\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3461 - metric_mdape: 1.2270 - mae: 0.7912 - mse: 1.1087\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3436 - metric_mdape: 1.2993 - mae: 0.7859 - mse: 1.0965\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3410 - metric_mdape: 1.3935 - mae: 0.7958 - mse: 1.1200\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3403 - metric_mdape: 1.3044 - mae: 0.8134 - mse: 1.1617\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3392 - metric_mdape: 1.2647 - mae: 0.8096 - mse: 1.1516\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3395 - metric_mdape: 1.3187 - mae: 0.8205 - mse: 1.1777\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3407 - metric_mdape: 1.2703 - mae: 0.8077 - mse: 1.1462\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3381 - metric_mdape: 1.3479 - mae: 0.8163 - mse: 1.1671\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3392 - metric_mdape: 1.3314 - mae: 0.8109 - mse: 1.1548\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 1.3379 - metric_mdape: 1.3700 - mae: 0.8170 - mse: 1.1670\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 1.3357 - metric_mdape: 1.2925 - mae: 0.8172 - mse: 1.1710\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3384 - metric_mdape: 1.2416 - mae: 0.7866 - mse: 1.0981\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3357 - metric_mdape: 1.2819 - mae: 0.7913 - mse: 1.1091\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3374 - metric_mdape: 1.4125 - mae: 0.8240 - mse: 1.1840\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 1.3362 - metric_mdape: 1.3153 - mae: 0.8206 - mse: 1.1795\n",
            "Epoch 24/50\n",
            " 1/16 [>.............................] - ETA: 0s - loss: 1.3793 - metric_mdape: 1.2784 - mae: 0.8657 - mse: 1.2760"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-ce2985cf3215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0moptimal_mdape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0moptimal_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0msmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msmape\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0moptimal_smape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-ce2985cf3215>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(combination, train_x, train_y, tscv)\u001b[0m\n\u001b[1;32m     34\u001b[0m                         metrics=[metric_mdape, \"mae\", \"mse\"]) # Backpropagation\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmodel_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m--> 142\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \"\"\"\n\u001b[1;32m    341\u001b[0m     args, kwargs, filtered_flat_args = (\n\u001b[0;32m--> 342\u001b[0;31m         self._function_spec.canonicalize_function_inputs(args, kwargs))\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_pure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mbind_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       bound_arguments = self.function_type.bind_with_defaults(\n\u001b[0m\u001b[1;32m    426\u001b[0m           args, sanitized_kwargs, self.default_values)\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mbind_with_defaults\u001b[0;34m(self, args, kwargs, default_values)\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"Returns BoundArguments with default values filled in.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m         \"\"\"\n\u001b[0;32m-> 3045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.16/Frameworks/Python.framework/Versions/3.9/lib/python3.9/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   2991\u001b[0m         \u001b[0;31m# keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2992\u001b[0m         \u001b[0mkwargs_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2993\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_VAR_KEYWORD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m                 \u001b[0;31m# Memorize that we have a '**kwargs'-like parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Time-series expanding window validation\n",
        "#with tf.device('/cpu:0'):\n",
        "    \n",
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, val_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, val_y_cv = train_y[train_index], train_y[test_index]\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential(name=\"model\")\n",
        "\n",
        "        #chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            #random_neuron = random.choice(hidden_neurons)\n",
        "            #chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(combination.hidden_neurons, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=smape_loss,\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[metric_mdape, \"mae\", \"mse\"]) # Backpropagation\n",
        "        \n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=1)\n",
        "\n",
        "        predictions = model_cv.predict(val_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(val_y_cv), de_standardise(predictions))\n",
        "        \n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        \n",
        "    mean_smape = np.mean(smape_scores)\n",
        "    mean_mdape = np.mean(mdape_scores)\n",
        "    hyperparameters = {\n",
        "        'learning_rate': combination.learning_rate,\n",
        "        'batch_size': combination.batch_size,\n",
        "        'regularization': combination.regularization,\n",
        "        'hidden_neurons': combination.hidden_neurons,\n",
        "        'hidden_layers': combination.hidden_layers\n",
        "    }\n",
        "    print(f\"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}\")\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 3)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.01\n",
            "Batch Size: 16\n",
            "\n",
            "Epoch 1/100\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 1.3530 - metric_mdape: 1.2490 - mae: 0.9047 - mse: 1.4720\n",
            "Epoch 1: loss improved from inf to 1.22661, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 1s 2ms/step - loss: 1.2266 - metric_mdape: 1.1055 - mae: 0.8089 - mse: 1.2471\n",
            "Epoch 2/100\n",
            "71/92 [======================>.......] - ETA: 0s - loss: 0.5922 - metric_mdape: 0.4790 - mae: 0.3698 - mse: 0.2821\n",
            "Epoch 2: loss improved from 1.22661 to 0.58647, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.5865 - metric_mdape: 0.4625 - mae: 0.3610 - mse: 0.2672\n",
            "Epoch 3/100\n",
            "78/92 [========================>.....] - ETA: 0s - loss: 0.4645 - metric_mdape: 0.3296 - mae: 0.2854 - mse: 0.1889\n",
            "Epoch 3: loss improved from 0.58647 to 0.46168, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.4617 - metric_mdape: 0.3323 - mae: 0.2816 - mse: 0.1824\n",
            "Epoch 4/100\n",
            "76/92 [=======================>......] - ETA: 0s - loss: 0.4292 - metric_mdape: 0.2982 - mae: 0.2566 - mse: 0.1611\n",
            "Epoch 4: loss improved from 0.46168 to 0.42782, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.4278 - metric_mdape: 0.2963 - mae: 0.2528 - mse: 0.1531\n",
            "Epoch 5/100\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 0.4017 - metric_mdape: 0.2608 - mae: 0.2308 - mse: 0.1306\n",
            "Epoch 5: loss improved from 0.42782 to 0.40254, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.4025 - metric_mdape: 0.2619 - mae: 0.2337 - mse: 0.1387\n",
            "Epoch 6/100\n",
            "60/92 [==================>...........] - ETA: 0s - loss: 0.4063 - metric_mdape: 0.2763 - mae: 0.2345 - mse: 0.1380\n",
            "Epoch 6: loss improved from 0.40254 to 0.39870, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3987 - metric_mdape: 0.2708 - mae: 0.2296 - mse: 0.1364\n",
            "Epoch 7/100\n",
            "84/92 [==========================>...] - ETA: 0s - loss: 0.3946 - metric_mdape: 0.2632 - mae: 0.2288 - mse: 0.1355\n",
            "Epoch 7: loss improved from 0.39870 to 0.39787, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 4ms/step - loss: 0.3979 - metric_mdape: 0.2661 - mae: 0.2291 - mse: 0.1348\n",
            "Epoch 8/100\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.4059 - metric_mdape: 0.2595 - mae: 0.2307 - mse: 0.1370\n",
            "Epoch 8: loss did not improve from 0.39787\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.4075 - metric_mdape: 0.2601 - mae: 0.2300 - mse: 0.1359\n",
            "Epoch 9/100\n",
            "59/92 [==================>...........] - ETA: 0s - loss: 0.4129 - metric_mdape: 0.2796 - mae: 0.2295 - mse: 0.1311\n",
            "Epoch 9: loss did not improve from 0.39787\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.4039 - metric_mdape: 0.2722 - mae: 0.2305 - mse: 0.1389\n",
            "Epoch 10/100\n",
            "67/92 [====================>.........] - ETA: 0s - loss: 0.3934 - metric_mdape: 0.2482 - mae: 0.2268 - mse: 0.1402\n",
            "Epoch 10: loss did not improve from 0.39787\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3979 - metric_mdape: 0.2498 - mae: 0.2272 - mse: 0.1351\n",
            "Epoch 11/100\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.3929 - metric_mdape: 0.2547 - mae: 0.2253 - mse: 0.1320\n",
            "Epoch 11: loss improved from 0.39787 to 0.39331, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3933 - metric_mdape: 0.2541 - mae: 0.2248 - mse: 0.1326\n",
            "Epoch 12/100\n",
            "63/92 [===================>..........] - ETA: 0s - loss: 0.3828 - metric_mdape: 0.2457 - mae: 0.2153 - mse: 0.1358\n",
            "Epoch 12: loss improved from 0.39331 to 0.39049, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3905 - metric_mdape: 0.2558 - mae: 0.2211 - mse: 0.1316\n",
            "Epoch 13/100\n",
            "70/92 [=====================>........] - ETA: 0s - loss: 0.4020 - metric_mdape: 0.2599 - mae: 0.2256 - mse: 0.1367\n",
            "Epoch 13: loss did not improve from 0.39049\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3932 - metric_mdape: 0.2519 - mae: 0.2209 - mse: 0.1301\n",
            "Epoch 14/100\n",
            "72/92 [======================>.......] - ETA: 0s - loss: 0.3945 - metric_mdape: 0.2538 - mae: 0.2192 - mse: 0.1335\n",
            "Epoch 14: loss improved from 0.39049 to 0.38622, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3862 - metric_mdape: 0.2545 - mae: 0.2177 - mse: 0.1299\n",
            "Epoch 15/100\n",
            "76/92 [=======================>......] - ETA: 0s - loss: 0.3791 - metric_mdape: 0.2358 - mae: 0.2156 - mse: 0.1271\n",
            "Epoch 15: loss improved from 0.38622 to 0.37783, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3778 - metric_mdape: 0.2372 - mae: 0.2136 - mse: 0.1312\n",
            "Epoch 16/100\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 0.3845 - metric_mdape: 0.2378 - mae: 0.2093 - mse: 0.1155\n",
            "Epoch 16: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3909 - metric_mdape: 0.2444 - mae: 0.2165 - mse: 0.1262\n",
            "Epoch 17/100\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.3920 - metric_mdape: 0.2542 - mae: 0.2182 - mse: 0.1271\n",
            "Epoch 17: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3936 - metric_mdape: 0.2532 - mae: 0.2187 - mse: 0.1288\n",
            "Epoch 18/100\n",
            "61/92 [==================>...........] - ETA: 0s - loss: 0.3912 - metric_mdape: 0.2365 - mae: 0.2208 - mse: 0.1416\n",
            "Epoch 18: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3781 - metric_mdape: 0.2268 - mae: 0.2107 - mse: 0.1254\n",
            "Epoch 19/100\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 0.3833 - metric_mdape: 0.2401 - mae: 0.2203 - mse: 0.1381\n",
            "Epoch 19: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3847 - metric_mdape: 0.2453 - mae: 0.2156 - mse: 0.1301\n",
            "Epoch 20/100\n",
            "65/92 [====================>.........] - ETA: 0s - loss: 0.3997 - metric_mdape: 0.2491 - mae: 0.2234 - mse: 0.1361\n",
            "Epoch 20: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3970 - metric_mdape: 0.2482 - mae: 0.2212 - mse: 0.1325\n",
            "Epoch 21/100\n",
            "68/92 [=====================>........] - ETA: 0s - loss: 0.3781 - metric_mdape: 0.2359 - mae: 0.2088 - mse: 0.1161\n",
            "Epoch 21: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3867 - metric_mdape: 0.2506 - mae: 0.2190 - mse: 0.1322\n",
            "Epoch 22/100\n",
            "72/92 [======================>.......] - ETA: 0s - loss: 0.3728 - metric_mdape: 0.2276 - mae: 0.2098 - mse: 0.1280\n",
            "Epoch 22: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3819 - metric_mdape: 0.2300 - mae: 0.2122 - mse: 0.1282\n",
            "Epoch 23/100\n",
            "90/92 [============================>.] - ETA: 0s - loss: 0.3877 - metric_mdape: 0.2388 - mae: 0.2188 - mse: 0.1306\n",
            "Epoch 23: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3899 - metric_mdape: 0.2400 - mae: 0.2195 - mse: 0.1307\n",
            "Epoch 24/100\n",
            "89/92 [============================>.] - ETA: 0s - loss: 0.3874 - metric_mdape: 0.2436 - mae: 0.2183 - mse: 0.1335\n",
            "Epoch 24: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3848 - metric_mdape: 0.2410 - mae: 0.2172 - mse: 0.1315\n",
            "Epoch 25/100\n",
            "78/92 [========================>.....] - ETA: 0s - loss: 0.3866 - metric_mdape: 0.2319 - mae: 0.2196 - mse: 0.1397\n",
            "Epoch 25: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3909 - metric_mdape: 0.2370 - mae: 0.2198 - mse: 0.1324\n",
            "Epoch 26/100\n",
            "86/92 [===========================>..] - ETA: 0s - loss: 0.3765 - metric_mdape: 0.2281 - mae: 0.2075 - mse: 0.1221\n",
            "Epoch 26: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.3794 - metric_mdape: 0.2314 - mae: 0.2101 - mse: 0.1239\n",
            "Epoch 27/100\n",
            "87/92 [===========================>..] - ETA: 0s - loss: 0.3800 - metric_mdape: 0.2315 - mae: 0.2130 - mse: 0.1280\n",
            "Epoch 27: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3810 - metric_mdape: 0.2416 - mae: 0.2133 - mse: 0.1273\n",
            "Epoch 28/100\n",
            "83/92 [==========================>...] - ETA: 0s - loss: 0.3767 - metric_mdape: 0.2295 - mae: 0.2110 - mse: 0.1341\n",
            "Epoch 28: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3786 - metric_mdape: 0.2268 - mae: 0.2086 - mse: 0.1290\n",
            "Epoch 29/100\n",
            "61/92 [==================>...........] - ETA: 0s - loss: 0.3669 - metric_mdape: 0.2093 - mae: 0.1992 - mse: 0.1103\n",
            "Epoch 29: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3798 - metric_mdape: 0.2267 - mae: 0.2106 - mse: 0.1272\n",
            "Epoch 30/100\n",
            "64/92 [===================>..........] - ETA: 0s - loss: 0.3902 - metric_mdape: 0.2430 - mae: 0.2139 - mse: 0.1235\n",
            "Epoch 30: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3879 - metric_mdape: 0.2517 - mae: 0.2182 - mse: 0.1315\n",
            "Epoch 31/100\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 0.3924 - metric_mdape: 0.2377 - mae: 0.2211 - mse: 0.1373\n",
            "Epoch 31: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3900 - metric_mdape: 0.2371 - mae: 0.2173 - mse: 0.1322\n",
            "Epoch 32/100\n",
            "69/92 [=====================>........] - ETA: 0s - loss: 0.3873 - metric_mdape: 0.2447 - mae: 0.2179 - mse: 0.1244\n",
            "Epoch 32: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3962 - metric_mdape: 0.2424 - mae: 0.2219 - mse: 0.1322\n",
            "Epoch 33/100\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 0.3779 - metric_mdape: 0.2334 - mae: 0.2074 - mse: 0.1181\n",
            "Epoch 33: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3809 - metric_mdape: 0.2299 - mae: 0.2111 - mse: 0.1265\n",
            "Epoch 34/100\n",
            "67/92 [====================>.........] - ETA: 0s - loss: 0.3861 - metric_mdape: 0.2341 - mae: 0.2116 - mse: 0.1252\n",
            "Epoch 34: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3795 - metric_mdape: 0.2316 - mae: 0.2109 - mse: 0.1269\n",
            "Epoch 35/100\n",
            "67/92 [====================>.........] - ETA: 0s - loss: 0.3895 - metric_mdape: 0.2417 - mae: 0.2181 - mse: 0.1447\n",
            "Epoch 35: loss did not improve from 0.37783\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.3805 - metric_mdape: 0.2360 - mae: 0.2105 - mse: 0.1288\n",
            "Epoch 35: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x13a6d8190>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=smape_loss,\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[metric_mdape, \"mae\", \"mse\"]) # Backpropagation\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=20)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.hdf5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=100, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_99 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_534 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_535 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_536 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19\n",
            "Trainable params: 19\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return standardised_x[-horizon:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_on_test(model, df_train=df_train, df_test=df_test, horizon=6):\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "    \n",
        "    for i in range(len(df_train)):\n",
        "        window = de_standardise(df_train[i][10:14])\n",
        "        labels = df_test[i][0:horizon]\n",
        "        test_preds = autoregression(model, window, 1)\n",
        "        print(f\"Destandardised test pred: {de_standardise(test_preds)}\")\n",
        "        print(f\"Labels: {labels}\")\n",
        "        print(f\"Full Labels: {df_test[i]}\")\n",
        "        print(f\"Window: {window}\")\n",
        "        smape_score, mdape_score = evaluate_pred(labels, de_standardise(test_preds))\n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        print(f\"Current mean SMAPE: {smape_score}, Current mean MDAPE: {mdape_score}\")\n",
        "\n",
        "    mean_smape_score = np.mean(smape_scores)\n",
        "    mean_mdape_score = np.mean(mdape_scores)\n",
        "    return mean_smape_score, mean_mdape_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n",
            "Destandardised test pred: [5119.34595957]\n",
            "Labels: [5379.75 6158.68 6876.58 7851.91 8407.84 9156.01]\n",
            "Full Labels: [5379.75 6158.68 6876.58 7851.91 8407.84 9156.01]\n",
            "Window: [3307.9736059  3720.19013103 4271.03427293 4820.44774776]\n",
            "Current mean SMAPE: 33.332707749615956, Current mean MDAPE: 30.177580270552845\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [4526.48378177]\n",
            "Labels: [4793.2 5602.  5065.  5056.  5067.2 5209.6]\n",
            "Full Labels: [4793.2 5602.  5065.  5056.  5067.2 5209.6]\n",
            "Window: [5390.19542625 3959.64919959 4021.64860993 4193.53850674]\n",
            "Current mean SMAPE: 12.424497480243168, Current mean MDAPE: 10.651507171698801\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [3850.68141368]\n",
            "Labels: [3070.2 3601.6 3407.4 3500.6 3437.8 3007. ]\n",
            "Full Labels: [3070.2 3601.6 3407.4 3500.6 3437.8 3007. ]\n",
            "Window: [4426.10100329 3164.92267558 3240.39146823 3475.35850694]\n",
            "Current mean SMAPE: 14.485572660909618, Current mean MDAPE: 12.509710819179498\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Destandardised test pred: [4927.01810769]\n",
            "Labels: [4656.   5228.52 5656.72 5077.02 5403.4  5009.52]\n",
            "Full Labels: [4656.   5228.52 5656.72 5077.02 5403.4  5009.52]\n",
            "Window: [3790.67452856 3232.7645425  3739.67379494 4476.09041411]\n",
            "Current mean SMAPE: 6.544219122300997, Current mean MDAPE: 5.793661000074529\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Destandardised test pred: [5635.8129705]\n",
            "Labels: [5250.9 4899.2 4317.9 4007.9 4323.4 4819.4]\n",
            "Full Labels: [5250.9 4899.2 4317.9 4007.9 4323.4 4819.4]\n",
            "Window: [5154.30206755 4476.64536182 4595.32543832 5332.44195171]\n",
            "Current mean SMAPE: 20.54506431406297, Current mean MDAPE: 23.64808785511765\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [4318.28730088]\n",
            "Labels: [4338.03 4185.92 3950.49 3715.59 3958.3  4140.39]\n",
            "Full Labels: [4338.03 4185.92 3950.49 3715.59 3958.3  4140.39]\n",
            "Window: [3737.22536912 3739.4896896  3806.57894568 4044.22106549]\n",
            "Current mean SMAPE: 6.729045711703523, Current mean MDAPE: 6.695562052705226\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [5306.32260342]\n",
            "Labels: [5163.21 5089.94 4786.4  4926.47 5068.83 5318.19]\n",
            "Full Labels: [5163.21 5089.94 4786.4  4926.47 5068.83 5318.19]\n",
            "Window: [5627.37711588 4604.57749528 4612.94506167 5038.28613579]\n",
            "Current mean SMAPE: 4.9042048741648525, Current mean MDAPE: 4.468267651159029\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [8116.39537112]\n",
            "Labels: [8809.8  8953.5  7991.01 7294.08 7163.37 7478.31]\n",
            "Full Labels: [8809.8  8953.5  7991.01 7294.08 7163.37 7478.31]\n",
            "Window: [3067.8221856  3484.49488248 6509.07285443 7976.99299953]\n",
            "Current mean SMAPE: 8.481328074237082, Current mean MDAPE: 8.940974505706919\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Destandardised test pred: [5294.3630008]\n",
            "Labels: [6277.6  7446.28 6741.69 6600.69 7850.17 8485.26]\n",
            "Full Labels: [6277.6  7446.28 6741.69 6600.69 7850.17 8485.26]\n",
            "Window: [4246.14700257 4587.79022545 4040.55928945 4941.36912565]\n",
            "Current mean SMAPE: 30.33148005105073, Current mean MDAPE: 25.18376674874492\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Destandardised test pred: [8928.27465748]\n",
            "Labels: [10042.9 10173.   8751.   3472.   3187.   3058. ]\n",
            "Full Labels: [10042.9 10173.   8751.   3472.   3187.   3058. ]\n",
            "Window: [8079.30490622 8749.77835859 9240.01121275 9496.57696223]\n",
            "Current mean SMAPE: 51.25316053956056, Current mean MDAPE: 84.6931728557903\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Destandardised test pred: [5157.67284457]\n",
            "Labels: [5388.4 5766.2 5937.9 5627.4 5982.7 5931.6]\n",
            "Full Labels: [5388.4 5766.2 5937.9 5627.4 5982.7 5931.6]\n",
            "Window: [2818.92714638 3398.21975062 3838.17595636 4748.33579236]\n",
            "Current mean SMAPE: 11.176814874861234, Current mean MDAPE: 11.800437896438343\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [5134.42074278]\n",
            "Labels: [ 5949.5  6616.5  7247.   8205.  10158.5 12495.5]\n",
            "Full Labels: [ 5949.5  6616.5  7247.   8205.  10158.5 12495.5]\n",
            "Window: [3190.14501883 3487.8503783  3947.4988162  4746.09979284]\n",
            "Current mean SMAPE: 44.88438074737648, Current mean MDAPE: 33.287178096013584\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [5976.26338182]\n",
            "Labels: [6271.  6901.8 6479.2 6452.3 6271.6 6405.2]\n",
            "Full Labels: [6271.  6901.8 6479.2 6452.3 6271.6 6405.2]\n",
            "Window: [4912.2154662  5087.76270658 5363.26097365 5862.25317039]\n",
            "Current mean SMAPE: 7.779071125617677, Current mean MDAPE: 7.037237721817098\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Destandardised test pred: [4336.18892585]\n",
            "Labels: [4010.8 4120.8 4084.2 4330.2 4552.4 4724. ]\n",
            "Full Labels: [4010.8 4120.8 4084.2 4330.2 4552.4 4724. ]\n",
            "Window: [2414.38738114 1675.7752917  1704.17567605 3491.80574331]\n",
            "Current mean SMAPE: 5.406568796727881, Current mean MDAPE: 5.698359736043265\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [5412.89024784]\n",
            "Labels: [5995.  6510.5 6670.  6941.5 7010.  7539.5]\n",
            "Full Labels: [5995.  6510.5 6670.  6941.5 7010.  7539.5]\n",
            "Window: [3899.20042992 4230.68507524 4629.83238817 5174.88316793]\n",
            "Current mean SMAPE: 22.120039942323046, Current mean MDAPE: 20.4342700704133\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [5856.79068563]\n",
            "Labels: [6658.3 7101.4 7375.8 5748.3 5915.4 5759.3]\n",
            "Full Labels: [6658.3 7101.4 7375.8 5748.3 5915.4 5759.3]\n",
            "Window: [5705.89155752 5548.07602257 5812.01330395 5864.71157819]\n",
            "Current mean SMAPE: 9.920148052784013, Current mean MDAPE: 6.962549810365068\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [5885.46224133]\n",
            "Labels: [6259.05 6189.35 5963.7  5454.85 5420.   5561.65]\n",
            "Full Labels: [6259.05 6189.35 5963.7  5454.85 5420.   5561.65]\n",
            "Window: [5471.5835546  6029.46212205 6056.05837136 5987.65896499]\n",
            "Current mean SMAPE: 5.665422945331646, Current mean MDAPE: 5.895497363104055\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [8755.30255523]\n",
            "Labels: [7062.4 6891.6 6413.6 5775.2 4983.4 4782.4]\n",
            "Full Labels: [7062.4 6891.6 6413.6 5775.2 4983.4 4782.4]\n",
            "Window: [6563.52126297 6496.92622102 6444.68208868 8590.25786552]\n",
            "Current mean SMAPE: 38.453971639452874, Current mean MDAPE: 44.05661791627078\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [5603.86220037]\n",
            "Labels: [5751.05 5261.98 6275.53 6026.32 5260.44 6278.  ]\n",
            "Full Labels: [5751.05 5261.98 6275.53 6026.32 5260.44 6278.  ]\n",
            "Window: [4945.90382395 5385.30391919 5623.29668586 5615.07417003]\n",
            "Current mean SMAPE: 7.5212709128747575, Current mean MDAPE: 6.76930246682862\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Destandardised test pred: [9258.01241586]\n",
            "Labels: [10045.3 10056.3  8687.2  8423.   7742.5  7399.7]\n",
            "Full Labels: [10045.3 10056.3  8687.2  8423.   7742.5  7399.7]\n",
            "Window: [4979.922578   7003.19170115 9238.7010477  9785.92268335]\n",
            "Current mean SMAPE: 12.0618469779135, Current mean MDAPE: 8.925831906164818\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [8558.54091491]\n",
            "Labels: [ 9806.85 10774.75 10476.5  11016.   10540.5  10580.5 ]\n",
            "Full Labels: [ 9806.85 10774.75 10476.5  11016.   10540.5  10580.5 ]\n",
            "Window: [7584.5531377  7790.8005491  8446.09759609 8939.28457932]\n",
            "Current mean SMAPE: 20.610800252531806, Current mean MDAPE: 18.95675761891264\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Destandardised test pred: [5284.52901354]\n",
            "Labels: [5052.8  4512.15 4350.8  4609.33 4772.4  5199.  ]\n",
            "Full Labels: [5052.8  4512.15 4350.8  4609.33 4772.4  5199.  ]\n",
            "Window: [2620.94071032 2790.11462431 3371.57905925 4716.71758537]\n",
            "Current mean SMAPE: 10.849670226096949, Current mean MDAPE: 12.689793210049691\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Destandardised test pred: [3637.82593294]\n",
            "Labels: [3434.8 3644.2 4183.4 4896.2 4876.4 3502.2]\n",
            "Full Labels: [3434.8 3644.2 4183.4 4896.2 4876.4 3502.2]\n",
            "Window: [6287.00649559 4166.91613673 3665.88032393 3412.04405669]\n",
            "Current mean SMAPE: 13.708559813615503, Current mean MDAPE: 9.476127498614872\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [6048.39976218]\n",
            "Labels: [6659.25 6907.26 7032.42 6788.04 8569.5  8602.47]\n",
            "Full Labels: [6659.25 6907.26 7032.42 6788.04 8569.5  8602.47]\n",
            "Window: [4482.76884207 5025.96629736 5235.51935204 5906.97913661]\n",
            "Current mean SMAPE: 19.800129523584317, Current mean MDAPE: 13.213396518947098\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Destandardised test pred: [5054.88358444]\n",
            "Labels: [5283.64 5789.4  5862.2  6434.6  6655.4  6970.6 ]\n",
            "Full Labels: [5283.64 5789.4  5862.2  6434.6  6655.4  6970.6 ]\n",
            "Window: [3619.17420575 3903.86998601 4201.27424755 4747.80849332]\n",
            "Current mean SMAPE: 19.32918364433486, Current mean MDAPE: 17.606853907210112\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [8262.42985043]\n",
            "Labels: [8839. 8757. 7748. 7596. 7232. 8216.]\n",
            "Full Labels: [8839. 8757. 7748. 7596. 7232. 8216.]\n",
            "Window: [5663.44461935 5904.80280839 6640.86549147 8198.57646101]\n",
            "Current mean SMAPE: 6.874978599786158, Current mean MDAPE: 6.581271302111571\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [3540.24090846]\n",
            "Labels: [3676.83 4721.37 5243.28 6276.36 7026.27 8885.16]\n",
            "Full Labels: [3676.83 4721.37 5243.28 6276.36 7026.27 8885.16]\n",
            "Window: [1991.66177032 2073.86330981 2183.43707922 3026.58948886]\n",
            "Current mean SMAPE: 46.48597975790945, Current mean MDAPE: 38.037228998764256\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Destandardised test pred: [10755.0998147]\n",
            "Labels: [12523.5 12166.  12881.5 13480.5  7086.5  7598. ]\n",
            "Full Labels: [12523.5 12166.  12881.5 13480.5  7086.5  7598. ]\n",
            "Window: [ 6414.79959889  8030.31492981  8636.84261272 10971.72245723]\n",
            "Current mean SMAPE: 23.919285462189343, Current mean MDAPE: 18.362374063398658\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Destandardised test pred: [3748.7851996]\n",
            "Labels: [4227. 4481. 4395. 4602. 4829. 4841.]\n",
            "Full Labels: [4227. 4481. 4395. 4602. 4829. 4841.]\n",
            "Window: [5572.64066184 6057.21211582 6964.53712027 4372.76899605]\n",
            "Current mean SMAPE: 19.4512179920895, Current mean MDAPE: 17.440259937733803\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [5567.4569243]\n",
            "Labels: [6747.02 6775.18 7079.44 7873.68 8304.85 8966.21]\n",
            "Full Labels: [6747.02 6775.18 7079.44 7873.68 8304.85 8966.21]\n",
            "Window: [4659.26893999 4879.7428145  5002.27774254 5413.33585494]\n",
            "Current mean SMAPE: 30.531706770146588, Current mean MDAPE: 25.323832221607965\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Destandardised test pred: [6716.20615074]\n",
            "Labels: [ 7928.83  7751.34  8468.2   9231.5   9811.36 11374.3 ]\n",
            "Full Labels: [ 7928.83  7751.34  8468.2   9231.5   9811.36 11374.3 ]\n",
            "Window: [4746.03924157 5232.71561728 5679.78673302 6602.63442828]\n",
            "Current mean SMAPE: 29.07375655507098, Current mean MDAPE: 23.967976776342905\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Destandardised test pred: [5494.98906396]\n",
            "Labels: [5025.  5062.6 4878.6 5099.6 5050.8 5505.4]\n",
            "Full Labels: [5025.  5062.6 4878.6 5099.6 5050.8 5505.4]\n",
            "Window: [3310.44856538 3747.88949894 4558.13066231 5220.16786591]\n",
            "Current mean SMAPE: 7.514551125327301, Current mean MDAPE: 8.667639855369647\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [4358.24355622]\n",
            "Labels: [4421.52 4417.08 3819.84 3816.98 3472.72 4063.7 ]\n",
            "Full Labels: [4421.52 4417.08 3819.84 3816.98 3472.72 4063.7 ]\n",
            "Window: [2399.3642242  2163.17973924 2614.66945437 3737.91565451]\n",
            "Current mean SMAPE: 9.800256149336768, Current mean MDAPE: 10.671543111200906\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Destandardised test pred: [7572.51447853]\n",
            "Labels: [8061.94 8551.94 9303.02 9592.14 9611.4  9505.  ]\n",
            "Full Labels: [8061.94 8551.94 9303.02 9592.14 9611.4  9505.  ]\n",
            "Window: [8003.60932356 7243.69413162 7036.80547303 7687.99820778]\n",
            "Current mean SMAPE: 18.135449596769497, Current mean MDAPE: 19.466398974477393\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [3850.68141368]\n",
            "Labels: [3070.2 3601.6 3407.4 3500.6 3437.8 3007. ]\n",
            "Full Labels: [3070.2 3601.6 3407.4 3500.6 3437.8 3007. ]\n",
            "Window: [4426.10101977 3164.92267846 3240.39146873 3475.35850703]\n",
            "Current mean SMAPE: 14.485572660909618, Current mean MDAPE: 12.509710819179498\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [3265.26120512]\n",
            "Labels: [ 3274.64  3970.12  5589.92  9170.12  8793.24 10438.96]\n",
            "Full Labels: [ 3274.64  3970.12  5589.92  9170.12  8793.24 10438.96]\n",
            "Window: [1608.34434066 1984.45825962 2406.87619543 2863.5333342 ]\n",
            "Current mean SMAPE: 60.6037272837599, Current mean MDAPE: 52.22642682947387\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [2211.45101297]\n",
            "Labels: [2509.23 2835.65 3193.03 3756.92 5400.23 8299.04]\n",
            "Full Labels: [2509.23 2835.65 3193.03 3756.92 5400.23 8299.04]\n",
            "Window: [ 857.39894038  765.74231457  853.58640505 1645.75437088]\n",
            "Current mean SMAPE: 54.18149187756526, Current mean MDAPE: 35.93894866604459\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Destandardised test pred: [7497.08377215]\n",
            "Labels: [ 8388.45  8570.2   8858.75  9702.85 11116.85 12344.85]\n",
            "Full Labels: [ 8388.45  8570.2   8858.75  9702.85 11116.85 12344.85]\n",
            "Window: [5002.9333234  5843.0445816  6732.52280178 7568.47649031]\n",
            "Current mean SMAPE: 25.77269073956722, Current mean MDAPE: 19.052020287502813\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [3457.84473531]\n",
            "Labels: [3464.8 5180.4 5090.2 5245.8 5217.4 5713.4]\n",
            "Full Labels: [3464.8 5180.4 5090.2 5245.8 5217.4 5713.4]\n",
            "Window: [1850.71317631 2190.35480585 2801.45309102 3100.55429093]\n",
            "Current mean SMAPE: 34.85220341674079, Current mean MDAPE: 33.48807399778203\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [2747.83497401]\n",
            "Labels: [2825.79 2994.88 3546.68 5304.05 6379.21 7146.98]\n",
            "Full Labels: [2825.79 2994.88 3546.68 5304.05 6379.21 7146.98]\n",
            "Window: [1066.05069687 1565.49687195 1744.22745259 2312.6133042 ]\n",
            "Current mean SMAPE: 44.794837929012424, Current mean MDAPE: 35.35869310712135\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [5305.49024745]\n",
            "Labels: [6462.07 5119.36 4035.35 4815.54 7010.89 4328.  ]\n",
            "Full Labels: [6462.07 5119.36 4035.35 4815.54 7010.89 4328.  ]\n",
            "Window: [3282.49658071 3362.07890162 4011.94880778 4900.07054136]\n",
            "Current mean SMAPE: 18.015334504827678, Current mean MDAPE: 20.241620363597132\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [4071.09244429]\n",
            "Labels: [3889.  3926.5 4380.  4951.  4854.  5091. ]\n",
            "Full Labels: [3889.  3926.5 4380.  4951.  4854.  5091. ]\n",
            "Window: [2280.22671307 2659.49842479 3110.60972434 3669.04420176]\n",
            "Current mean SMAPE: 12.469122226725633, Current mean MDAPE: 11.590902690524832\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [7320.18948181]\n",
            "Labels: [ 8887.5   9787.14  9683.85 10154.67  9313.98  9246.51]\n",
            "Full Labels: [ 8887.5   9787.14  9683.85 10154.67  9313.98  9246.51]\n",
            "Window: [4521.55818429 5489.76318225 6550.12480689 7364.53609121]\n",
            "Current mean SMAPE: 25.941764269331923, Current mean MDAPE: 22.907350875002074\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [4829.10062829]\n",
            "Labels: [4598.9 4356.6 4016.2 4734.  5037.7 5105.6]\n",
            "Full Labels: [4598.9 4356.6 4016.2 4734.  5037.7 5105.6]\n",
            "Window: [4046.01814396 4186.33567519 4651.77124316 4659.21996755]\n",
            "Current mean SMAPE: 7.555838182740819, Current mean MDAPE: 5.210584111047831\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [5271.87549658]\n",
            "Labels: [5434.38 6037.95 6516.51 7095.96 7396.23 7678.38]\n",
            "Full Labels: [5434.38 6037.95 6516.51 7095.96 7396.23 7678.38]\n",
            "Window: [3421.49945777 3844.89815511 4588.20817714 5033.627181  ]\n",
            "Current mean SMAPE: 22.983376451524165, Current mean MDAPE: 22.402833976450925\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [5466.9707188]\n",
            "Labels: [5383.5  5640.85 5708.2  5906.7  6017.45 6952.3 ]\n",
            "Full Labels: [5383.5  5640.85 5708.2  5906.7  6017.45 6952.3 ]\n",
            "Window: [3916.01459455 4252.06630405 4580.32035321 5211.38731181]\n",
            "Current mean SMAPE: 8.370875417732133, Current mean MDAPE: 5.835298884723084\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [4613.11729274]\n",
            "Labels: [4734.9 5266.5 5429.7 5457.9 5872.5 6412.2]\n",
            "Full Labels: [4734.9 5266.5 5429.7 5457.9 5872.5 6412.2]\n",
            "Window: [6020.66810585 3860.51441852 3613.66252324 4141.94094157]\n",
            "Current mean SMAPE: 17.587929523717573, Current mean MDAPE: 15.258675178240866\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [4444.48367844]\n",
            "Labels: [5101.18 5934.92 6366.5  6574.92 6989.74 8629.08]\n",
            "Full Labels: [5101.18 5934.92 6366.5  6574.92 6989.74 8629.08]\n",
            "Window: [2227.57015079 2581.66827639 3185.21494837 3968.63911596]\n",
            "Current mean SMAPE: 37.53966929196269, Current mean MDAPE: 31.295997959472473\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [6158.02067842]\n",
            "Labels: [6402.27 7184.37 7854.54 7973.55 8616.99 8504.07]\n",
            "Full Labels: [6402.27 7184.37 7854.54 7973.55 8616.99 8504.07]\n",
            "Window: [2559.22689399 2968.47895645 3348.97806738 5492.18941179]\n",
            "Current mean SMAPE: 22.411670799886156, Current mean MDAPE: 22.184308583204494\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [5402.79863239]\n",
            "Labels: [6167.5 5986.5 5006.5 4556.  4466.5 4350.5]\n",
            "Full Labels: [6167.5 5986.5 5006.5 4556.  4466.5 4350.5]\n",
            "Window: [4333.06338388 4051.14159218 4487.44977863 5104.67371126]\n",
            "Current mean SMAPE: 14.773497520993532, Current mean MDAPE: 15.49266827138394\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Destandardised test pred: [5212.2912393]\n",
            "Labels: [5888.6 7721.4 7292.2 7492.8 6424.6 6068. ]\n",
            "Full Labels: [5888.6 7721.4 7292.2 7492.8 6424.6 6068. ]\n",
            "Window: [7837.87581586 5389.54326778 5235.64507186 5073.57788758]\n",
            "Current mean SMAPE: 26.026237293327032, Current mean MDAPE: 23.69608520627471\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [5944.54026044]\n",
            "Labels: [6089.81 6426.15 6852.72 7395.21 8045.42 8543.22]\n",
            "Full Labels: [6089.81 6426.15 6852.72 7395.21 8045.42 8543.22]\n",
            "Window: [3473.13764805 4572.06433841 5387.52375922 5859.37940786]\n",
            "Current mean SMAPE: 18.67528773985798, Current mean MDAPE: 16.434589341330298\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Destandardised test pred: [7455.41088925]\n",
            "Labels: [8987.5  9001.75 7292.25 7216.75 7217.   7242.  ]\n",
            "Full Labels: [8987.5  9001.75 7292.25 7216.75 7217.   7242.  ]\n",
            "Window: [5299.10871896 5506.71277582 6239.13723577 7382.06151626]\n",
            "Current mean SMAPE: 8.174557465622943, Current mean MDAPE: 3.3052518089291896\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [6260.21693063]\n",
            "Labels: [6781.45 7420.2  8232.45 8585.2  8170.85 9184.75]\n",
            "Full Labels: [6781.45 7420.2  8232.45 8585.2  8170.85 9184.75]\n",
            "Window: [5192.672635   5401.13146113 5590.1130057  6175.522276  ]\n",
            "Current mean SMAPE: 24.640168802973424, Current mean MDAPE: 23.670173475297833\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [3374.51505604]\n",
            "Labels: [3204.4  2357.91 2576.76 2922.   3391.19 2833.4 ]\n",
            "Full Labels: [3204.4  2357.91 2576.76 2922.   3391.19 2833.4 ]\n",
            "Window: [4161.23925244 3508.12036918 2922.27106461 3092.73993631]\n",
            "Current mean SMAPE: 16.624860453601165, Current mean MDAPE: 17.29210643475189\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [5500.17233838]\n",
            "Labels: [7520.38 8417.04 8164.52 8111.18 8383.62 9360.96]\n",
            "Full Labels: [7520.38 8417.04 8164.52 8111.18 8383.62 9360.96]\n",
            "Window: [3679.25594518 4149.24529041 4516.01942582 5227.26889952]\n",
            "Current mean SMAPE: 40.634130663892535, Current mean MDAPE: 33.513534481111265\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [4013.61771585]\n",
            "Labels: [4334.   4479.5  5005.35 5873.65 5968.25 7016.2 ]\n",
            "Full Labels: [4334.   4479.5  5005.35 5873.65 5968.25 7016.2 ]\n",
            "Window: [2625.94006815 2841.80951193 3243.65916459 3653.2641038 ]\n",
            "Current mean SMAPE: 28.645376804518623, Current mean MDAPE: 25.740423023310765\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Destandardised test pred: [4518.09904447]\n",
            "Labels: [4937.6 5960.8 6573.6 7488.  8069.4 9598.4]\n",
            "Full Labels: [4937.6 5960.8 6573.6 7488.  8069.4 9598.4]\n",
            "Window: [3386.88455715 3694.7047975  4132.36333956 4274.45858442]\n",
            "Current mean SMAPE: 41.89132112210404, Current mean MDAPE: 35.465584410381965\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [4838.11858985]\n",
            "Labels: [5103.1 5781.1 6189.1 6599.  6738.  7425. ]\n",
            "Full Labels: [5103.1 5781.1 6189.1 6599.  6738.  7425. ]\n",
            "Window: [4228.01158303 4492.66202703 4831.19085473 4722.95839958]\n",
            "Current mean SMAPE: 25.56661677947804, Current mean MDAPE: 24.256231441624042\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [6423.89686913]\n",
            "Labels: [7277.95 7191.25 6512.65 6534.9  6551.   8267.35]\n",
            "Full Labels: [7277.95 7191.25 6512.65 6534.9  6551.   8267.35]\n",
            "Window: [5234.50452511 5883.52829189 6325.99370108 6523.75139769]\n",
            "Current mean SMAPE: 8.979792334854174, Current mean MDAPE: 6.3054298883963\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [7326.30723729]\n",
            "Labels: [7745. 8205. 8636. 8734. 8940. 9608.]\n",
            "Full Labels: [7745. 8205. 8636. 8734. 8940. 9608.]\n",
            "Window: [7357.29838755 6522.15221782 7776.85163812 7624.34903667]\n",
            "Current mean SMAPE: 16.26661285401323, Current mean MDAPE: 15.641445600496981\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [6603.19926402]\n",
            "Labels: [7021.66 6923.6  6811.   7498.4  8495.8  9474.4 ]\n",
            "Full Labels: [7021.66 6923.6  6811.   7498.4  8495.8  9474.4 ]\n",
            "Window: [4311.63956946 4555.69142466 5363.38999931 6400.24174988]\n",
            "Current mean SMAPE: 14.576758055099667, Current mean MDAPE: 8.949063303615194\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [5390.11123622]\n",
            "Labels: [5156.69 5225.47 4591.06 5036.13 5587.5  6762.5 ]\n",
            "Full Labels: [5156.69 5225.47 4591.06 5036.13 5587.5  6762.5 ]\n",
            "Window: [3699.19179127 3695.84906347 4063.98133611 4993.13648382]\n",
            "Current mean SMAPE: 9.418625320865411, Current mean MDAPE: 5.777702704232392\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [2815.00317633]\n",
            "Labels: [2724.56 2914.08 3042.   3400.68 4276.46 4350.6 ]\n",
            "Full Labels: [2724.56 2914.08 3042.   3400.68 4276.46 4350.6 ]\n",
            "Window: [1828.11610558 1855.82281848 2155.03149323 2439.48201132]\n",
            "Current mean SMAPE: 19.56634913254265, Current mean MDAPE: 12.342217985147986\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Destandardised test pred: [5012.78684589]\n",
            "Labels: [4851.63 5563.15 6255.27 6941.23 7688.57 8523.72]\n",
            "Full Labels: [4851.63 5563.15 6255.27 6941.23 7688.57 8523.72]\n",
            "Window: [3404.34572498 3784.88875187 4280.58278158 4729.81773678]\n",
            "Current mean SMAPE: 27.00003091008931, Current mean MDAPE: 23.82271167748006\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Destandardised test pred: [6048.97107767]\n",
            "Labels: [6086.  6441.8 6102.  5690.6 5697.2 5863.2]\n",
            "Full Labels: [6086.  6441.8 6102.  5690.6 5697.2 5863.2]\n",
            "Window: [5056.80123129 4796.43021548 4935.83028771 5800.40530035]\n",
            "Current mean SMAPE: 3.831155857013243, Current mean MDAPE: 4.6332741266368425\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [4470.48852646]\n",
            "Labels: [4452.2 4772.8 4898.4 4851.2 4778.8 4912.6]\n",
            "Full Labels: [4452.2 4772.8 4898.4 4851.2 4778.8 4912.6]\n",
            "Window: [4125.39713473 4286.60449858 4490.70578725 4335.68351277]\n",
            "Current mean SMAPE: 6.724076406355778, Current mean MDAPE: 7.149714936779297\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Destandardised test pred: [6342.50001061]\n",
            "Labels: [6197.76 8351.82 8716.8  9638.82 9430.02 8999.04]\n",
            "Full Labels: [6197.76 8351.82 8716.8  9638.82 9430.02 8999.04]\n",
            "Window: [6077.04170467 4815.28529832 6259.53417721 6336.30523101]\n",
            "Current mean SMAPE: 29.37068414862734, Current mean MDAPE: 28.37923319022895\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [4599.85906855]\n",
            "Labels: [4793.3 4382.1 4128.3 5070.7 5876.7 5782.4]\n",
            "Full Labels: [4793.3 4382.1 4128.3 5070.7 5876.7 5782.4]\n",
            "Window: [4684.00796983 4664.92739472 4755.57229408 4502.23765146]\n",
            "Current mean SMAPE: 12.77764666407957, Current mean MDAPE: 10.354059137321286\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [3822.01327027]\n",
            "Labels: [4142.63 4784.77 4729.16 4559.7  4512.97 4294.07]\n",
            "Full Labels: [4142.63 4784.77 4729.16 4559.7  4512.97 4294.07]\n",
            "Window: [3274.72217275 3488.33763023 3394.70208529 3572.52086493]\n",
            "Current mean SMAPE: 16.24242226508345, Current mean MDAPE: 15.744435381040542\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [6591.86021141]\n",
            "Labels: [7530.76 8351.41 8655.88 9381.43 9142.76 8963.63]\n",
            "Full Labels: [7530.76 8351.41 8655.88 9381.43 9142.76 8963.63]\n",
            "Window: [6123.81514836 6042.83715096 6092.71900142 6593.62682525]\n",
            "Current mean SMAPE: 26.9609216324014, Current mean MDAPE: 25.152608154988364\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [7153.41410528]\n",
            "Labels: [8321.9 8770.7 8756.  8503.  8414.5 8786.2]\n",
            "Full Labels: [8321.9 8770.7 8756.  8503.  8414.5 8786.2]\n",
            "Window: [6811.67018612 6924.47228257 7320.24764945 7410.21583865]\n",
            "Current mean SMAPE: 18.248059807994377, Current mean MDAPE: 17.087298104241572\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [6026.8369896]\n",
            "Labels: [6145.56 5822.06 6043.04 6296.06 6752.52 7243.14]\n",
            "Full Labels: [6145.56 5822.06 6043.04 6296.06 6752.52 7243.14]\n",
            "Window: [2428.17126984 2928.02947222 5272.70465764 5835.92831509]\n",
            "Current mean SMAPE: 6.622315622956622, Current mean MDAPE: 3.896657692004721\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Destandardised test pred: [4283.6603174]\n",
            "Labels: [4308.62 4951.6  4842.88 4731.   5177.76 5649.54]\n",
            "Full Labels: [4308.62 4951.6  4842.88 4731.   5177.76 5649.54]\n",
            "Window: [5993.06965147 5281.68068901 4325.25762058 4177.9475836 ]\n",
            "Current mean SMAPE: 13.937781649045197, Current mean MDAPE: 12.518312670492474\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [6161.74544094]\n",
            "Labels: [6548.6 6573.  6207.  4222.  2635.  2673. ]\n",
            "Full Labels: [6548.6 6573.  6207.  4222.  2635.  2673. ]\n",
            "Window: [4193.3471136  4489.64824488 4936.92344285 5911.3941025 ]\n",
            "Current mean SMAPE: 34.966627120783414, Current mean MDAPE: 26.10024143032866\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [3984.00753559]\n",
            "Labels: [5037.5 5697.5 5986.5 6850.  8368.  9783.5]\n",
            "Full Labels: [5037.5 5697.5 5986.5 6850.  8368.  9783.5]\n",
            "Window: [2776.46989344 3079.26973135 3893.73470299 3773.91607302]\n",
            "Current mean SMAPE: 51.17688017193588, Current mean MDAPE: 37.644721632633974\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [4562.30726173]\n",
            "Labels: [4692.2 5301.2 5465.  5169.6 6558.  6914.2]\n",
            "Full Labels: [4692.2 5301.2 5465.  5169.6 6558.  6914.2]\n",
            "Window: [3507.0842721  3813.08974762 4145.98570583 4318.75249852]\n",
            "Current mean SMAPE: 20.85893094067449, Current mean MDAPE: 15.227962262744537\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [5560.25113454]\n",
            "Labels: [6246. 6875. 7174. 7262. 7500. 7992.]\n",
            "Full Labels: [6246. 6875. 7174. 7262. 7500. 7992.]\n",
            "Window: [4222.74749011 4506.75581077 4871.60726688 5367.7562717 ]\n",
            "Current mean SMAPE: 25.04042454936904, Current mean MDAPE: 22.96400998626232\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [7652.3426568]\n",
            "Labels: [8203.5 9098.5 3828.5 3203.5 3001.5 3365. ]\n",
            "Full Labels: [8203.5 9098.5 3828.5 3203.5 3001.5 3365. ]\n",
            "Window: [5914.44821216 6281.73968713 6816.96694525 7719.95671542]\n",
            "Current mean SMAPE: 56.321798280762266, Current mean MDAPE: 113.644118247668\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [5835.14991777]\n",
            "Labels: [6929.8 7320.  7506.6 6149.  6322.6 6220.4]\n",
            "Full Labels: [6929.8 7320.  7506.6 6149.  6322.6 6220.4]\n",
            "Window: [7100.71974582 6253.34595552 5978.00554222 5879.82596989]\n",
            "Current mean SMAPE: 14.071498868282742, Current mean MDAPE: 11.752959215170907\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [8321.7169944]\n",
            "Labels: [9371.6 9197.1 8942.5 8251.4 8047.6 8181.3]\n",
            "Full Labels: [9371.6 9197.1 8942.5 8251.4 8047.6 8181.3]\n",
            "Window: [8369.6517252  9182.46505191 8999.92138408 8924.41624221]\n",
            "Current mean SMAPE: 5.8253650464396705, Current mean MDAPE: 5.174067901168884\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [4306.78665178]\n",
            "Labels: [2652.4 2678.4 2487.4 2164.1 1845.2 1717.6]\n",
            "Full Labels: [2652.4 2678.4 2487.4 2164.1 1845.2 1717.6]\n",
            "Window: [2449.9729261  2437.20776207 2376.23385836 3679.37592521]\n",
            "Current mean SMAPE: 63.32243398955942, Current mean MDAPE: 86.07731593242536\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [3318.04122651]\n",
            "Labels: [3110.79 3272.7  3715.08 3721.51 3928.03 3994.  ]\n",
            "Full Labels: [3110.79 3272.7  3715.08 3721.51 3928.03 3994.  ]\n",
            "Window: [3154.13960778 3165.55793136 3233.16013799 3118.93877415]\n",
            "Current mean SMAPE: 10.983706320142122, Current mean MDAPE: 10.764377030134277\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [5029.02107782]\n",
            "Labels: [6223.4 6009.8 4951.3 4771.  4512.2 4531.7]\n",
            "Full Labels: [6223.4 6009.8 4951.3 4771.  4512.2 4531.7]\n",
            "Window: [2792.01274625 4344.54498059 4902.9503716  4950.59381503]\n",
            "Current mean SMAPE: 11.176422726262954, Current mean MDAPE: 11.21406600580929\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [6284.64749249]\n",
            "Labels: [6881.   7657.65 7811.5  8069.   7730.   7769.5 ]\n",
            "Full Labels: [6881.   7657.65 7811.5  8069.   7730.   7769.5 ]\n",
            "Window: [5306.52156013 5833.84377302 6025.57891028 6324.2875593 ]\n",
            "Current mean SMAPE: 19.506265144258936, Current mean MDAPE: 18.904631638100913\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [7293.6506098]\n",
            "Labels: [7219.31 6548.4  6621.26 7037.04 7249.14 7830.  ]\n",
            "Full Labels: [7219.31 6548.4  6621.26 7037.04 7249.14 7830.  ]\n",
            "Window: [4118.06001145 4347.938002   5111.59265035 6940.33496381]\n",
            "Current mean SMAPE: 5.457162452746781, Current mean MDAPE: 5.248249321556607\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [2875.25965551]\n",
            "Labels: [2362.8 2487.8 2728.4 3353.  3031.8 2058.6]\n",
            "Full Labels: [2362.8 2487.8 2728.4 3353.  3031.8 2058.6]\n",
            "Window: [5026.08915293 3064.66060176 2730.48560531 2577.12998093]\n",
            "Current mean SMAPE: 15.50049918969946, Current mean MDAPE: 14.911267755781154\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [3837.20187786]\n",
            "Labels: [4007.78 4109.02 4325.1  4122.88 5304.48 5445.02]\n",
            "Full Labels: [4007.78 4109.02 4325.1  4122.88 5304.48 5445.02]\n",
            "Window: [2641.07661908 2922.76740834 3123.30229646 3500.62240188]\n",
            "Current mean SMAPE: 16.17777320601281, Current mean MDAPE: 9.104855989972169\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [5444.26287789]\n",
            "Labels: [5948.76 6644.8  7047.6  7487.6  7803.8  8340.6 ]\n",
            "Full Labels: [5948.76 6644.8  7047.6  7487.6  7803.8  8340.6 ]\n",
            "Window: [4740.81696324 5010.18196857 4999.2078445  5309.91287279]\n",
            "Current mean SMAPE: 27.27205648613438, Current mean MDAPE: 25.019864518063827\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [7758.26245284]\n",
            "Labels: [8731. 9537. 9595. 8911. 8407. 8355.]\n",
            "Full Labels: [8731. 9537. 9595. 8911. 8407. 8355.]\n",
            "Window: [6415.62515321 6646.10940181 7072.54414532 7878.94522543]\n",
            "Current mean SMAPE: 13.800015914677118, Current mean MDAPE: 12.038655826685323\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [3926.063373]\n",
            "Labels: [3864.38 4235.66 4374.38 4880.84 6341.14 7311.92]\n",
            "Full Labels: [3864.38 4235.66 4374.38 4880.84 6341.14 7311.92]\n",
            "Window: [2778.89255473 2909.57869708 2983.05427199 3534.9839976 ]\n",
            "Current mean SMAPE: 24.826111077525567, Current mean MDAPE: 14.905208603606864\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [4079.84156583]\n",
            "Labels: [ 5014.47  5263.46  5595.9   6329.87  6867.47 11633.17]\n",
            "Full Labels: [ 5014.47  5263.46  5595.9   6329.87  6867.47 11633.17]\n",
            "Window: [3075.27219768 3573.10013459 3816.35127355 3873.39897287]\n",
            "Current mean SMAPE: 44.587700036847075, Current mean MDAPE: 31.31925519425589\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [4058.97880097]\n",
            "Labels: [3937.94 4435.61 5227.23 6273.5  7136.57 7449.37]\n",
            "Full Labels: [3937.94 4435.61 5227.23 6273.5  7136.57 7449.37]\n",
            "Window: [2459.98275145 2761.9882315  3146.60469051 3669.15582084]\n",
            "Current mean SMAPE: 32.30338607158439, Current mean MDAPE: 28.824474347546357\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [9012.91709496]\n",
            "Labels: [8917.5 9370.  9358.5 8923.5 8397.5 8804.5]\n",
            "Full Labels: [8917.5 9370.  9358.5 8923.5 8397.5 8804.5]\n",
            "Window: [5144.82843002 5967.98997525 7179.09824567 9008.45469299]\n",
            "Current mean SMAPE: 3.1862459456144445, Current mean MDAPE: 3.0299412160357884\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Destandardised test pred: [3224.49513932]\n",
            "Labels: [3013.44 3527.64 3879.96 3996.44 3850.54 4105.64]\n",
            "Full Labels: [3013.44 3527.64 3879.96 3996.44 3850.54 4105.64]\n",
            "Window: [2334.20783064 2353.78687036 2829.50920231 2918.8161104 ]\n",
            "Current mean SMAPE: 16.219670627694104, Current mean MDAPE: 16.576111723324484\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [7072.51592914]\n",
            "Labels: [7661.38 8816.56 9366.04 9715.2  9485.74 9974.  ]\n",
            "Full Labels: [7661.38 8816.56 9366.04 9715.2  9485.74 9974.  ]\n",
            "Window: [6825.17325172 6755.10331905 6976.79458083 7246.14505165]\n",
            "Current mean SMAPE: 25.420677619554592, Current mean MDAPE: 24.964103334367053\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [4333.75254801]\n",
            "Labels: [4214.2 4489.6 4578.  4512.4 4419.4 4507. ]\n",
            "Full Labels: [4214.2 4489.6 4578.  4512.4 4419.4 4507. ]\n",
            "Window: [3670.92889862 3875.25755726 4121.55507252 4134.79213769]\n",
            "Current mean SMAPE: 3.621088596075814, Current mean MDAPE: 3.6576315737585947\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [2723.54139138]\n",
            "Labels: [2689. 2801. 2465. 2135. 1801. 1710.]\n",
            "Full Labels: [2689. 2801. 2465. 2135. 1801. 1710.]\n",
            "Window: [2762.41110697 2726.59694372 2650.20446515 2513.9107814 ]\n",
            "Current mean SMAPE: 20.795736950161224, Current mean MDAPE: 19.027418047831354\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [4002.60502478]\n",
            "Labels: [3192.  4865.6 4231.  3781.4 2379.3 2723. ]\n",
            "Full Labels: [3192.  4865.6 4231.  3781.4 2379.3 2723. ]\n",
            "Window: [3531.46127082 4072.03322239 3810.23331392 3844.69082994]\n",
            "Current mean SMAPE: 23.691946200822148, Current mean MDAPE: 21.565777605381935\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [2831.04034829]\n",
            "Labels: [2385. 3145. 3157. 3463. 3539. 4167.]\n",
            "Full Labels: [2385. 3145. 3157. 3463. 3539. 4167.]\n",
            "Window: [2091.94235679 2153.03991244 2200.03198468 2480.50559732]\n",
            "Current mean SMAPE: 19.831151390198176, Current mean MDAPE: 18.475404231113405\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [5692.02612514]\n",
            "Labels: [6145. 7808. 7448. 8869. 8302. 9070.]\n",
            "Full Labels: [6145. 7808. 7448. 8869. 8302. 9070.]\n",
            "Window: [5667.93054806 6146.48784591 5504.23537303 5687.19119028]\n",
            "Current mean SMAPE: 32.071993670592086, Current mean MDAPE: 29.268984108693296\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [2426.10708807]\n",
            "Labels: [2093.4 2064.8 1809.7 1640.3 1937.  2370.7]\n",
            "Full Labels: [2093.4 2064.8 1809.7 1640.3 1937.  2370.7]\n",
            "Window: [1514.97911563 1758.04134523 1852.38723542 2082.1602662 ]\n",
            "Current mean SMAPE: 20.549560327414074, Current mean MDAPE: 21.374579587414743\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [4745.84285093]\n",
            "Labels: [4740. 5140. 4120. 3780. 3980. 4220.]\n",
            "Full Labels: [4740. 5140. 4120. 3780. 3980. 4220.]\n",
            "Window: [8607.53958886 7380.31942805 7281.05589991 5250.68478248]\n",
            "Current mean SMAPE: 12.359269699364022, Current mean MDAPE: 13.825545762063172\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [4860.86323479]\n",
            "Labels: [4277.9 4460.  4383.  4404.  4402.  5144. ]\n",
            "Full Labels: [4277.9 4460.  4383.  4404.  4402.  5144. ]\n",
            "Window: [2849.44735719 3389.53578751 4002.64626281 4525.08059599]\n",
            "Current mean SMAPE: 9.521389770110183, Current mean MDAPE: 10.398898149527986\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [7029.52785068]\n",
            "Labels: [7115.2 6958.4 5961.1 6322.8 6499.  7402. ]\n",
            "Full Labels: [7115.2 6958.4 5961.1 6322.8 6499.  7402. ]\n",
            "Window: [7014.7785076  6919.92123883 7203.0437168  7265.87265044]\n",
            "Current mean SMAPE: 7.0447370829226506, Current mean MDAPE: 6.597635112197913\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [5072.17757736]\n",
            "Labels: [5531.65 6406.35 5817.1  5525.3  5294.5  5297.4 ]\n",
            "Full Labels: [5531.65 6406.35 5817.1  5525.3  5294.5  5297.4 ]\n",
            "Window: [4421.26302423 4464.81227924 3959.69589887 4712.6767823 ]\n",
            "Current mean SMAPE: 10.463153590944435, Current mean MDAPE: 8.25355545131343\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [1706.37338599]\n",
            "Labels: [1650.75 2140.65 1757.35 1671.3  1747.   2085.95]\n",
            "Full Labels: [1650.75 2140.65 1757.35 1671.3  1747.   2085.95]\n",
            "Window: [1133.53895268 1332.12556672 1277.77822418 1381.99368923]\n",
            "Current mean SMAPE: 8.880381624340362, Current mean MDAPE: 3.1351744140441866\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [5534.64186887]\n",
            "Labels: [5655.28 9712.32 6237.2  6003.52 8888.88 9321.84]\n",
            "Full Labels: [5655.28 9712.32 6237.2  6003.52 8888.88 9321.84]\n",
            "Window: [5883.02800476 5214.85390083 4937.08143265 5345.98325071]\n",
            "Current mean SMAPE: 29.08574338981192, Current mean MDAPE: 24.49960773898005\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Destandardised test pred: [3731.25722081]\n",
            "Labels: [2545. 3450. 1740.  360. 3210. 2280.]\n",
            "Full Labels: [2545. 3450. 1740.  360. 3210. 2280.]\n",
            "Window: [6643.73928476 5564.02937483 5548.3301406  3982.2077746 ]\n",
            "Current mean SMAPE: 57.75516797029804, Current mean MDAPE: 55.13145909096306\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [5013.15439869]\n",
            "Labels: [5696.15 6483.45  742.6  5794.6  4617.2  5971.1 ]\n",
            "Full Labels: [5696.15 6483.45  742.6  5794.6  4617.2  5971.1 ]\n",
            "Window: [4231.47396841 4558.64919447 4935.75735903 4905.63878783]\n",
            "Current mean SMAPE: 37.808608285035056, Current mean MDAPE: 14.764394847490708\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [7897.06871875]\n",
            "Labels: [7612.4  199.6 7759.6 7802.  2109.6 6560.4]\n",
            "Full Labels: [7612.4  199.6 7759.6 7802.  2109.6 6560.4]\n",
            "Window: [9334.27875602 9609.2687823  8085.6020369  8326.83035646]\n",
            "Current mean SMAPE: 55.156753162172556, Current mean MDAPE: 12.057172428880092\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [7447.14972573]\n",
            "Labels: [8680. 8305. 7825. 8025. 7785. 9430.]\n",
            "Full Labels: [8680. 8305. 7825. 8025. 7785. 9430.]\n",
            "Window: [5539.47854997 5792.27624624 7024.63959309 7571.04567879]\n",
            "Current mean SMAPE: 11.088704718423987, Current mean MDAPE: 8.764974729970195\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [23154.00017886]\n",
            "Labels: [25805. 16960. 10480.  7475. 11160. 12505.]\n",
            "Full Labels: [25805. 16960. 10480.  7475. 11160. 12505.]\n",
            "Window: [ 4611.47387808  6754.22667866 14040.56341877 23125.24609828]\n",
            "Current mean SMAPE: 58.18165903335327, Current mean MDAPE: 96.31552906890835\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [4203.02903139]\n",
            "Labels: [5057.9 5982.1 7016.4 6686.8 6658.9 2828.4]\n",
            "Full Labels: [5057.9 5982.1 7016.4 6686.8 6658.9 2828.4]\n",
            "Window: [1418.05846528 2011.59773142 2683.809603   3660.18918052]\n",
            "Current mean SMAPE: 38.914005023253814, Current mean MDAPE: 37.01270970307634\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [2781.01880091]\n",
            "Labels: [3644.38 4284.78 4706.34 5213.12 5935.9  6479.36]\n",
            "Full Labels: [3644.38 4284.78 4706.34 5213.12 5935.9  6479.36]\n",
            "Window: [1460.57144743 1713.1935033  1921.97336308 2370.05283854]\n",
            "Current mean SMAPE: 55.66222268557822, Current mean MDAPE: 43.781281845741894\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [5208.09850504]\n",
            "Labels: [2481. 1598.  587. 1250. 1455.  618.]\n",
            "Full Labels: [2481. 1598.  587. 1250. 1455.  618.]\n",
            "Window: [6537.35040816 3998.53632143 4614.36885625 4879.71454984]\n",
            "Current mean SMAPE: 121.55044935904174, Current mean MDAPE: 287.2963974198063\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [8287.60770465]\n",
            "Labels: [ 8236.8  8328.8  8353.7 10565.4 11281.7  5541.9]\n",
            "Full Labels: [ 8236.8  8328.8  8353.7 10565.4 11281.7  5541.9]\n",
            "Window: [4689.92637853 5137.54961624 6702.45118284 8221.151457  ]\n",
            "Current mean SMAPE: 16.062766962240858, Current mean MDAPE: 11.175076117203526\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [7990.85902869]\n",
            "Labels: [7303.3  7809.05 3659.1  5924.35 5004.9  9385.95]\n",
            "Full Labels: [7303.3  7809.05 3659.1  5924.35 5004.9  9385.95]\n",
            "Window: [3917.9878293   887.93787013 4070.75662727 7088.14740977]\n",
            "Current mean SMAPE: 29.56149081590927, Current mean MDAPE: 24.87261292673873\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [6787.25059706]\n",
            "Labels: [5939.64 6334.18 1558.   5533.68 5446.26 6532.78]\n",
            "Full Labels: [5939.64 6334.18 1558.   5533.68 5446.26 6532.78]\n",
            "Window: [6552.46571886 5741.2385008  6336.82373764 6787.88115409]\n",
            "Current mean SMAPE: 31.94022335345491, Current mean MDAPE: 18.46193729935471\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Destandardised test pred: [6640.14368252]\n",
            "Labels: [8141.45 8500.   9115.7  8088.35 6772.2  9537.8 ]\n",
            "Full Labels: [8141.45 8500.   9115.7  8088.35 6772.2  9537.8 ]\n",
            "Window: [2780.89299967 3492.50252494 4723.27669187 6267.73717108]\n",
            "Current mean SMAPE: 22.29375950281641, Current mean MDAPE: 20.16047214777783\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [3147.77092982]\n",
            "Labels: [3043.   3373.08 4412.32 5624.    412.   4172.  ]\n",
            "Full Labels: [3043.   3373.08 4412.32 5624.    412.   4172.  ]\n",
            "Window: [2489.1675274  2630.3518173  2784.15506803 2873.0281369 ]\n",
            "Current mean SMAPE: 46.98294170101747, Current mean MDAPE: 26.604789285534135\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Destandardised test pred: [7451.96301001]\n",
            "Labels: [7795.55 5036.15 4601.45 6746.7  4827.35 6615.  ]\n",
            "Full Labels: [7795.55 5036.15 4601.45 6746.7  4827.35 6615.  ]\n",
            "Window: [6902.28885783 7833.84680012 8540.09069002 8017.70837075]\n",
            "Current mean SMAPE: 25.846177228183993, Current mean MDAPE: 30.310971590785414\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [5638.41167603]\n",
            "Labels: [5690. 6170. 3775. 7855. 7985. 9815.]\n",
            "Full Labels: [5690. 6170. 3775. 7855. 7985. 9815.]\n",
            "Window: [4973.19151856 1204.43351575 4368.77586526 5075.16077642]\n",
            "Current mean SMAPE: 28.477093607474707, Current mean MDAPE: 28.80313784692763\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [5482.59561254]\n",
            "Labels: [5395. 6004. 6129. 4791. 4753. 4206.]\n",
            "Full Labels: [5395. 6004. 6129. 4791. 4753. 4206.]\n",
            "Window: [4000.8061706  4038.90782986 6491.70062022 5661.98935854]\n",
            "Current mean SMAPE: 12.649161772469544, Current mean MDAPE: 12.490980770007178\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [4644.69929139]\n",
            "Labels: [4474.8 3816.8 2790.4 4068.  4270.8 3671.6]\n",
            "Full Labels: [4474.8 3816.8 2790.4 4068.  4270.8 3671.6]\n",
            "Window: [4221.8598031  3820.03546554 3349.75620647 4196.73733613]\n",
            "Current mean SMAPE: 19.700378508004867, Current mean MDAPE: 17.93370440911192\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Destandardised test pred: [4053.06602686]\n",
            "Labels: [ 4807.9  4719.   4450.2  6249.8  7761.4 10113.8]\n",
            "Full Labels: [ 4807.9  4719.   4450.2  6249.8  7761.4 10113.8]\n",
            "Window: [3234.02975235 3634.79020666 3863.81828617 3860.83320008]\n",
            "Current mean SMAPE: 38.75708387959684, Current mean MDAPE: 25.424368370445084\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [8424.5177101]\n",
            "Labels: [9325. 7108. 6192. 3748. 1649. 6165.]\n",
            "Full Labels: [9325. 7108. 6192. 3748. 1649. 6165.]\n",
            "Window: [1335.45370836 4060.87939896 5418.10389482 8089.36818159]\n",
            "Current mean SMAPE: 49.99657047844401, Current mean MDAPE: 36.35280295701881\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [8238.36245413]\n",
            "Labels: [9227.5 7182.  3622.  4894.5 6023.  8049.5]\n",
            "Full Labels: [9227.5 7182.  3622.  4894.5 6023.  8049.5]\n",
            "Window: [4350.61901714 4745.695828   4262.9092699  7585.67162223]\n",
            "Current mean SMAPE: 31.19721644003543, Current mean MDAPE: 25.74509147087321\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Destandardised test pred: [6278.4239559]\n",
            "Labels: [5855. 4420. 1355. 2560. 2705. 3815.]\n",
            "Full Labels: [5855. 4420. 1355. 2560. 2705. 3815.]\n",
            "Window: [1191.15076857 3025.8263845  3808.89461729 5760.93155803]\n",
            "Current mean SMAPE: 63.871432242900994, Current mean MDAPE: 98.33822707065684\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [4887.46694039]\n",
            "Labels: [1906.8 2307.8  634.6 3389.2 3387.4 2815.8]\n",
            "Full Labels: [1906.8 2307.8  634.6 3389.2 3387.4 2815.8]\n",
            "Window: [3183.77419477 2868.15048409 4124.83133471 4531.36548358]\n",
            "Current mean SMAPE: 73.28736495386164, Current mean MDAPE: 92.67664720250951\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [3884.48042842]\n",
            "Labels: [4138.5  4856.04 5763.38 6722.46 7895.96 8313.9 ]\n",
            "Full Labels: [4138.5  4856.04 5763.38 6722.46 7895.96 8313.9 ]\n",
            "Window: [2116.75920621 2356.85136109 2851.62748819 3448.47581043]\n",
            "Current mean SMAPE: 43.62531852277784, Current mean MDAPE: 37.40851931727926\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Destandardised test pred: [7197.76076418]\n",
            "Labels: [ 8003.8  8629.9  8251.1  8941.5  9484.5 10395.3]\n",
            "Full Labels: [ 8003.8  8629.9  8251.1  8941.5  9484.5 10395.3]\n",
            "Window: [6511.42556954 6689.27447467 6968.47553307 7361.36071829]\n",
            "Current mean SMAPE: 21.28528443068133, Current mean MDAPE: 18.04836300589367\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [7612.16387115]\n",
            "Labels: [7636.05 6775.35 1399.65 5427.6  6233.55 7554.3 ]\n",
            "Full Labels: [7636.05 6775.35 1399.65 5427.6  6233.55 7554.3 ]\n",
            "Window: [7161.79443072 7138.36902538 7359.12332944 7823.33158265]\n",
            "Current mean SMAPE: 34.00064380694203, Current mean MDAPE: 17.23344476715673\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Destandardised test pred: [6909.80556957]\n",
            "Labels: [6747.5 6221.5 6009.5 5205.5 5264.  5640. ]\n",
            "Full Labels: [6747.5 6221.5 6009.5 5205.5 5264.  5640. ]\n",
            "Window: [4972.65721322 5693.57751231 6087.45106465 6891.04143631]\n",
            "Current mean SMAPE: 17.034518707784354, Current mean MDAPE: 18.747827734192583\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [6113.02470874]\n",
            "Labels: [7613.  7778.5 2413.  6567.5 5868.  3307. ]\n",
            "Full Labels: [7613.  7778.5 2413.  6567.5 5868.  3307. ]\n",
            "Window: [6909.2465734  6619.88065035 6082.49161381 6179.43603242]\n",
            "Current mean SMAPE: 33.910321257099326, Current mean MDAPE: 20.557040181401547\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Destandardised test pred: [4105.91734082]\n",
            "Labels: [5611.9  5498.5  5495.1  5496.95 5584.7  5470.1 ]\n",
            "Full Labels: [5611.9  5498.5  5495.1  5496.95 5584.7  5470.1 ]\n",
            "Window: [5621.60042829 5516.86632495 5762.90910687 4359.0403437 ]\n",
            "Current mean SMAPE: 29.485670083099652, Current mean MDAPE: 25.3160639644087\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [4004.12325183]\n",
            "Labels: [3683.4 3937.  4128.4 4063.  3795.2 4135.8]\n",
            "Full Labels: [3683.4 3937.  4128.4 4063.  3795.2 4135.8]\n",
            "Window: [3361.26277861 1856.90598626 2183.51854759 3299.81074583]\n",
            "Current mean SMAPE: 3.8572135652857478, Current mean MDAPE: 3.09705809872643\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Destandardised test pred: [5489.1291804]\n",
            "Labels: [4553.  4694.5 4841.5 6244.  5838.5 6496. ]\n",
            "Full Labels: [4553.  4694.5 4841.5 6244.  5838.5 6496. ]\n",
            "Window: [4674.75025014 5506.55629377 6068.74735141 5637.8932865 ]\n",
            "Current mean SMAPE: 13.77098384376616, Current mean MDAPE: 14.43824060935609\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Destandardised test pred: [4131.43228093]\n",
            "Labels: [3436. 1746. 1071. 3001. 6140. 8670.]\n",
            "Full Labels: [3436. 1746. 1071. 3001. 6140. 8670.]\n",
            "Window: [5407.83875549 4167.17178221 3883.23006189 3880.56526083]\n",
            "Current mean SMAPE: 59.82024220271048, Current mean MDAPE: 45.00823754751521\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Destandardised test pred: [3735.01878736]\n",
            "Labels: [4249.5  4078.8  6529.5  8049.   8069.85 8949.9 ]\n",
            "Full Labels: [4249.5  4078.8  6529.5  8049.   8069.85 8949.9 ]\n",
            "Window: [1878.46161852 2510.56703324 2776.49173082 3345.50105289]\n",
            "Current mean SMAPE: 50.836045130688504, Current mean MDAPE: 48.19713462150491\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Destandardised test pred: [7418.54544097]\n",
            "Labels: [8130.23 8242.13 7871.93 7311.   7519.88 8014.2 ]\n",
            "Full Labels: [8130.23 8242.13 7871.93 7311.   7519.88 8014.2 ]\n",
            "Window: [5574.5549218  6040.56861131 6803.24925698 7508.19586997]\n",
            "Current mean SMAPE: 6.023116588358879, Current mean MDAPE: 6.595999427704627\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "Destandardised test pred: [6118.38590992]\n",
            "Labels: [6405.  5934.5 6204.5 6358.  6031.  5938. ]\n",
            "Full Labels: [6405.  5934.5 6204.5 6358.  6031.  5938. ]\n",
            "Window: [5353.10087922 5868.23015386 6252.49027693 6232.69829846]\n",
            "Current mean SMAPE: 2.8830308235782725, Current mean MDAPE: 3.068207060776966\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Destandardised test pred: [6410.60257213]\n",
            "Labels: [7134. 7544. 6744. 7334. 7196. 8164.]\n",
            "Full Labels: [7134. 7544. 6744. 7334. 7196. 8164.]\n",
            "Window: [5653.4201808  5997.09853164 6723.84224304 6598.57239253]\n",
            "Current mean SMAPE: 13.506091385892747, Current mean MDAPE: 11.752499823609199\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Destandardised test pred: [3304.86453178]\n",
            "Labels: [3286.5 3703.  2749.5 3783.5 4504.5 5363.5]\n",
            "Full Labels: [3286.5 3703.  2749.5 3783.5 4504.5 5363.5]\n",
            "Window: [2260.01286788 2304.55225188 2638.63414408 2942.94847521]\n",
            "Current mean SMAPE: 20.331862148139017, Current mean MDAPE: 16.424673418668707\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Destandardised test pred: [3627.93223054]\n",
            "Labels: [3923.31 4326.66 4720.41 5351.37 6283.68 8147.7 ]\n",
            "Full Labels: [3923.31 4326.66 4720.41 5351.37 6283.68 8147.7 ]\n",
            "Window: [1794.09264012 2109.18271202 2624.3022246  3192.98938931]\n",
            "Current mean SMAPE: 36.717299207164714, Current mean MDAPE: 27.674624073517833\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Destandardised test pred: [4555.16557435]\n",
            "Labels: [3330.55 3326.2  3975.25 4276.15 6718.35 7899.05]\n",
            "Full Labels: [3330.55 3326.2  3975.25 4276.15 6718.35 7899.05]\n",
            "Window: [2879.44641365 2600.80562239 2944.41348392 3979.81860969]\n",
            "Current mean SMAPE: 29.03935489436981, Current mean MDAPE: 34.48365985609236\n"
          ]
        }
      ],
      "source": [
        "loaded_model = tf.keras.models.load_model(\"best_model.hdf5\", custom_objects={\"smape_loss\": smape_loss, \"metric_mdape\": metric_mdape})\n",
        "test1, test2 = evaluate_model_on_test(loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_99 (Flatten)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_534 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_535 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            " dense_536 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19\n",
            "Trainable params: 19\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24.607080591140555"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = (np.array([[940.66, 1084.86, 1244.98, 1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "# print(data.shape)\n",
        "#def make_preds(model, input_data):\n",
        "#  forecast = model.predict(input_data)\n",
        "#  preds = tf.squeeze(forecast)\n",
        "#  return preds\n",
        "\n",
        "#pred = make_preds(model, data)\n",
        "# inversed = de_standardise(np.array(pred))\n",
        "# inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
