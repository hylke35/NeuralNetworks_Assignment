{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zaur72VQkZnP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import seaborn\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from keras.callbacks import EarlyStopping\n",
        "from collections import namedtuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "SV4WTnKRk7V8",
        "outputId": "f3fd8ddc-f190-42ac-f8a5-37cbffcff798"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"M3C.xls\", usecols=\"A:Z\")\n",
        "\n",
        "df_micro = df.iloc[0:146,]\n",
        "df_micro = df_micro.iloc[:,6:27]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Detrend\n",
        "#for i in range(len(df_train)):\n",
        "#    data = df_train.iloc[i]\n",
        "#    poly_fit = np.polyfit(np.arange(14), data, 2)\n",
        "#    trend = np.polyval(poly_fit, np.arange(14))\n",
        "#    df_train.iloc[i] = df_train.iloc[i] - trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qDPl7xrxzPaB"
      },
      "outputs": [],
      "source": [
        "df_train = df_micro.iloc[:,:-6]\n",
        "df_test = df_micro.iloc[:, -6:]\n",
        "\n",
        "#Standardising\n",
        "scaler = StandardScaler()\n",
        "df_train = scaler.fit_transform(df_train.to_numpy().reshape(-1,1))\n",
        "df_train = pd.DataFrame(df_train)\n",
        "MEAN = scaler.mean_\n",
        "STD = scaler.scale_\n",
        "\n",
        "df_train = df_train.to_numpy().reshape(-1,14)\n",
        "df_test = df_test.to_numpy().reshape(-1,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KPru18rPwBtN"
      },
      "outputs": [],
      "source": [
        "def get_labelled_window(x, horizon=1):\n",
        "  return x[:, :-horizon], x[:, -horizon]\n",
        "\n",
        "def make_windows(x, window_size=4, horizon=1):\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of window size\n",
        "  windowed_array = x[window_indexes]\n",
        "  windows, labels = get_labelled_window(windowed_array, horizon=horizon)\n",
        "  return windows.reshape(-1,4), labels.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = []\n",
        "train_y = []\n",
        "test_x = []\n",
        "test_y = []\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "    windows_train, labels_train = make_windows(df_train[i], window_size=4, horizon=1)\n",
        "    windows_test, labels_test = make_windows(df_test[i], window_size=4, horizon=1)\n",
        "    train_x = np.concatenate((np.array(train_x).reshape(-1,4), windows_train.reshape(-1,4)))\n",
        "    train_y = np.concatenate((np.array(train_y).reshape(-1,1), labels_train.reshape(-1,1)))\n",
        "    test_x = np.concatenate((np.array(test_x).reshape(-1,4), windows_test.reshape(-1,4)))\n",
        "    test_y = np.concatenate((np.array(test_y).reshape(-1,1), labels_test.reshape(-1,1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMAPE\n",
        "def evaluate_smape(y_true, y_pred):\n",
        "    return 200 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
        "\n",
        "def evaluate_mdape(y_true, y_pred):\n",
        " return np.median((np.abs(np.subtract(y_true, y_pred)/ y_true))) * 100\n",
        "\n",
        "def calculate_average_rankings(y_true, y_pred):\n",
        "    num_series = len(y_pred)\n",
        "    num_methods = len(y_pred[0])\n",
        "\n",
        "    ranks = []  # to store ranks for each series\n",
        "\n",
        "    for series_index in range(num_series):\n",
        "        sape_values = [\n",
        "            abs((y_true[series_index] - forecast) / y_true[series_index]) * 100\n",
        "            for forecast in y_pred[series_index]\n",
        "        ]\n",
        "        sorted_sape = sorted(sape_values)  # sort SAPE values in ascending order\n",
        "        series_ranks = [sorted_sape.index(sape) + 1 for sape in sape_values]  # assign ranks to SAPE values\n",
        "        ranks.append(series_ranks)\n",
        "\n",
        "    mean_ranks = []  # to store mean ranks for each forecasting method\n",
        "\n",
        "    for method_index in range(num_methods):\n",
        "        total_rank = sum(ranks[series_index][method_index] for series_index in range(num_series))\n",
        "        mean_rank = total_rank / num_series\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    return mean_ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pred(y_true, y_pred):\n",
        "    # Symmetric mean absolute percentage error\n",
        "    smape = evaluate_smape(y_true, y_pred)\n",
        "    # Median symmetric absolute percentage error\n",
        "    mdape = evaluate_mdape(y_true, y_pred)\n",
        "    return smape, mdape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true_set, y_pred_set):\n",
        "    # Average Ranking\n",
        "    avg_ranking = None\n",
        "    # Percentage Better\n",
        "    percentage_better = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Destandardise\n",
        "def de_standardise(value):\n",
        "    return value * STD + MEAN\n",
        "\n",
        "def standardise(value):\n",
        "    return (value - MEAN) / STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "Combination = namedtuple(\"Combination\", \"learning_rate batch_size regularization hidden_layers hidden_neurons\")\n",
        "\n",
        "learning_rates = np.array([0.001, 0.01, 0.1])\n",
        "batch_sizes = np.array([16, 32, 64, 128])\n",
        "regularizations = np.array([0.001, 0.01, 0.001])\n",
        "hidden_layers = np.array([2, 3, 4, 6])\n",
        "hidden_neurons = np.array([2, 3, 4, 5, 6, 7, 8])\n",
        "\n",
        "combinations = list(itertools.starmap(Combination, itertools.product(learning_rates, batch_sizes, regularizations, hidden_layers, hidden_neurons)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "8/8 [==============================] - 0s 891us/step\n",
            "Current mean SMAPE: 34.16183184337592, Current hyperparameters: {'learning_rate': 0.1, 'batch_size': 16, 'regularization': 0.001, 'hidden_neurons': 2, 'hidden_layers': 6}\n",
            "Best Hyperparameters: {'learning_rate': 0.1, 'batch_size': 16, 'regularization': 0.001, 'hidden_neurons': 2, 'hidden_layers': 6}\n",
            "Best SMAPE Score: 34.16183184337592\n",
            "Best MDAPE Score: 25.08078977482512\n"
          ]
        }
      ],
      "source": [
        "# Time-series expanding window validation\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "eval_scores = []\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "def cross_validation(combination, train_x=train_x, train_y=train_y, tscv=tscv):\n",
        "    hidden_neurons = np.arange(2, 9)\n",
        "    smape_scores = []\n",
        "    mdape_scores = []\n",
        "\n",
        "    # Cross-Validation\n",
        "    for train_index, test_index in tscv.split(train_x):\n",
        "        train_x_cv, val_x_cv = train_x[train_index], train_x[test_index]\n",
        "        train_y_cv, val_y_cv = train_y[train_index], train_y[test_index]\n",
        "\n",
        "        # Create model with selected hyperparameters\n",
        "        model_cv = tf.keras.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "        ], name=\"model\")\n",
        "\n",
        "        #chosen_hidden_neurons = []\n",
        "\n",
        "        for i in range(combination.hidden_layers):\n",
        "            #random_neuron = random.choice(hidden_neurons)\n",
        "            #chosen_hidden_neurons.append(random_neuron)\n",
        "            model_cv.add(tf.keras.layers.Dense(combination.hidden_neurons, \n",
        "                                            activation=\"relu\", \n",
        "                                            kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                            kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "        model_cv.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                        kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(combination.regularization)))\n",
        "\n",
        "\n",
        "        model_cv.compile(loss=\"mae\",\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate=combination.learning_rate),\n",
        "                        metrics=[\"mae\", \"mse\"]) # Backpropagation\n",
        "        \n",
        "        model_cv.fit(train_x_cv, train_y_cv, epochs=50, batch_size=combination.batch_size, verbose=0)\n",
        "\n",
        "        predictions = model_cv.predict(val_x_cv)\n",
        "        smape_score, mdape_score = evaluate_pred(de_standardise(val_y_cv), de_standardise(predictions))\n",
        "        \n",
        "        smape_scores.append(smape_score)\n",
        "        mdape_scores.append(mdape_score)\n",
        "        \n",
        "    mean_smape = np.mean(smape_scores)\n",
        "    mean_mdape = np.mean(mdape_scores)\n",
        "    hyperparameters = {\n",
        "        'learning_rate': combination.learning_rate,\n",
        "        'batch_size': combination.batch_size,\n",
        "        'regularization': combination.regularization,\n",
        "        'hidden_neurons': combination.hidden_neurons,\n",
        "        'hidden_layers': combination.hidden_layers\n",
        "    }\n",
        "    print(f\"Current mean SMAPE: {mean_smape}, Current hyperparameters: {hyperparameters}\")\n",
        "    return mean_smape, mean_mdape, hyperparameters\n",
        "\n",
        "random_combinations = random.sample(combinations, 1)\n",
        "results = map(cross_validation, random_combinations)\n",
        "\n",
        "optimal_smape = float('inf')\n",
        "optimal_mdape = float('inf')\n",
        "optimal_hyperparameters = {}\n",
        "for result in results:\n",
        "    smape, mdape, hyperparameters = result\n",
        "    if smape < optimal_smape:\n",
        "        optimal_smape = smape\n",
        "        optimal_mdape = mdape\n",
        "        optimal_hyperparameters = hyperparameters\n",
        "print(\"Best Hyperparameters:\", optimal_hyperparameters)\n",
        "print(\"Best SMAPE Score:\", optimal_smape)\n",
        "print(\"Best MDAPE Score:\", optimal_mdape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "u4z0s2GEn4gr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regularization: 0.001\n",
            "Learning Rate: 0.1\n",
            "Batch Size: 16\n",
            "\n",
            "Epoch 1/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 1.0152 - mse: 1.0134 - mae: 0.7865 \n",
            "Epoch 1: loss improved from inf to 0.98813, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 1s 2ms/step - loss: 0.9881 - mse: 0.9866 - mae: 0.7759\n",
            "Epoch 2/1000\n",
            "75/92 [=======================>......] - ETA: 0s - loss: 0.9956 - mse: 0.9956 - mae: 0.7788\n",
            "Epoch 2: loss improved from 0.98813 to 0.98689, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9869 - mse: 0.9869 - mae: 0.7792\n",
            "Epoch 3/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 1.0045 - mse: 1.0045 - mae: 0.7785\n",
            "Epoch 3: loss improved from 0.98689 to 0.98660, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9866 - mse: 0.9866 - mae: 0.7760\n",
            "Epoch 4/1000\n",
            "82/92 [=========================>....] - ETA: 0s - loss: 0.9904 - mse: 0.9904 - mae: 0.7734\n",
            "Epoch 4: loss improved from 0.98660 to 0.98130, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9813 - mse: 0.9813 - mae: 0.7744\n",
            "Epoch 5/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 1.0088 - mse: 1.0088 - mae: 0.7781\n",
            "Epoch 5: loss did not improve from 0.98130\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9878 - mse: 0.9878 - mae: 0.7749\n",
            "Epoch 6/1000\n",
            "78/92 [========================>.....] - ETA: 0s - loss: 0.9209 - mse: 0.9209 - mae: 0.7755\n",
            "Epoch 6: loss did not improve from 0.98130\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9840 - mse: 0.9840 - mae: 0.7770\n",
            "Epoch 7/1000\n",
            "76/92 [=======================>......] - ETA: 0s - loss: 1.0287 - mse: 1.0287 - mae: 0.7942\n",
            "Epoch 7: loss did not improve from 0.98130\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9885 - mse: 0.9885 - mae: 0.7796\n",
            "Epoch 8/1000\n",
            "72/92 [======================>.......] - ETA: 0s - loss: 0.9879 - mse: 0.9879 - mae: 0.7754\n",
            "Epoch 8: loss improved from 0.98130 to 0.98105, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9811 - mse: 0.9811 - mae: 0.7727\n",
            "Epoch 9/1000\n",
            "63/92 [===================>..........] - ETA: 0s - loss: 1.0269 - mse: 1.0269 - mae: 0.7713\n",
            "Epoch 9: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9926 - mse: 0.9926 - mae: 0.7788\n",
            "Epoch 10/1000\n",
            "82/92 [=========================>....] - ETA: 0s - loss: 0.9989 - mse: 0.9989 - mae: 0.7803\n",
            "Epoch 10: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9913 - mse: 0.9913 - mae: 0.7770\n",
            "Epoch 11/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 0.9107 - mse: 0.9107 - mae: 0.7682\n",
            "Epoch 11: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9829 - mse: 0.9829 - mae: 0.7779\n",
            "Epoch 12/1000\n",
            "84/92 [==========================>...] - ETA: 0s - loss: 1.0077 - mse: 1.0077 - mae: 0.7826\n",
            "Epoch 12: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9878 - mse: 0.9878 - mae: 0.7772\n",
            "Epoch 13/1000\n",
            "75/92 [=======================>......] - ETA: 0s - loss: 0.9886 - mse: 0.9886 - mae: 0.7685\n",
            "Epoch 13: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9824 - mse: 0.9824 - mae: 0.7751\n",
            "Epoch 14/1000\n",
            "88/92 [===========================>..] - ETA: 0s - loss: 0.9894 - mse: 0.9894 - mae: 0.7749\n",
            "Epoch 14: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9855 - mse: 0.9855 - mae: 0.7761\n",
            "Epoch 15/1000\n",
            "68/92 [=====================>........] - ETA: 0s - loss: 0.9387 - mse: 0.9387 - mae: 0.7775\n",
            "Epoch 15: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9889 - mse: 0.9889 - mae: 0.7765\n",
            "Epoch 16/1000\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 0.9080 - mse: 0.9080 - mae: 0.7682\n",
            "Epoch 16: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9871 - mse: 0.9871 - mae: 0.7765\n",
            "Epoch 17/1000\n",
            "72/92 [======================>.......] - ETA: 0s - loss: 0.9270 - mse: 0.9270 - mae: 0.7797\n",
            "Epoch 17: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9860 - mse: 0.9860 - mae: 0.7771\n",
            "Epoch 18/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 1.0143 - mse: 1.0143 - mae: 0.7802\n",
            "Epoch 18: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9929 - mse: 0.9929 - mae: 0.7759\n",
            "Epoch 19/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 1.0015 - mse: 1.0015 - mae: 0.7796\n",
            "Epoch 19: loss did not improve from 0.98105\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9867 - mse: 0.9867 - mae: 0.7761\n",
            "Epoch 20/1000\n",
            "63/92 [===================>..........] - ETA: 0s - loss: 0.9982 - mse: 0.9982 - mae: 0.7675\n",
            "Epoch 20: loss improved from 0.98105 to 0.97965, saving model to best_model.hdf5\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9797 - mse: 0.9797 - mae: 0.7737\n",
            "Epoch 21/1000\n",
            "71/92 [======================>.......] - ETA: 0s - loss: 0.8960 - mse: 0.8960 - mae: 0.7633\n",
            "Epoch 21: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9857 - mse: 0.9857 - mae: 0.7743\n",
            "Epoch 22/1000\n",
            "77/92 [========================>.....] - ETA: 0s - loss: 0.9979 - mse: 0.9979 - mae: 0.7705\n",
            "Epoch 22: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9857 - mse: 0.9857 - mae: 0.7744\n",
            "Epoch 23/1000\n",
            "82/92 [=========================>....] - ETA: 0s - loss: 0.9610 - mse: 0.9610 - mae: 0.7675\n",
            "Epoch 23: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9808 - mse: 0.9808 - mae: 0.7744\n",
            "Epoch 24/1000\n",
            "80/92 [=========================>....] - ETA: 0s - loss: 0.9764 - mse: 0.9764 - mae: 0.7682\n",
            "Epoch 24: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9857 - mse: 0.9857 - mae: 0.7759\n",
            "Epoch 25/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 0.9907 - mse: 0.9907 - mae: 0.7691\n",
            "Epoch 25: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9853 - mse: 0.9853 - mae: 0.7744\n",
            "Epoch 26/1000\n",
            "86/92 [===========================>..] - ETA: 0s - loss: 0.9906 - mse: 0.9906 - mae: 0.7748\n",
            "Epoch 26: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9864 - mse: 0.9864 - mae: 0.7740\n",
            "Epoch 27/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 1.0147 - mse: 1.0147 - mae: 0.7845\n",
            "Epoch 27: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9874 - mse: 0.9874 - mae: 0.7765\n",
            "Epoch 28/1000\n",
            "86/92 [===========================>..] - ETA: 0s - loss: 0.9739 - mse: 0.9739 - mae: 0.7751\n",
            "Epoch 28: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9815 - mse: 0.9815 - mae: 0.7757\n",
            "Epoch 29/1000\n",
            "80/92 [=========================>....] - ETA: 0s - loss: 1.0033 - mse: 1.0033 - mae: 0.7809\n",
            "Epoch 29: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9846 - mse: 0.9846 - mae: 0.7762\n",
            "Epoch 30/1000\n",
            "88/92 [===========================>..] - ETA: 0s - loss: 0.9910 - mse: 0.9910 - mae: 0.7747\n",
            "Epoch 30: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9877 - mse: 0.9877 - mae: 0.7753\n",
            "Epoch 31/1000\n",
            "74/92 [=======================>......] - ETA: 0s - loss: 1.0239 - mse: 1.0239 - mae: 0.7883\n",
            "Epoch 31: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9813 - mse: 0.9813 - mae: 0.7747\n",
            "Epoch 32/1000\n",
            "87/92 [===========================>..] - ETA: 0s - loss: 0.9848 - mse: 0.9848 - mae: 0.7728\n",
            "Epoch 32: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9872 - mse: 0.9872 - mae: 0.7749\n",
            "Epoch 33/1000\n",
            "82/92 [=========================>....] - ETA: 0s - loss: 0.9736 - mse: 0.9736 - mae: 0.7730\n",
            "Epoch 33: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9907 - mse: 0.9907 - mae: 0.7799\n",
            "Epoch 34/1000\n",
            "86/92 [===========================>..] - ETA: 0s - loss: 0.9957 - mse: 0.9957 - mae: 0.7795\n",
            "Epoch 34: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9871 - mse: 0.9871 - mae: 0.7769\n",
            "Epoch 35/1000\n",
            "79/92 [========================>.....] - ETA: 0s - loss: 0.9932 - mse: 0.9932 - mae: 0.7744\n",
            "Epoch 35: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9822 - mse: 0.9822 - mae: 0.7738\n",
            "Epoch 36/1000\n",
            "86/92 [===========================>..] - ETA: 0s - loss: 0.9947 - mse: 0.9947 - mae: 0.7764\n",
            "Epoch 36: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9842 - mse: 0.9842 - mae: 0.7753\n",
            "Epoch 37/1000\n",
            "75/92 [=======================>......] - ETA: 0s - loss: 1.0227 - mse: 1.0227 - mae: 0.7845\n",
            "Epoch 37: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9962 - mse: 0.9962 - mae: 0.7817\n",
            "Epoch 38/1000\n",
            "83/92 [==========================>...] - ETA: 0s - loss: 0.9961 - mse: 0.9961 - mae: 0.7757\n",
            "Epoch 38: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9838 - mse: 0.9838 - mae: 0.7741\n",
            "Epoch 39/1000\n",
            "83/92 [==========================>...] - ETA: 0s - loss: 0.9945 - mse: 0.9945 - mae: 0.7770\n",
            "Epoch 39: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9820 - mse: 0.9820 - mae: 0.7736\n",
            "Epoch 40/1000\n",
            "85/92 [==========================>...] - ETA: 0s - loss: 0.9878 - mse: 0.9878 - mae: 0.7744\n",
            "Epoch 40: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9810 - mse: 0.9810 - mae: 0.7726\n",
            "Epoch 41/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 0.9843 - mse: 0.9843 - mae: 0.7762\n",
            "Epoch 41: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9858 - mse: 0.9858 - mae: 0.7748\n",
            "Epoch 42/1000\n",
            "56/92 [=================>............] - ETA: 0s - loss: 1.0108 - mse: 1.0108 - mae: 0.7671\n",
            "Epoch 42: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9831 - mse: 0.9831 - mae: 0.7756\n",
            "Epoch 43/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 1.0180 - mse: 1.0180 - mae: 0.7887\n",
            "Epoch 43: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9939 - mse: 0.9939 - mae: 0.7791\n",
            "Epoch 44/1000\n",
            "78/92 [========================>.....] - ETA: 0s - loss: 1.0062 - mse: 1.0062 - mae: 0.7790\n",
            "Epoch 44: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9834 - mse: 0.9834 - mae: 0.7758\n",
            "Epoch 45/1000\n",
            "64/92 [===================>..........] - ETA: 0s - loss: 0.9568 - mse: 0.9568 - mae: 0.7872\n",
            "Epoch 45: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9927 - mse: 0.9927 - mae: 0.7789\n",
            "Epoch 46/1000\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 1.0533 - mse: 1.0533 - mae: 0.7951\n",
            "Epoch 46: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 1.0010 - mse: 1.0010 - mae: 0.7799\n",
            "Epoch 47/1000\n",
            "77/92 [========================>.....] - ETA: 0s - loss: 1.0049 - mse: 1.0049 - mae: 0.7761\n",
            "Epoch 47: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9865 - mse: 0.9865 - mae: 0.7740\n",
            "Epoch 48/1000\n",
            "75/92 [=======================>......] - ETA: 0s - loss: 0.8882 - mse: 0.8882 - mae: 0.7670\n",
            "Epoch 48: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9841 - mse: 0.9841 - mae: 0.7763\n",
            "Epoch 49/1000\n",
            "75/92 [=======================>......] - ETA: 0s - loss: 0.9961 - mse: 0.9961 - mae: 0.7771\n",
            "Epoch 49: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9939 - mse: 0.9939 - mae: 0.7802\n",
            "Epoch 50/1000\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 1.0181 - mse: 1.0181 - mae: 0.7824\n",
            "Epoch 50: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9826 - mse: 0.9826 - mae: 0.7741\n",
            "Epoch 51/1000\n",
            "74/92 [=======================>......] - ETA: 0s - loss: 1.0259 - mse: 1.0259 - mae: 0.7837\n",
            "Epoch 51: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9950 - mse: 0.9950 - mae: 0.7782\n",
            "Epoch 52/1000\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 0.9723 - mse: 0.9723 - mae: 0.7637\n",
            "Epoch 52: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9879 - mse: 0.9879 - mae: 0.7772\n",
            "Epoch 53/1000\n",
            "70/92 [=====================>........] - ETA: 0s - loss: 1.0336 - mse: 1.0336 - mae: 0.7862\n",
            "Epoch 53: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9890 - mse: 0.9890 - mae: 0.7753\n",
            "Epoch 54/1000\n",
            "73/92 [======================>.......] - ETA: 0s - loss: 0.9303 - mse: 0.9303 - mae: 0.7722\n",
            "Epoch 54: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9893 - mse: 0.9893 - mae: 0.7751\n",
            "Epoch 55/1000\n",
            "71/92 [======================>.......] - ETA: 0s - loss: 0.9810 - mse: 0.9810 - mae: 0.7755\n",
            "Epoch 55: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.9916 - mse: 0.9916 - mae: 0.7794\n",
            "Epoch 56/1000\n",
            "64/92 [===================>..........] - ETA: 0s - loss: 0.9310 - mse: 0.9310 - mae: 0.7697\n",
            "Epoch 56: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9857 - mse: 0.9857 - mae: 0.7731\n",
            "Epoch 57/1000\n",
            "68/92 [=====================>........] - ETA: 0s - loss: 0.9231 - mse: 0.9231 - mae: 0.7725\n",
            "Epoch 57: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9842 - mse: 0.9842 - mae: 0.7753\n",
            "Epoch 58/1000\n",
            "87/92 [===========================>..] - ETA: 0s - loss: 0.9843 - mse: 0.9843 - mae: 0.7754\n",
            "Epoch 58: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9856 - mse: 0.9856 - mae: 0.7777\n",
            "Epoch 59/1000\n",
            "88/92 [===========================>..] - ETA: 0s - loss: 1.0020 - mse: 1.0020 - mae: 0.7795\n",
            "Epoch 59: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9944 - mse: 0.9944 - mae: 0.7781\n",
            "Epoch 60/1000\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 0.9674 - mse: 0.9674 - mae: 0.7635\n",
            "Epoch 60: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9846 - mse: 0.9846 - mae: 0.7755\n",
            "Epoch 61/1000\n",
            "68/92 [=====================>........] - ETA: 0s - loss: 1.0185 - mse: 1.0185 - mae: 0.7756\n",
            "Epoch 61: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9927 - mse: 0.9927 - mae: 0.7773\n",
            "Epoch 62/1000\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 1.0067 - mse: 1.0067 - mae: 0.7690\n",
            "Epoch 62: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9857 - mse: 0.9857 - mae: 0.7743\n",
            "Epoch 63/1000\n",
            "65/92 [====================>.........] - ETA: 0s - loss: 1.0172 - mse: 1.0172 - mae: 0.7755\n",
            "Epoch 63: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9859 - mse: 0.9859 - mae: 0.7761\n",
            "Epoch 64/1000\n",
            "60/92 [==================>...........] - ETA: 0s - loss: 0.9392 - mse: 0.9392 - mae: 0.7787\n",
            "Epoch 64: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9833 - mse: 0.9833 - mae: 0.7757\n",
            "Epoch 65/1000\n",
            "67/92 [====================>.........] - ETA: 0s - loss: 0.9420 - mse: 0.9420 - mae: 0.7751\n",
            "Epoch 65: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9888 - mse: 0.9888 - mae: 0.7777\n",
            "Epoch 66/1000\n",
            "66/92 [====================>.........] - ETA: 0s - loss: 0.8853 - mse: 0.8853 - mae: 0.7558\n",
            "Epoch 66: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9865 - mse: 0.9865 - mae: 0.7764\n",
            "Epoch 67/1000\n",
            "64/92 [===================>..........] - ETA: 0s - loss: 0.9651 - mse: 0.9651 - mae: 0.7864\n",
            "Epoch 67: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9916 - mse: 0.9916 - mae: 0.7766\n",
            "Epoch 68/1000\n",
            "64/92 [===================>..........] - ETA: 0s - loss: 1.0300 - mse: 1.0300 - mae: 0.7785\n",
            "Epoch 68: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9850 - mse: 0.9850 - mae: 0.7762\n",
            "Epoch 69/1000\n",
            "63/92 [===================>..........] - ETA: 0s - loss: 0.9282 - mse: 0.9282 - mae: 0.7682\n",
            "Epoch 69: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9904 - mse: 0.9904 - mae: 0.7707\n",
            "Epoch 70/1000\n",
            "81/92 [=========================>....] - ETA: 0s - loss: 0.9987 - mse: 0.9987 - mae: 0.7748\n",
            "Epoch 70: loss did not improve from 0.97965\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.9930 - mse: 0.9930 - mae: 0.7783\n",
            "Epoch 70: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x13caf77c0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Regularization: {optimal_hyperparameters['regularization']}\")\n",
        "print(f\"Learning Rate: {optimal_hyperparameters['learning_rate']}\")\n",
        "print(f\"Batch Size: {optimal_hyperparameters['batch_size']}\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 1)),\n",
        "], name=\"model\")\n",
        "\n",
        "for i in range(optimal_hyperparameters[\"hidden_layers\"]):\n",
        "    model.add(tf.keras.layers.Dense(optimal_hyperparameters[\"hidden_neurons\"], \n",
        "                                    activation=\"relu\", \n",
        "                                    kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"linear\", \n",
        "                                kernel_initializer=tf.initializers.HeNormal(), \n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(optimal_hyperparameters[\"regularization\"])))\n",
        "\n",
        "print()\n",
        "model.compile(loss=\"mse\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_hyperparameters[\"learning_rate\"]), \n",
        "                metrics=[\"mse\", \"mae\"]) # Backpropagation\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.hdf5', monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "# Train the model on the full training dataset\n",
        "model.fit(train_x, train_y, epochs=1000, batch_size=optimal_hyperparameters[\"batch_size\"], verbose=1, callbacks=[early_stopping, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 52ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([5205.8997691])"
            ]
          },
          "execution_count": 322,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def autoregression(model, x, horizon=6):\n",
        "    standardised_x = standardise(x)\n",
        "    for i in range(horizon):\n",
        "        forecast = model.predict(np.array([standardised_x[i:i+4]]))\n",
        "        pred = np.array([tf.squeeze(forecast).numpy()])\n",
        "        standardised_x = np.concatenate((standardised_x, pred))\n",
        "    return de_standardise(standardised_x[-horizon:])\n",
        "\n",
        "#autoregression(model, np.array([4793.2, 5602, 5065, 5056]), 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model_on_test(model, df_train, test_x, test_y):\n",
        "    for i in range(len(df_train)):\n",
        "        window = df_train[-1:-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0531dc392abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
          ]
        }
      ],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5379.75, 6158.68, 6876.58, 7851.91],\n",
              "       [6158.68, 6876.58, 7851.91, 8407.84],\n",
              "       [6876.58, 7851.91, 8407.84, 9156.01],\n",
              "       ...,\n",
              "       [8147.7 , 3330.55, 3326.2 , 3975.25],\n",
              "       [3330.55, 3326.2 , 3975.25, 4276.15],\n",
              "       [3326.2 , 3975.25, 4276.15, 6718.35]])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for i in range(len(df_train)):\n",
        "    train_x[]\n",
        "    predicted_values = autoregression(model, , 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.21687544 -1.13856182 -1.0407236  -0.92424603]]\n",
            "1/1 [==============================] - 0s 53ms/step\n"
          ]
        }
      ],
      "source": [
        "# data = (np.array([[940.66, 1084.86, 1244.98, 1445.02]]) - scaler.mean_) / scaler.scale_\n",
        "# print(data.shape)\n",
        "#def make_preds(model, input_data):\n",
        "#  forecast = model.predict(input_data)\n",
        "#  preds = tf.squeeze(forecast)\n",
        "#  return preds\n",
        "\n",
        "#pred = make_preds(model, data)\n",
        "# inversed = de_standardise(np.array(pred))\n",
        "# inversed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
