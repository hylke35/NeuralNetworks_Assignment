{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed-window sequences for training and validation data\n",
    "def create_sequences(X, window_size):\n",
    "    seq_X = []\n",
    "    seq_y = []\n",
    "    for i in range(len(X) - window_size):\n",
    "        seq_X.append(X[i:i+window_size])\n",
    "        seq_y.append(X[i+window_size])\n",
    "    return seq_X, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess(y, details):\n",
    "    # print(details)\n",
    "    mean = details[2][0]\n",
    "    std = details[2][1]\n",
    "    PF = details[2][2]\n",
    "    PFtype = details[2][3]\n",
    "    time = details[3]\n",
    "\n",
    "    # print(\"mean:\", mean, \"std:\", std, \"PF\", PF, \"type\", PFtype, \"time\", time)\n",
    "\n",
    "    if(PFtype == 1):\n",
    "        return ((y * std) + mean) * np.exp(PF[0] * time + PF[1])    \n",
    "    # print(\"here\")\n",
    "    return (y * std + mean) * (PF[0] * np.square(time) + PF[1] * time + PF[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, smoothing, alpha = None):\n",
    "\n",
    "    # #Exponential smoothing\n",
    "    if smoothing:\n",
    "        data = data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    PFtype = -1\n",
    "    PF1 = np.polyfit(np.linspace(0,len(data) - 1,num=len(data)), np.log(data), 1)\n",
    "    PF2 = np.polyfit(np.linspace(0,len(data) - 1,num=len(data)),data, 2)\n",
    "    error1 = data - (np.exp(PF1[0] * np.linspace(0, len(data) - 1, num=len(data)) + PF1[1]))\n",
    "    error2 = data - (PF2[0] * np.square(np.linspace(0,len(data) - 1,num=len(data))) + PF2[1] * np.linspace(0, len(data) - 1, num=len(data)) + PF2[2])\n",
    "    \n",
    "    #Otto: dit is de keuze voro welke je preporcessed. je kan die plotjes un commenten om de fit te zien\n",
    "    if(np.sum(np.square(error1)) < np.sum(np.square(error2))):\n",
    "        PF = PF1\n",
    "        preprocessed = data / (np.exp(PF[0] * np.linspace(0,len(data) - 1,num=len(data)) + PF[1]))\n",
    "        PFtype = 1\n",
    "    else:\n",
    "        PF = PF2\n",
    "        preprocessed = data / (PF2[0] * np.square(np.linspace(0,len(data) - 1,num=len(data))) + PF2[1] * np.linspace(0,len(data) - 1,num=len(data)) + PF2[2])\n",
    "        PFtype = 2\n",
    "    \n",
    "    m = np.mean(preprocessed)\n",
    "    s = np.std(preprocessed)\n",
    "    preprocessed = (preprocessed - m)/s\n",
    "    details = [m, s, PF, PFtype]\n",
    "    \n",
    "    return preprocessed, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_clean(y_true, y_pred):\n",
    "    smape = 100 * np.mean(2*np.abs(y_pred - y_true) / (y_true + y_pred))\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(model, validation):\n",
    "    validation_x = []\n",
    "    validation_y = []\n",
    "    for val in validation:\n",
    "        validation_x.append(val[0])\n",
    "        validation_y.append(val[1])\n",
    "    validation_x = np.array(validation_x)\n",
    "    validation_y = np.array(validation_y)\n",
    "    smape = 0\n",
    "    prediction = model.predict(validation_x, verbose=0)\n",
    "    # _, acc = model.evaluate(validation_x, validation_y, verbose = 0)\n",
    "\n",
    "    # print(\"sse val is; \", np.sum(np.square(validation_y - prediction))/len(validation_y), \"val is \", acc)\n",
    "    for i in range(len(validation)):\n",
    "        observation = validation[i]\n",
    "        pred = prediction[i]\n",
    "        #print(pred, observation[1], pred - observation[1])\n",
    "        x_hat = reprocess(pred, observation)\n",
    "        x = reprocess(observation[1], observation)\n",
    "        #print(x_hat, x, x_hat - x)\n",
    "\n",
    "        smape += 2*np.abs(x_hat-x)/(x+x_hat)\n",
    "\n",
    "    smape /= len(validation)\n",
    "    smape *=100\n",
    "\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_validation, y_validation, window_size, options): #x_validation, y_validation\n",
    "    # Build the FFNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(window_size, 1))) \n",
    "    model.add(keras.layers.Dense(options.layers[0], activation='sigmoid'))\n",
    "\n",
    "    if len(options.layers) > 2:\n",
    "        for i in range(1,len(options.layers)-1):\n",
    "            model.add(keras.layers.Dense(options.layers[i], activation=options.activation))\n",
    "                \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    #early stopping and saving the best model SOURCE: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50, min_delta=0.001)\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "    # fit model\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), batch_size=options.batchSize, verbose = 0, callbacks=[es, mc], epochs= 1000) #fit the model with early stop\n",
    "\n",
    "    ##this is the best model\n",
    "    saved_model = load_model(\"best_model.h5\")\n",
    "\n",
    "    _, train_acc = saved_model.evaluate(x_train, y_train, verbose = 0)\n",
    "    _, test_acc = saved_model.evaluate(x_validation, y_validation, verbose = 0)\n",
    "\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # # plot the different accuracies. maybe then dont do a early stop. \n",
    "    # pyplot.plot(history.history['loss'], label='train')\n",
    "    # pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    # pyplot.legend()\n",
    "    # pyplot.show()\n",
    "\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_train = df.iloc[:,:14]\n",
    "df_test = df.iloc[:,14:]\n",
    "\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_observations(data, window_size):\n",
    "    observations = []\n",
    "    details = []\n",
    "    for index, row in data.iterrows():\n",
    "        preprocessed, detail = preprocess(np.array(row))\n",
    "        details.append(detail)\n",
    "        for i in range(len(preprocessed) - window_size):\n",
    "            observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], detail, i+window_size])\n",
    "\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(data, v): \n",
    "    np.random.shuffle(np.array(data))\n",
    "\n",
    "    train = data[:int(np.floor(len(data)*(1-v)))]\n",
    "    validation = data[int(np.floor(len(data)*(1-v))):]\n",
    "\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split(data, fold, v, window_size, alpha):\n",
    "    prep = []\n",
    "    validationSet = []\n",
    "    validation = data.iloc[fold-v:fold]\n",
    "    train = data[~data.isin(validation)].dropna()\n",
    "\n",
    "    for index, row in train.iterrows():\n",
    "        p, details = preprocess(row, smoothing = 1, alpha = alpha)\n",
    "        prep.append(p)\n",
    "\n",
    "    train = np.array(prep)\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        for j in range(train.shape[1] - window_size):\n",
    "            x_train.append(train[i][j:j + window_size])\n",
    "            y_train.append(train[i][j + window_size])\n",
    "\n",
    "    x_train = np.array(x_train).reshape(len(x_train), window_size)\n",
    "    y_train = np.array(y_train).reshape(len(y_train))\n",
    "\n",
    "    for index, row in validation.iterrows():\n",
    "        p, details = preprocess(row, smoothing = 0)\n",
    "        prep.append(p)\n",
    "        for i in range(len(p) - window_size):\n",
    "            validationSet.append([p[i:i + window_size], p[i + window_size], details, i + window_size])\n",
    "\n",
    "    x_validation = np.array([x[0] for x in validationSet]).reshape(len(validationSet), window_size)\n",
    "    y_validation = np.array([x[1] for x in validationSet]).reshape(len(validationSet))\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, validationSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(rows, k):\n",
    "    folds = []\n",
    "    rows = np.array_split(np.arange(rows), k)\n",
    "    for row in rows:\n",
    "        folds.append(row[-1])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfolds(df_train, k, options, window_size, val_frac, alpha):\n",
    "    avgSmape = []\n",
    "    train = df_train.sample(frac = 1)\n",
    "    folds = get_folds(len(train), k) #Indicates the points to which data should be used in every fold\n",
    "\n",
    "    validation_size = int(np.floor(val_frac*len(train)))\n",
    "\n",
    "    for fold in folds:\n",
    "        x_train = []\n",
    "        y_train= []\n",
    "        x_train, y_train, x_validation, y_validation, validationSet = preprocess_and_split(train, fold, validation_size, window_size, options.smoothingFactor) #takes df's returns np arrays\n",
    "\n",
    "\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "\n",
    "        validationPrediction = model.predict(x_validation, verbose=0)\n",
    "        yHatReal = []\n",
    "        yReal = []\n",
    "\n",
    "        for i in range(len(y_validation)):\n",
    "            yReal.append(reprocess(y_validation[i], validationSet[i]))\n",
    "            yHatReal.append(reprocess(validationPrediction[i], validationSet[i]) )\n",
    "        smapeVal = 0\n",
    "        for i in range(len(yReal)):\n",
    "            smapeVal += smape_clean(yReal[i], yHatReal[i])\n",
    "        smapeVal /= len(yReal)\n",
    "        print(\"smape equals\", smapeVal, \"with\", fold, \"time step as training\")\n",
    "        avgSmape.append(smapeVal)\n",
    "    return np.mean(avgSmape), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.788, Test: 0.733\n",
      "smape equals 12.460819688737514 with 29 time step as training\n",
      "Train: 0.778, Test: 0.779\n",
      "smape equals 6.524412001087878 with 58 time step as training\n",
      "Train: 0.803, Test: 0.788\n",
      "smape equals 8.535477329066701 with 87 time step as training\n",
      "Train: 0.752, Test: 0.789\n",
      "smape equals 7.671258583447182 with 116 time step as training\n",
      "Train: 0.790, Test: 0.651\n",
      "smape equals 15.208235313127863 with 145 time step as training\n",
      "10.080040583093428\n"
     ]
    }
   ],
   "source": [
    "options = [[[3,3],'sigmoid','relu', 'sigmoid',16, 1, 0,0]]\n",
    "options = pd.DataFrame(options)\n",
    "options = options.set_axis(['layers', 'activation1', 'activation', 'activation2' 'batchSize', 'smoothingFactor', 'averageSmape', 'varianceSmape'], axis=1)\n",
    "# print(options)\n",
    "window_size = 3\n",
    "val_frac = 0.1\n",
    "smape, model = kfolds(df_train, 5, options.iloc[0], window_size, val_frac, options.iloc[0].smoothingFactor)\n",
    "print(smape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lays = [[1],[2],[2,2],[3,3],[2,5]]\n",
    "batchSizes = [8,16,32]\n",
    "acts1 = ['sigmoid', 'relu','linear']\n",
    "activationFunctions = ['sigmoid', 'relu']\n",
    "acts2 = ['sigmoid', 'relu','linear']\n",
    "smoothingfactors = [0.9,1]\n",
    "window_size = 3\n",
    "val_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.800, Test: 0.712\n",
      "smape equals 9.856578386244037 with 29 time step as training\n",
      "Train: 0.802, Test: 0.721\n",
      "smape equals 9.531836632992242 with 58 time step as training\n",
      "Train: 0.827, Test: 0.709\n",
      "smape equals 10.282762073831162 with 87 time step as training\n",
      "Train: 0.793, Test: 0.721\n",
      "smape equals 10.4800494499943 with 116 time step as training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[308], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m smape_avg\u001b[39m=\u001b[39m[]\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     smp, model \u001b[39m=\u001b[39m kfolds(df_train, \u001b[39m5\u001b[39;49m, options\u001b[39m.\u001b[39;49miloc[\u001b[39m0\u001b[39;49m], \u001b[39m2\u001b[39;49m, window_size, val_frac)\n\u001b[1;32m     30\u001b[0m     smape_avg\u001b[39m.\u001b[39mappend(smp)\n\u001b[1;32m     32\u001b[0m options\u001b[39m.\u001b[39miat[i,\u001b[39m3\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(smape_avg)\n",
      "Cell \u001b[0;32mIn[297], line 14\u001b[0m, in \u001b[0;36mkfolds\u001b[0;34m(df_train, k, options, window_size, val_frac, alpha)\u001b[0m\n\u001b[1;32m     10\u001b[0m y_train\u001b[39m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m x_train, y_train, x_validation, y_validation, validationSet \u001b[39m=\u001b[39m preprocess_and_split(train, fold, validation_size, window_size, options\u001b[39m.\u001b[39msmoothingFactor) \u001b[39m#takes df's returns np arrays\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[39m=\u001b[39m build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n\u001b[1;32m     16\u001b[0m validationPrediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_validation, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m yHatReal \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[304], line 22\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(x_train, y_train, x_validation, y_validation, window_size, options)\u001b[0m\n\u001b[1;32m     18\u001b[0m mc \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mbest_model.h5\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, validation_data\u001b[39m=\u001b[39;49m(x_validation, y_validation), batch_size\u001b[39m=\u001b[39;49moptions\u001b[39m.\u001b[39;49mbatchSize, verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[es, mc], epochs\u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m) \u001b[39m#fit the model with early stop\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m##this is the best model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m saved_model \u001b[39m=\u001b[39m load_model(\u001b[39m\"\u001b[39m\u001b[39mbest_model.h5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1607\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1608\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1609\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1610\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1611\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1612\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1613\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1614\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1615\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1616\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1617\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1618\u001b[0m )\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   1945\u001b[0m ):\n\u001b[1;32m   1946\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1947\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[1;32m   1948\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1949\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2495\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m-> 2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mgraph_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2725\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[39m# cache_key_deletion_observer is useless here. It's based on all captures.\u001b[39;00m\n\u001b[1;32m   2722\u001b[0m \u001b[39m# A new cache key will be built later when saving ConcreteFunction because\u001b[39;00m\n\u001b[1;32m   2723\u001b[0m \u001b[39m# only active captures should be saved.\u001b[39;00m\n\u001b[1;32m   2724\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2725\u001b[0m   func_cache_key, _ \u001b[39m=\u001b[39m function_context\u001b[39m.\u001b[39;49mmake_cache_key(\n\u001b[1;32m   2726\u001b[0m       (args, kwargs), captures)\n\u001b[1;32m   2727\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2728\u001b[0m   func_cache_key, _ \u001b[39m=\u001b[39m function_context\u001b[39m.\u001b[39mmake_cache_key(\n\u001b[1;32m   2729\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflat_input_signature, captures)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function_context.py:131\u001b[0m, in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    129\u001b[0m   captures \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    130\u001b[0m signature_context \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mInternalTracingContext()\n\u001b[0;32m--> 131\u001b[0m args_signature \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39;49mfrom_object(\n\u001b[1;32m    132\u001b[0m     args, signature_context)\n\u001b[1;32m    133\u001b[0m captures_dict_tracetype \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mfrom_object(\n\u001b[1;32m    134\u001b[0m     captures, signature_context)\n\u001b[1;32m    135\u001b[0m captures_signature \u001b[39m=\u001b[39m function_cache\u001b[39m.\u001b[39mCaptureSnapshot(\n\u001b[1;32m    136\u001b[0m     captures_dict_tracetype\u001b[39m.\u001b[39mmapping)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:111\u001b[0m, in \u001b[0;36mfrom_object\u001b[0;34m(obj, context)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mNamedTuple\u001b[39m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    109\u001b[0m         named_tuple_type, \u001b[39mtuple\u001b[39m(from_object(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m obj))\n\u001b[1;32m    110\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39;49mTuple(\u001b[39m*\u001b[39;49m(from_object(c, context) \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m obj))\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    114\u001b[0m   \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mDict({k: from_object(obj[k], context) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m obj})\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:111\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mNamedTuple\u001b[39m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    109\u001b[0m         named_tuple_type, \u001b[39mtuple\u001b[39m(from_object(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m obj))\n\u001b[1;32m    110\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mTuple(\u001b[39m*\u001b[39m(from_object(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m obj))\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    114\u001b[0m   \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mDict({k: from_object(obj[k], context) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m obj})\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:96\u001b[0m, in \u001b[0;36mfrom_object\u001b[0;34m(obj, context)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m   context \u001b[39m=\u001b[39m InternalTracingContext()\n\u001b[0;32m---> 96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(obj, trace\u001b[39m.\u001b[39;49mSupportsTracingProtocol):\n\u001b[1;32m     97\u001b[0m   \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__wrapped__\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/typing.py:1122\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_protocol:\n\u001b[1;32m   1118\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mhasattr\u001b[39m(instance, attr) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m             \u001b[39m# All *methods* can be blocked by setting them to None.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m             (\u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, attr, \u001b[39mNone\u001b[39;00m)) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m              \u001b[39mgetattr\u001b[39m(instance, attr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1122\u001b[0m             \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m _get_protocol_attrs(\u001b[39mcls\u001b[39;49m)):\n\u001b[1;32m   1123\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__instancecheck__\u001b[39m(instance)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/typing.py:1071\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m annotations \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(base, \u001b[39m'\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m'\u001b[39m, {})\n\u001b[0;32m-> 1071\u001b[0m \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(base\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(annotations\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attr\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_abc_\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m attr \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n\u001b[1;32m   1073\u001b[0m         attrs\u001b[39m.\u001b[39madd(attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lays = [[3,3]]\n",
    "batchSizes = [16]\n",
    "acts1 = ['sigmoid', 'relu','linear']\n",
    "activationFunctions = ['sigmoid', 'relu']\n",
    "acts2 = ['sigmoid', 'relu','linear']\n",
    "smoothingfactors = [0.9,1]\n",
    "window_size = 3\n",
    "val_frac = 0.1\n",
    "\n",
    "options = []\n",
    "\n",
    "for layer in lays:\n",
    "    for act1 in acts1:\n",
    "        for act2 in acts2:\n",
    "            for activation in activationFunctions:\n",
    "                for smooth in smoothingfactors:\n",
    "                    for batchSize in batchSizes:\n",
    "                        options.append([layer, act1, activation, act2, batchSize, smooth, 0, 0])\n",
    "\n",
    "\n",
    "options = pd.DataFrame(options)\n",
    "options = options.set_axis(['layers', 'activation1', 'activation', 'activation2', 'batchSize', 'smoothingFactor', 'averageSmape', 'varianceSmape'], axis=1)\n",
    "\n",
    "# print(options.iloc[0].layers)\n",
    "\n",
    "for i in range(len(options)):\n",
    "    smape_avg=[]\n",
    "    for j in range(1):\n",
    "        smp, model = kfolds(df_train, 5, options.iloc[0], 2, window_size, val_frac)\n",
    "        smape_avg.append(smp)\n",
    "\n",
    "    options.iat[i,3] = np.mean(smape_avg)\n",
    "    options.iat[i,4] = np.std(smape_avg)\n",
    "\n",
    "\n",
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(by=\"averageSmape\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   layers activation  batchSize  smoothingFactor  averageSmape  varianceSmape\n",
      "0  [3, 3]       relu         16                1             0              0\n"
     ]
    }
   ],
   "source": [
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(by=\"averageSmape\")\n",
    "print(res)\n",
    "\n",
    "res = options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.663, Test: 0.535\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train= []\n",
    "val_frac = 0.1\n",
    "validation_size = int(np.floor(val_frac*len(df_train)))\n",
    "x_train, y_train, x_validation, y_validation, validationSet = preprocess_and_split(df_train, len(df_train), validation_size, window_size, res.iloc[0]['smoothingFactor']) #takes df's returns np arrays\n",
    "\n",
    "model = build_model(x_train, y_train, x_validation, y_validation, window_size, res.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   layers activation  batchSize  smoothingFactor  averageSmape  varianceSmape\n",
      "0  [2, 2]       relu         16                1             0              0\n"
     ]
    }
   ],
   "source": [
    "print(op.sort_values(by=\"averageSmape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 850us/step\n",
      "(146, 1)\n",
      "10/10 [==============================] - 0s 574us/step\n",
      "(292, 1)\n",
      "14/14 [==============================] - 0s 550us/step\n",
      "(438, 1)\n",
      "19/19 [==============================] - 0s 499us/step\n",
      "(584, 1)\n",
      "23/23 [==============================] - 0s 531us/step\n",
      "(730, 1)\n",
      "28/28 [==============================] - 0s 505us/step\n",
      "(876, 1)\n",
      "             0          1          2          3          4          5\n",
      "0     0.180527  13.681183  24.603234  37.541540  44.096648  52.126508\n",
      "1    31.817585  46.799337  37.168746  36.997012  37.210670  39.879151\n",
      "2    18.595027  34.270973  28.866578  31.503552  29.735731  16.531106\n",
      "3     6.114288  17.667179  25.446289  14.746162  20.926489  13.414364\n",
      "4    20.660160  13.779500   1.171077   6.277023   1.298368  12.144159\n",
      "..         ...        ...        ...        ...        ...        ...\n",
      "141   4.308522  11.924643   7.486099   5.044625  10.316634  11.865892\n",
      "142  10.559050   4.979802  16.155487   7.800020   9.695947   2.917132\n",
      "143  10.205582  22.056392   7.622262  24.178258  41.144204  57.523887\n",
      "144   7.467979  17.214770  25.822511  38.044059  53.258172  76.461142\n",
      "145  17.069596  17.199331   0.583792   7.876261  51.848553  66.608493\n",
      "\n",
      "[146 rows x 6 columns]\n",
      "[8.809862595456883, 12.677327454959855, 23.582137438359435, 24.360603024955584, 28.928880356404598, 28.50194206013342]\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "observations = []\n",
    "window_size = 3\n",
    "\n",
    "df_full = pd.DataFrame()\n",
    "df_full = df_train\n",
    "df_full = df_full.drop(df_full.columns[14:], axis=1)\n",
    "\n",
    "num_predictions = 6\n",
    "\n",
    "# Make predictions using autoregressive approach\n",
    "for pred in range(num_predictions):\n",
    "\n",
    "    PF = []\n",
    "    for index, row in df_full.iterrows():\n",
    "        # print(row)\n",
    "        if pred == 0:\n",
    "            preprocessed, details = preprocess(row, smoothing = 1, alpha = res.iloc[0].smoothingFactor)\n",
    "            PF.append(details[2:])\n",
    "            observations.append([preprocessed[11+pred:14+pred],0, details, 14+pred]) #y is unknown and first time point to predict is 15(or 14?)`\n",
    "            # df_full.loc[index,:] = preprocessed\n",
    "        else:\n",
    "            observations.append([row[11+pred:14+pred],0, details, 14+pred])\n",
    "\n",
    "    # Reshape the input for prediction\n",
    "    x = []\n",
    "    for i in (range(len(observations))):\n",
    "        x.append(observations[i][0])\n",
    "    x = np.array(x).reshape(len(x),window_size)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(x)\n",
    "\n",
    "    print(prediction.shape)\n",
    "\n",
    "    y_u = []\n",
    "    for i in range(len(prediction)):\n",
    "        y_u.append(reprocess(prediction[i], observations[i]))\n",
    "        # y_u.append(prediction[i])\n",
    "\n",
    "    # print(pd.DataFrame(y_u).shape)\n",
    "    predictions[15+pred] = pd.DataFrame(y_u)\n",
    "    df_full[15+pred] = pd.DataFrame(y_u)\n",
    "\n",
    "smapes = pd.DataFrame(columns=[i for i in range(num_predictions)])\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    smape_row = []\n",
    "    for j in range(num_predictions):\n",
    "        smape_row.append(smape_clean(predictions.iloc[i, j], df_test.iloc[i, j]))\n",
    "    smapes.loc[i] = smape_row\n",
    "\n",
    "print(smapes)\n",
    "\n",
    "smape_avgs = []\n",
    "for i in range(num_predictions):\n",
    "    smape_avgs.append(np.mean(smapes.iloc[:,i]))\n",
    "print(smape_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = []\n",
    "# y_hat = []\n",
    "# details = []\n",
    "# offset = 7\n",
    "# for i in range(0,10):\n",
    "#     y.append(observations[i+offset][1])\n",
    "#     y_hat.append(y[i] + 0.4)\n",
    "#     details.append(observations[i+offset])\n",
    "\n",
    "# repY = []\n",
    "# repY_hat = []\n",
    "# smape = 0\n",
    "# for i in range(10):\n",
    "#     repY.append(reprocess(y[i], details[i]))\n",
    "#     repY_hat.append(reprocess(y_hat[i], details[i]))\n",
    "#     smape += smape_clean(repY[i], repY_hat[i])\n",
    "\n",
    "# smape /= len(repY)\n",
    "# print(smape)\n",
    "# pyplot.plot(repY, label='original')\n",
    "# pyplot.plot(repY_hat, label='altered')\n",
    "# pyplot.plot(df_train.iloc[1,3:10], label = 'og')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38918c03f5e50da0883dd5b0b10b29c968b274fb52fc312fcc1e11a6fe51f463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
