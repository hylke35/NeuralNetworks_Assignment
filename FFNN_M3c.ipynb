{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 10:39:18.223525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed-window sequences for training and validation data\n",
    "def create_sequences(X, window_size):\n",
    "    seq_X = []\n",
    "    seq_y = []\n",
    "    for i in range(len(X) - window_size):\n",
    "        seq_X.append(X[i:i+window_size])\n",
    "        seq_y.append(X[i+window_size])\n",
    "    return seq_X, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    PF = np.polyfit(np.linspace(0,len(data),num=len(data)), np.log(data), 1)\n",
    "\n",
    "    preprocessed = data / (np.exp(PF[0] * np.linspace(0,len(data),num=len(data)) + PF[1]))\n",
    "    m = np.mean(preprocessed)\n",
    "    s = np.std(preprocessed)\n",
    "    preprocessed = (preprocessed - m)/s\n",
    "\n",
    "    details = [m, s, PF]\n",
    "\n",
    "    return preprocessed, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess(y, details):\n",
    "    mean = details[2][0]\n",
    "    std = details[2][1]\n",
    "    PF = details[2][2]\n",
    "    time = details[3]\n",
    "    \n",
    "    return ((y * std) + mean) * np.exp(PF[0] * time + PF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_loss(y_true, y_pred):\n",
    "    smape = 100 * tf.reduce_mean(2*tf.abs(y_pred - y_true) / (y_true + y_pred))\n",
    "    return smape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(model, validation):\n",
    "    smape = 0\n",
    "    prediction = model.predict(x_validation)\n",
    "    for i in range(len(validation)):\n",
    "        observation = validation[i]\n",
    "        pred = prediction[i]\n",
    "\n",
    "        x_hat = reprocess(pred, observation)\n",
    "        x = reprocess(observation[1], observation)\n",
    "\n",
    "        smape += 2*np.abs(x_hat-x)/(x+x_hat)\n",
    "\n",
    "    smape /= len(validation)\n",
    "    smape *=100\n",
    "\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_validation, y_validation, window_size, options):\n",
    "    # Build the FFNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(window_size, 1))) \n",
    "    model.add(keras.layers.Dense(options[0][0], activation='relu'))\n",
    "\n",
    "    if len(options[0]) > 1:\n",
    "        if len(options[0]) > 2:\n",
    "            for i in range(1,len(options[0])-1):\n",
    "                model.add(keras.layers.Dense(options[0][i], activation=options[1]))\n",
    "                \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Train the model\n",
    "    # model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], validation_data=(x_validation, y_validation), verbose = 0)\n",
    "    model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], verbose = 0)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(x_validation)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_validation, y_validation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_train = df.iloc[:,:14]\n",
    "df_test = df.iloc[:,14:]\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "observations = []\n",
    "PF = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    preprocessed, p = preprocess(np.array(row))\n",
    "    PF.append(p)\n",
    "   \n",
    "    for i in range(len(preprocessed) - window_size):\n",
    "        observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], p, i+window_size])     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling: dont use for now\n",
    "# np.random.shuffle(observations)\n",
    "\n",
    "train = observations[:int(np.floor(len(observations)*0.8))]\n",
    "validation = observations[int(np.floor(len(observations)*0.8)):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_validation = []\n",
    "y_validation = []\n",
    "\n",
    "folds = [6,10,13]\n",
    "\n",
    "\n",
    "for i in range(len(train)):\n",
    "    x_train.append(train[i][0])\n",
    "    y_train.append(train[i][1])\n",
    "\n",
    "for i in range(len(validation)):\n",
    "    x_validation.append(validation[i][0])\n",
    "    y_validation.append(validation[i][1])\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train),window_size)\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation),window_size)\n",
    "y_validation = np.array(y_validation).reshape(len(x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 13:10:41.575380: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 564us/step\n",
      "11/11 [==============================] - 0s 647us/step - loss: 0.9466 - mse: 0.9466\n",
      "[[50, 50], 'sigmoid', 16, 50, 0, 0]\n",
      "11/11 [==============================] - 0s 574us/step\n",
      "          0        1   2   3          4    5\n",
      "0  [50, 50]  sigmoid  16  50  14.737995  0.0\n"
     ]
    }
   ],
   "source": [
    "lays = [[50,50]]\n",
    "epochs = [50]\n",
    "batchSizes = [16]\n",
    "activationFunctions = ['sigmoid']\n",
    "\n",
    "options = []\n",
    "\n",
    "for layer in lays:\n",
    "    for activation in activationFunctions:\n",
    "        for batchSize in batchSizes:\n",
    "            for epoch in epochs:\n",
    "                options.append([layer, activation, batchSize, epoch, 0, 0])\n",
    "\n",
    "\n",
    "for i in range(len(options)):\n",
    "    smape_avg=[]\n",
    "    for j in range(1):\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options[i])\n",
    "\n",
    "        print(options[i])\n",
    "        \n",
    "        smape_avg.append(smape(model, validation))\n",
    "\n",
    "    options[i][4] = np.mean(smape_avg)\n",
    "    options[i][5] = np.std(smape_avg)\n",
    "\n",
    "\n",
    "op = pd.DataFrame(options)\n",
    "res = op.sort_values(4, ascending=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 547us/step\n",
      "11/11 [==============================] - 0s 632us/step - loss: 0.9475 - mse: 0.9475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for the final model build we should do something better but i focussed on the autoregression for now\n",
    "lays = [[50,50]]\n",
    "epochs = [50]\n",
    "batchSizes = [16]\n",
    "activationFunctions = ['sigmoid']\n",
    "\n",
    "options = [layer, activation, batchSize, epoch, 0, 0]\n",
    "\n",
    "model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_clean(y_true, y_pred):\n",
    "    smape = 100 * np.mean(2*np.abs(y_pred - y_true) / (y_true + y_pred))\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 650us/step\n",
      "10/10 [==============================] - 0s 619us/step\n",
      "14/14 [==============================] - 0s 497us/step\n",
      "19/19 [==============================] - 0s 489us/step\n",
      "23/23 [==============================] - 0s 480us/step\n",
      "28/28 [==============================] - 0s 485us/step\n",
      "             0          1          2          3          4          5\n",
      "0     4.465903  17.940350  28.812749  41.659446  48.154611  56.098679\n",
      "1    20.356964   4.834220  14.884544  15.061395  14.841357  12.082491\n",
      "2    51.590416  36.408719  41.739634  39.151551  40.889319  53.526732\n",
      "3    24.966125  13.479411   5.626795  16.403325  10.201004  17.732010\n",
      "4    10.544811  17.442926  29.892019  37.132113  29.767556  19.071480\n",
      "..         ...        ...        ...        ...        ...        ...\n",
      "141   7.056340   0.570344   3.878374   6.320658   1.042651   0.511384\n",
      "142   9.995224   4.414794  15.593730   7.235500   9.131881   3.482306\n",
      "143   2.130148  14.039114  15.677949  16.177441  33.341478  50.025334\n",
      "144  30.199691  39.684936  47.975102  59.608610  73.870324  95.163238\n",
      "145  35.443804  35.570379  18.077521  10.819877  34.014729  49.489588\n",
      "\n",
      "[146 rows x 6 columns]\n",
      "[16.471092820578566, 19.996444354696248, 28.62700340093956, 28.044519387002804, 33.042898245276874, 36.46776761336691]\n"
     ]
    }
   ],
   "source": [
    "##TESTING\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "observations = []\n",
    "window_size = 3\n",
    "\n",
    "df_full = pd.DataFrame()\n",
    "df_full = df_train\n",
    "df_full = df_full.drop(df_full.columns[14:], axis=1)\n",
    "\n",
    "num_predictions = 6\n",
    "\n",
    "# Make predictions using autoregressive approach\n",
    "for pred in range(num_predictions):\n",
    "\n",
    "    PF = []\n",
    "    for index, row in df_full.iterrows():\n",
    "        preprocessed, p = preprocess(np.array(row))\n",
    "        PF.append(p)\n",
    "        observations.append([preprocessed[11+pred:14+pred],0, p, 14+pred]) #y is unknown and first time point to predict is 15(or 14?)`\n",
    "\n",
    "    # Reshape the input for prediction\n",
    "    x = []\n",
    "    for i in (range(len(observations))):\n",
    "        x.append(observations[i][0])\n",
    "    x = np.array(x).reshape(len(x),window_size)\n",
    "    \n",
    "    # Make the prediction\n",
    "    prediction = model.predict(x)\n",
    "\n",
    "    y_u = []\n",
    "    for i in range(len(prediction)):\n",
    "        y_u.append(reprocess(prediction[i], observations[i]))\n",
    "\n",
    "    # print(pd.DataFrame(y_u).shape)\n",
    "    predictions[15+pred] = pd.DataFrame(pd.DataFrame(y_u))\n",
    "    df_full[15+pred] = pd.DataFrame(y_u)\n",
    "\n",
    "smapes = pd.DataFrame(columns=[i for i in range(num_predictions)])\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    smape_row = []\n",
    "    for j in range(num_predictions):\n",
    "        smape_row.append(smape_clean(predictions.iloc[i, j], df_test.iloc[i, j]))\n",
    "    smapes.loc[i] = smape_row\n",
    "\n",
    "print(smapes)\n",
    "\n",
    "smape_avgs = []\n",
    "for i in range(num_predictions):\n",
    "    smape_avgs.append(np.mean(smapes.iloc[:,i]))\n",
    "print(smape_avgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38918c03f5e50da0883dd5b0b10b29c968b274fb52fc312fcc1e11a6fe51f463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
