{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import seaborn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed-window sequences for training and validation data\n",
    "def create_sequences(X, window_size):\n",
    "    seq_X = []\n",
    "    seq_y = []\n",
    "    for i in range(len(X) - window_size):\n",
    "        seq_X.append(X[i:i+window_size])\n",
    "        seq_y.append(X[i+window_size])\n",
    "    return seq_X, seq_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    PF = np.polyfit(np.linspace(0,len(data),num=len(data)), np.log(data), 1)\n",
    "\n",
    "    preprocessed = data / (np.exp(PF[0] * np.linspace(0,len(data),num=len(data)) + PF[1]))\n",
    "    m = np.mean(preprocessed)\n",
    "    s = np.std(preprocessed)\n",
    "    preprocessed = (preprocessed - m)/s\n",
    "\n",
    "    details = [m, s, PF]\n",
    "\n",
    "    return preprocessed, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "df = df.iloc[:146,6:26]\n",
    "\n",
    "df_train = df.iloc[:,:14]\n",
    "df_test = df.iloc[:,14:]\n",
    "\n",
    "window_size = 3\n",
    "\n",
    "observations = []\n",
    "PF = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    preprocessed, p = preprocess(np.array(row))\n",
    "    PF.append(p)\n",
    "   \n",
    "    for i in range(len(preprocessed) - window_size):\n",
    "        observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], p, i+window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(observations)\n",
    "train = observations[:int(np.floor(len(observations)*0.8))]\n",
    "validation = observations[int(np.floor(len(observations)*0.8)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_validation, y_validation, window_size, options):\n",
    "    # Build the FFNN model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(window_size, 1))) \n",
    "    model.add(keras.layers.Dense(options[0][0], activation='relu'))\n",
    "\n",
    "    if len(options[0]) > 1:\n",
    "        if len(options[0]) > 2:\n",
    "            for i in range(1,len(options[0])-1):\n",
    "                model.add(keras.layers.Dense(options[0][i], activation=options[1]))\n",
    "                \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=options[3], batch_size=options[2], validation_data=(x_validation, y_validation), verbose = 0)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(x_validation)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_validation, y_validation)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    x_train.append(train[i][0])\n",
    "    y_train.append(train[i][1])\n",
    "\n",
    "x_validation = []\n",
    "y_validation = []\n",
    "\n",
    "for i in range(len(validation)):\n",
    "    x_validation.append(validation[i][0])\n",
    "    y_validation.append(validation[i][1])\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train),window_size)\n",
    "y_train = np.array(y_train).reshape(len(y_train))\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation),window_size)\n",
    "y_validation = np.array(y_validation).reshape(len(x_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "\n",
    "    df = pd.read_excel(\"/Users/lars/Documents/GitHub/NeuralNetworks_Assignment/M3C.xls\")\n",
    "    df = df.iloc[:146,6:26]\n",
    "\n",
    "    df_train = df.iloc[:,:14]\n",
    "    df_test = df.iloc[:,14:]\n",
    "\n",
    "    window_size = 3\n",
    "\n",
    "    observations = []\n",
    "    PF = []\n",
    "\n",
    "    for index, row in df_train.iterrows():\n",
    "        preprocessed, p = preprocess(np.array(row))\n",
    "        PF.append(p)\n",
    "   \n",
    "    for i in range(len(preprocessed) - window_size):\n",
    "        observations.append([preprocessed[i:i+window_size],preprocessed[i+window_size], p, i+window_size])\n",
    "\n",
    "    np.random.shuffle(observations)\n",
    "    train = observations[:int(np.floor(len(observations)*0.8))]\n",
    "    validation = observations[int(np.floor(len(observations)*0.8)):]\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        x_train.append(train[i][0])\n",
    "        y_train.append(train[i][1])\n",
    "\n",
    "    x_validation = []\n",
    "    y_validation = []\n",
    "\n",
    "    for i in range(len(validation)):\n",
    "        x_validation.append(validation[i][0])\n",
    "        y_validation.append(validation[i][1])\n",
    "\n",
    "    x_train = np.array(x_train).reshape(len(x_train),window_size)\n",
    "    y_train = np.array(y_train).reshape(len(y_train))\n",
    "    x_validation = np.array(x_validation).reshape(len(x_validation),window_size)\n",
    "    y_validation = np.array(y_validation).reshape(len(x_validation))\n",
    "\n",
    "    return x_train, y_train, x_validation, y_validation, window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprocess(y, details):\n",
    "    mean = details[2][0]\n",
    "    std = details[2][1]\n",
    "    PF = details[2]\n",
    "    time = details[3]\n",
    "    \n",
    "    return ((y * std) + mean) * np.exp(PF[0] * time + PF[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(model, validation):\n",
    "    smape = 0\n",
    "    prediction = model.predict(x_validation)\n",
    "    for i in range(len(validation)):\n",
    "        observation = validation[i]\n",
    "        pred = prediction[i]\n",
    "\n",
    "        x_hat = reprocess(pred, observation)\n",
    "        x = reprocess(observation[1], observation)\n",
    "\n",
    "        smape += 2*np.abs(x-x_hat)/(x+x_hat)*100\n",
    "\n",
    "    smape /= len(validation)\n",
    "\n",
    "    return smape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lays = [[500,500]]\n",
    "# epochs = [15]\n",
    "# batchSizes = [16]\n",
    "# activationFunctions = ['sigmoid','relu']\n",
    "\n",
    "# options = []\n",
    "\n",
    "# for layer in lays:\n",
    "#     for activation in activationFunctions:\n",
    "#         for batchSize in batchSizes:\n",
    "#             for epoch in epochs:\n",
    "#                 options.append([layer, activation, batchSize, epoch, 0, 0])\n",
    "\n",
    "\n",
    "# for i in range(len(options)):\n",
    "#     smape_avg=[]\n",
    "#     for j in range(10):\n",
    "#         model = build_model(x_train, y_train, x_validation, y_validation, window_size, options[i])\n",
    "\n",
    "#         print(options[i])\n",
    "        \n",
    "#         smape_avg.append(smape(model, validation))\n",
    "\n",
    "\n",
    "\n",
    "#     options[i][4] = np.mean(smape_avg)\n",
    "#     options[i][5] = np.std(smape_avg)\n",
    "\n",
    "\n",
    "# op = pd.DataFrame(options)\n",
    "# res = op.sort_values(4, ascending=False)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smape(params):\n",
    "    x_train, y_train, x_validation, y_validation, window_size = get_data()\n",
    "    n_layers, l1, l2, l3, epochs, n_batch = params\n",
    "\n",
    "    layers = [int(l1), int(l2), int(l3)]\n",
    "    lays = []\n",
    "    for i in range(int(n_layers)):\n",
    "        lays.append(layers[i])\n",
    "        \n",
    "    acts = ['sigmoid','relu']\n",
    "\n",
    "    options = [lays, 'sigmoid', int(n_batch), int(epochs), 0, 0]\n",
    "\n",
    "    smape_avg=[]\n",
    "    for j in range(1):\n",
    "        model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "        smape_avg.append(smape(model, validation))\n",
    "\n",
    "    return np.mean(smape_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5002 - mse: 0.5002\n",
      "11/11 [==============================] - 0s 569us/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9067 - mse: 0.9067\n",
      "11/11 [==============================] - 0s 666us/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3160 - mse: 0.3160\n",
      "11/11 [==============================] - 0s 509us/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.8483 - mse: 1.8483\n",
      "11/11 [==============================] - 0s 537us/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4852 - mse: 1.4852\n",
      "11/11 [==============================] - 0s 573us/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1363 - mse: 1.1363\n",
      "11/11 [==============================] - 0s 568us/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5441 - mse: 0.5441\n",
      "11/11 [==============================] - 0s 740us/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8787 - mse: 0.8787\n",
      "11/11 [==============================] - 0s 568us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3313 - mse: 0.3313\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4694 - mse: 1.4694\n",
      "11/11 [==============================] - 0s 657us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3749 - mse: 1.3749\n",
      "11/11 [==============================] - 0s 542us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0389 - mse: 1.0389\n",
      "11/11 [==============================] - 0s 655us/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4590 - mse: 1.4590\n",
      "11/11 [==============================] - 0s 505us/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7137 - mse: 0.7137\n",
      "11/11 [==============================] - 0s 542us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6764 - mse: 1.6764\n",
      "11/11 [==============================] - 0s 499us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1151 - mse: 1.1151\n",
      "11/11 [==============================] - 0s 478us/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2213 - mse: 0.2213\n",
      "11/11 [==============================] - 0s 556us/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4699 - mse: 1.4699\n",
      "11/11 [==============================] - 0s 550us/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2702 - mse: 1.2702\n",
      "11/11 [==============================] - 0s 529us/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7984 - mse: 0.7984\n",
      "11/11 [==============================] - 0s 524us/step\n",
      "1/1 [==============================] - 0s 32ms/step2.0% GA is running.\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3598 - mse: 0.3598\n",
      "11/11 [==============================] - 0s 490us/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0240 - mse: 1.0240\n",
      "11/11 [==============================] - 0s 493us/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0552 - mse: 1.0552\n",
      "11/11 [==============================] - 0s 489us/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5715 - mse: 0.5715\n",
      "11/11 [==============================] - 0s 483us/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4443 - mse: 0.4443\n",
      "11/11 [==============================] - 0s 484us/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4902 - mse: 0.4902\n",
      "11/11 [==============================] - 0s 536us/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7000 - mse: 0.7000\n",
      "11/11 [==============================] - 0s 561us/step\n"
     ]
    }
   ],
   "source": [
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "\n",
    "varbound = np.array([[1, 3], [1, 100], [1, 100], [1, 100], [1, 100], [1, 100]])\n",
    "vartype = np.array([['int'], ['int'], ['int'], ['int'], ['int'], ['int']])\n",
    "\n",
    "algorithm_param = {'max_num_iteration': 50,\\\n",
    "                   'population_size':20,\\\n",
    "                   'mutation_probability':0.25,\\\n",
    "                   'elit_ratio': 0.01,\\\n",
    "                   'crossover_probability': 0.5,\\\n",
    "                   'parents_portion': 0.1,\\\n",
    "                   'crossover_type':'uniform',\\\n",
    "                   'max_iteration_without_improv':None,\\\n",
    "                    'function_timeout':120.0,\\\n",
    "                    'func_timeout':120.0}\n",
    "\n",
    "model = ga(function=get_smape, \n",
    "            dimension=6,\n",
    "            variable_type_mixed=vartype, \n",
    "            variable_boundaries=varbound,\n",
    "            algorithm_parameters=algorithm_param)\n",
    "\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'variable': array([ 1., 77., 53., 46., 92.,  1.]), 'function': 11.443584442138672}\n"
     ]
    }
   ],
   "source": [
    "print(model.output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "\n",
    "\n",
    "lays = [500,500]\n",
    "epochs = [15]\n",
    "batchsize = [16]\n",
    "activation = ['sigmoid']\n",
    "\n",
    "options = [lays, activation, batchsize, epochs, 0, 0]\n",
    "\n",
    "smape_avg=[]\n",
    "for j in range(20):\n",
    "    model = build_model(x_train, y_train, x_validation, y_validation, window_size, options)\n",
    "    smape_avg.append(smape(model, validation))\n",
    "\n",
    "    options[i][4] = np.mean(smape_avg)\n",
    "    options[i][5] = np.std(smape_avg)\n",
    "\n",
    "print(pd.DataFrame(options))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38918c03f5e50da0883dd5b0b10b29c968b274fb52fc312fcc1e11a6fe51f463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
